{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5OTDqy+2dSg1pPSr+Liix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d461ffaa9bf4636a590f9521827a10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf6e4455f8f34b62bb8868e806aad73d",
              "IPY_MODEL_d0e8224961354c088e37ac15fbd7272b"
            ],
            "layout": "IPY_MODEL_3f03b66ffe704ec1aeed52c2d69ba823"
          }
        },
        "bf6e4455f8f34b62bb8868e806aad73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24785c055464f5987eeb42d3b76d05a",
            "placeholder": "​",
            "style": "IPY_MODEL_e182e18bfd324945920e0e1e4392d16e",
            "value": "<div>\n    <div style=\"text-align:center;\">\n        <h3>Initial Molecule</h3>\n        <p>SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</p>\n    </div>\n    <div id=\"3dmolviewer_17395713091075783\"  style=\"position: relative; width: 400px; height: 400px;\">\n        <p id=\"3dmolwarning_17395713091075783\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    //this is to ignore the existence of requirejs amd\n    var savedexports, savedmodule;\n    if (typeof exports !== 'undefined') savedexports = exports;\n    else exports = {}\n    if (typeof module !== 'undefined') savedmodule = module;\n    else module = {}\n\n    var tag = document.createElement('script');\n    tag.src = uri;\n    tag.async = true;\n    tag.onload = () => {\n        exports = savedexports;\n        module = savedmodule;\n        resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('script')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n});\n};\n\nif(typeof $3Dmolpromise === 'undefined') {\n$3Dmolpromise = null;\n  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.4.2/3Dmol-min.js');\n}\n\nvar viewer_17395713091075783 = null;\nvar warn = document.getElementById(\"3dmolwarning_17395713091075783\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n$3Dmolpromise.then(function() {\nviewer_17395713091075783 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_17395713091075783\"),{backgroundColor:\"white\"});\nviewer_17395713091075783.zoomTo();\n\tviewer_17395713091075783.addModel(\"\\n     RDKit          3D\\n\\n113119  0  0  0  0  0  0  0  0999 V2000\\n   -0.8488   -3.7510    5.8005 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.0581   -3.5174    4.3450 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.2075   -4.5011    3.5632 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.1064   -2.1959    3.8710 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3462   -1.8506    2.5040 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -2.8347   -1.7808    2.2106 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.5895   -2.1068    3.1221 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4398   -1.5477    0.8003 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -4.9722   -1.2914    0.9822 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.7540   -0.3716   -0.0343 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -2.8570    1.0718    0.6534 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -3.2763    2.0631   -0.2950 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.2637    3.0316   -0.0390 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.8129    3.1053    1.0865 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.7202    3.9288   -1.1121 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.8072    4.5335   -1.9883 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.2647    5.3491   -3.0298 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.6355    5.5836   -3.1912 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.5481    5.0038   -2.3052 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.0948    4.1817   -1.2693 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.5746    1.6398    1.4213 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -1.8538    2.9784    1.7816 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.3496    1.7257    0.4303 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8639    0.7991    0.7284 C   0  0  2  0  0  0  0  0  0  0  0  0\\n    1.5861    0.5624   -0.4827 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.5234    1.4670   -1.0003 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.6852    2.5970   -0.4592 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.3959    1.0849   -2.1693 C   0  0  2  0  0  0  0  0  0  0  0  0\\n    2.8337    0.0421   -2.9282 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.8439    0.7687   -1.7092 C   0  0  1  0  0  0  0  0  0  0  0  0\\n    4.9133   -0.4369   -0.8689 N   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.8239   -0.5366    0.2424 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.6295    0.4123    0.4591 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.8650   -1.7348    1.1338 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.9050   -2.7715    1.0501 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.9539   -3.8618    1.9233 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.9488   -3.9362    2.9020 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.9063   -2.9244    2.9939 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.8665   -1.8316    2.1220 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.7841    0.6407   -2.8922 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.6707    1.6885   -3.1991 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.5391    1.5886   -4.2904 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.5292    0.4454   -5.0926 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.6443   -0.5976   -4.8053 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.7730   -0.5021   -3.7146 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.4268   -0.5126    1.3495 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.1971   -1.7640    0.9936 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.6371   -0.5344    2.2110 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.1818    0.8199    2.7239 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.3594    0.6889    3.7562 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.0504    1.5578    3.5303 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.0619   -0.4101   -1.6013 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -1.8989   -0.1216   -2.3981 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.5372    1.0939   -2.9878 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.3597    1.1414   -3.8919 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.2019    2.1428   -2.7906 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.3682    0.2118   -2.1066 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.8734   -1.0192   -2.5657 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.6931   -1.6849   -2.1905 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -3.9925   -2.8954   -1.2990 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.2171   -2.8759    0.0177 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -3.6103   -3.9856    0.7941 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.1701   -2.9687    6.2139 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.8288   -3.6988    6.3284 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.3870   -4.7484    5.9690 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.9545   -2.6623    1.8594 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.1803   -0.4940    1.7211 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.4794   -2.2083    1.3491 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.4822   -1.0196    0.0461 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.6901   -0.6347   -0.0558 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.6666    1.0092    1.3897 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.7460    4.3924   -1.8495 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.5498    5.8268   -3.6958 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.9878    6.2273   -3.9890 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.6106    5.1854   -2.4284 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.8187    3.7349   -0.5987 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.5443    2.9846    2.5012 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.6903    1.5269   -0.5974 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.0435    2.7673    0.3934 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.5650    1.2728    1.4467 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.4428    1.9743   -2.8321 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.6334   -0.7204   -2.3285 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.1663    1.6442   -1.0961 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.4674   -1.3069   -1.2302 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.1208   -2.7671    0.3083 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.2061   -4.6445    1.8506 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.9852   -4.7813    3.5787 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.6729   -2.9785    3.7565 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.6116   -1.0506    2.2363 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.6977    2.5874   -2.5953 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    8.2302    2.3932   -4.5123 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    8.1946    0.3679   -5.9422 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.6336   -1.4889   -5.4220 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.0934   -1.3216   -3.5121 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.6492   -2.3401    0.2185 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.3643   -2.3837    1.9003 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.2019   -1.5170    0.5973 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.0915   -0.0120    4.5686 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.3261    0.3807    3.3404 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.5806    1.6509    4.2624 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.4445    2.3614    2.9452 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.4234    2.0886    4.4324 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.7239    0.8480    3.8921 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.2670    0.2428   -3.7353 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.7000    1.1782   -4.9547 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.2493    2.0488   -3.6671 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.2699    0.9100   -2.9641 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.0204    0.6375   -1.3156 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.1719   -2.0305   -3.1129 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.7197   -3.8225   -1.8553 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.0826   -2.9816   -1.1041 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.1265   -2.9731   -0.2251 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.2160   -4.7847    0.3605 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  2  3  2  0\\n  2  4  1  0\\n  4  5  1  0\\n  5  6  1  0\\n  6  7  2  0\\n  6  8  1  0\\n  8  9  1  1\\n  8 10  1  0\\n 10 11  1  0\\n 11 12  1  0\\n 12 13  1  0\\n 13 14  2  0\\n 13 15  1  0\\n 15 16  2  0\\n 16 17  1  0\\n 17 18  2  0\\n 18 19  1  0\\n 19 20  2  0\\n 11 21  1  0\\n 21 22  1  1\\n 21 23  1  0\\n 23 24  1  0\\n 24 25  1  0\\n 25 26  1  0\\n 26 27  2  0\\n 26 28  1  0\\n 28 29  1  0\\n 28 30  1  0\\n 30 31  1  0\\n 31 32  1  0\\n 32 33  2  0\\n 32 34  1  0\\n 34 35  2  0\\n 35 36  1  0\\n 36 37  2  0\\n 37 38  1  0\\n 38 39  2  0\\n 30 40  1  0\\n 40 41  2  0\\n 41 42  1  0\\n 42 43  2  0\\n 43 44  1  0\\n 44 45  2  0\\n 24 46  1  0\\n 46 47  1  0\\n 46 48  2  0\\n 48 49  1  0\\n 49 50  1  0\\n 49 51  1  0\\n 10 52  1  0\\n 52 53  1  6\\n 53 54  1  0\\n 54 55  1  0\\n 54 56  2  0\\n 52 57  1  0\\n 57 58  1  0\\n 58 59  1  0\\n 59 60  1  0\\n 60 61  1  0\\n 61 62  1  0\\n 48  5  1  0\\n 59 52  1  0\\n 61  8  1  0\\n 20 15  1  0\\n 49 21  1  0\\n 39 34  1  0\\n 45 40  1  0\\n  1 63  1  0\\n  1 64  1  0\\n  1 65  1  0\\n  5 66  1  6\\n  9 67  1  0\\n  9 68  1  0\\n  9 69  1  0\\n 10 70  1  6\\n 11 71  1  1\\n 16 72  1  0\\n 17 73  1  0\\n 18 74  1  0\\n 19 75  1  0\\n 20 76  1  0\\n 22 77  1  0\\n 23 78  1  0\\n 23 79  1  0\\n 24 80  1  1\\n 28 81  1  6\\n 29 82  1  0\\n 30 83  1  1\\n 31 84  1  0\\n 35 85  1  0\\n 36 86  1  0\\n 37 87  1  0\\n 38 88  1  0\\n 39 89  1  0\\n 41 90  1  0\\n 42 91  1  0\\n 43 92  1  0\\n 44 93  1  0\\n 45 94  1  0\\n 47 95  1  0\\n 47 96  1  0\\n 47 97  1  0\\n 50 98  1  0\\n 50 99  1  0\\n 50100  1  0\\n 51101  1  0\\n 51102  1  0\\n 51103  1  0\\n 55104  1  0\\n 55105  1  0\\n 55106  1  0\\n 57107  1  0\\n 57108  1  0\\n 59109  1  6\\n 60110  1  0\\n 60111  1  0\\n 61112  1  6\\n 62113  1  0\\nM  END\\n\",\"sdf\");\n\tviewer_17395713091075783.setStyle({\"stick\": {}});\n\tviewer_17395713091075783.setBackgroundColor(\"0xeeeeee\");\n\tviewer_17395713091075783.zoomTo();\nviewer_17395713091075783.render();\n});\n</script></div>"
          }
        },
        "d0e8224961354c088e37ac15fbd7272b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfe4c1af3d884dc2b2d7bfe49c4f0f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_a0012579288c46d2bfbaf9d72d874d95",
            "value": "<div>\n    <div style=\"text-align:center;\">\n        <h3>Optimized Molecule</h3>\n        <p>SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</p>\n    </div>\n    <div id=\"3dmolviewer_17395713105415018\"  style=\"position: relative; width: 400px; height: 400px;\">\n        <p id=\"3dmolwarning_17395713105415018\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    //this is to ignore the existence of requirejs amd\n    var savedexports, savedmodule;\n    if (typeof exports !== 'undefined') savedexports = exports;\n    else exports = {}\n    if (typeof module !== 'undefined') savedmodule = module;\n    else module = {}\n\n    var tag = document.createElement('script');\n    tag.src = uri;\n    tag.async = true;\n    tag.onload = () => {\n        exports = savedexports;\n        module = savedmodule;\n        resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('script')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n});\n};\n\nif(typeof $3Dmolpromise === 'undefined') {\n$3Dmolpromise = null;\n  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.4.2/3Dmol-min.js');\n}\n\nvar viewer_17395713105415018 = null;\nvar warn = document.getElementById(\"3dmolwarning_17395713105415018\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n$3Dmolpromise.then(function() {\nviewer_17395713105415018 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_17395713105415018\"),{backgroundColor:\"white\"});\nviewer_17395713105415018.zoomTo();\n\tviewer_17395713105415018.addModel(\"\\n     RDKit          3D\\n\\n121123  0  0  0  0  0  0  0  0999 V2000\\n   -2.5674    4.0849    4.3802 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.6446    3.2673    3.5394 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.4603    3.0808    3.9182 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.1113    2.7170    2.3400 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3212    1.8937    1.4788 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -2.2124    1.3898    0.3462 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.0631    2.1658   -0.0754 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.1203   -0.0131   -0.3160 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -1.1325   -0.9243    0.4632 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.6019    0.2049   -1.7739 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -0.2860    0.7092   -1.7904 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.7244   -1.0542   -2.6479 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.1822   -1.4916   -2.7663 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.8543   -1.8085   -1.4021 C   0  0  2  0  0  0  0  0  0  0  0  0\\n   -3.4164   -3.2054   -0.8878 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.2821   -1.8346   -1.6010 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.0033   -2.6694   -2.4710 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.4907   -2.5596   -2.5057 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.4241   -3.4998   -3.2231 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.5816   -0.6385   -0.3684 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -4.1584   -0.9287    1.0488 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.5474   -1.2040    0.9819 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.3086   -0.0581    1.2914 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.7135   -0.2660    0.8057 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -8.5921   -1.0970    1.5215 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -9.8889   -1.3232    1.0478 C   0  0  0  0  0  0  0  0  0  0  0  0\\n  -10.3129   -0.7298   -0.1464 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -9.4413    0.0909   -0.8688 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -8.1422    0.3138   -0.4017 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.1363    2.7150    0.9148 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.1939    2.3396    1.0695 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.3339    3.1626    0.4892 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.6728    1.0995    1.8394 C   0  0  2  0  0  0  0  0  0  0  0  0\\n    2.5937    1.4962    3.0055 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.3679    0.2465    0.9262 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.6734   -1.0960    1.1786 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.3247   -1.6448    2.2603 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.4653   -1.8661    0.1558 C   0  0  2  0  0  0  0  0  0  0  0  0\\n    3.0505   -3.2101    0.1229 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.9905   -1.7513    0.4170 C   0  0  1  0  0  0  0  0  0  0  0  0\\n    5.7447   -2.5303   -0.5710 N   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.1131   -2.9181   -0.3500 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.6206   -2.7826    0.7978 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.9328   -3.4978   -1.4406 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    7.6716   -3.1936   -2.7899 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    8.4617   -3.7435   -3.8057 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    9.5180   -4.6020   -3.4873 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    9.7939   -4.9038   -2.1519 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    9.0088   -4.3526   -1.1331 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.4449   -0.3055    0.3763 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.8516    0.3403    1.5552 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.2136    1.6886    1.5338 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.1690    2.4078    0.3380 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.7782    1.7764   -0.8526 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.6558    2.5642   -2.1215 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.4091    0.4235   -0.8265 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.5578    4.0335    0.2165 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.5507    3.9497   -1.3205 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.1549    5.2955    0.7352 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.0041    0.4045    0.0000 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4484    4.4008    3.7841 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.0427    4.9907    4.7503 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.9106    3.4818    5.2479 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.0452    1.0402    2.0989 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.9287   -1.8823   -0.0324 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.4798   -1.1326    1.4944 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.1451   -0.4436    0.5265 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.2390    0.9805   -2.2582 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.3469   -0.0480   -1.6911 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.1022   -1.8869   -2.2618 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3466   -0.8174   -3.6660 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.7540   -0.6811   -3.2727 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.2169   -2.3776   -3.4373 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.3436   -3.2687   -0.6612 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.5995   -3.9847   -1.6522 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.9980   -3.4987    0.0104 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.9224   -3.2879   -3.2282 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.7744   -1.5311   -2.8157 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.9049   -2.7658   -1.4931 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.2244    0.1798   -0.7671 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.6591   -1.8150    1.4886 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.9243   -0.0968    1.7534 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.8906    0.8621    0.8250 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -6.3196    0.0914    2.3910 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -8.2714   -1.5654    2.4449 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  -10.5598   -1.9671    1.6007 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  -11.3150   -0.9071   -0.5104 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -9.7686    0.5398   -1.7995 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -7.4731    0.9395   -0.9807 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.5445    4.0314    1.1442 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.1493    3.4917   -0.5455 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.2699    2.5710    0.4311 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.8685    0.5158    2.3045 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.5364    1.9548    2.6402 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.0805    2.2179    3.6734 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.8641    0.6008    3.6050 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.2391   -1.4293   -0.8415 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.3573   -3.6421    0.9620 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.1859   -2.1813    1.4249 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.2761   -2.8054   -1.4636 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.8763   -2.5135   -3.0657 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    8.2603   -3.5006   -4.8416 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   10.1265   -5.0278   -4.2754 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   10.6131   -5.5679   -1.9064 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    9.2376   -4.6049   -0.1044 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.8753   -0.1954    2.4948 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.5119    2.1808    2.4506 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.4284    3.4584    0.3420 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.9042    1.9308   -2.9985 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.6137    2.9326   -2.2245 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.3473    3.4316   -2.1153 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.0800   -0.0597   -1.7379 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.6237    4.2223    0.4685 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.8015    4.9403   -1.7579 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.4159    3.6231   -1.7365 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3235    3.2345   -1.6683 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.3915    5.1997    1.8169 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.0817    5.5090    0.1634 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.5050    6.1796    0.6084 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.7844   -0.1947    0.0000 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.7803   -0.2098    0.0000 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  2  3  2  0\\n  2  4  1  0\\n  4  5  1  0\\n  5  6  1  0\\n  6  7  2  0\\n  6  8  1  0\\n  8  9  1  1\\n  8 10  1  0\\n 10 11  1  0\\n 10 12  1  0\\n 12 13  1  0\\n 13 14  1  0\\n 14 15  1  0\\n 14 16  1  6\\n 16 17  1  0\\n 17 18  1  0\\n 17 19  2  0\\n 14 20  1  0\\n 20 21  1  0\\n 21 22  1  0\\n 22 23  1  0\\n 23 24  1  0\\n 24 25  2  0\\n 25 26  1  0\\n 26 27  2  0\\n 27 28  1  0\\n 28 29  2  0\\n  5 30  1  0\\n 30 31  2  3\\n 31 32  1  0\\n 31 33  1  0\\n 33 34  1  0\\n 33 35  1  0\\n 35 36  1  0\\n 36 37  2  0\\n 36 38  1  0\\n 38 39  1  0\\n 38 40  1  0\\n 40 41  1  0\\n 41 42  1  0\\n 42 43  2  0\\n 42 44  1  0\\n 44 45  2  0\\n 45 46  1  0\\n 46 47  2  0\\n 47 48  1  0\\n 48 49  2  0\\n 40 50  1  0\\n 50 51  2  0\\n 51 52  1  0\\n 52 53  2  0\\n 53 54  1  0\\n 54 55  1  0\\n 54 56  2  0\\n 30 57  1  0\\n 57 58  1  0\\n 57 59  1  0\\n 20  8  1  0\\n 29 24  1  0\\n 49 44  1  0\\n 56 50  1  0\\n  1 61  1  0\\n  1 62  1  0\\n  1 63  1  0\\n  5 64  1  1\\n  9 65  1  0\\n  9 66  1  0\\n  9 67  1  0\\n 10 68  1  6\\n 11 69  1  0\\n 12 70  1  0\\n 12 71  1  0\\n 13 72  1  0\\n 13 73  1  0\\n 15 74  1  0\\n 15 75  1  0\\n 15 76  1  0\\n 18 77  1  0\\n 18 78  1  0\\n 18 79  1  0\\n 20 80  1  6\\n 21 81  1  0\\n 21 82  1  0\\n 23 83  1  0\\n 23 84  1  0\\n 25 85  1  0\\n 26 86  1  0\\n 27 87  1  0\\n 28 88  1  0\\n 29 89  1  0\\n 32 90  1  0\\n 32 91  1  0\\n 32 92  1  0\\n 33 93  1  1\\n 34 94  1  0\\n 34 95  1  0\\n 34 96  1  0\\n 38 97  1  6\\n 39 98  1  0\\n 40 99  1  1\\n 41100  1  0\\n 45101  1  0\\n 46102  1  0\\n 47103  1  0\\n 48104  1  0\\n 49105  1  0\\n 51106  1  0\\n 52107  1  0\\n 53108  1  0\\n 55109  1  0\\n 55110  1  0\\n 55111  1  0\\n 56112  1  0\\n 57113  1  0\\n 58114  1  0\\n 58115  1  0\\n 58116  1  0\\n 59117  1  0\\n 59118  1  0\\n 59119  1  0\\n 60120  1  0\\n 60121  1  0\\nM  END\\n\",\"sdf\");\n\tviewer_17395713105415018.setStyle({\"stick\": {}});\n\tviewer_17395713105415018.setBackgroundColor(\"0xeeeeee\");\n\tviewer_17395713105415018.zoomTo();\nviewer_17395713105415018.render();\n});\n</script></div>"
          }
        },
        "3f03b66ffe704ec1aeed52c2d69ba823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e24785c055464f5987eeb42d3b76d05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e182e18bfd324945920e0e1e4392d16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfe4c1af3d884dc2b2d7bfe49c4f0f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0012579288c46d2bfbaf9d72d874d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshayAbraham/LigandXplorer_bioinformatics/blob/main/LigandXplorer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LigandXplore:** A Computational Pipeline for Early-Stage Drug Discovery"
      ],
      "metadata": {
        "id": "dmzs5ooKlA6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "Drug discovery is a complex and resource-intensive process that requires the identification and optimization of potential therapeutic molecules. LigandXplore is an innovative computational tool designed to assist researchers in the early-stage filtering of drug candidates by ranking, optimizing, and analyzing ligand structures.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Druglikeness Prediction:**  Assessing the potential of a molecule to become a successful drug using established rules like Lipinski's Rule of Five and Veber's criteria, as well as predicting key ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) properties.\n",
        "* **Molecular Optimization:** Employing a genetic algorithm to iteratively improve a molecule's structure, aiming to enhance its druglikeness and predicted ADMET profile.  This optimization process can be tailored to focus on specific properties or target interactions.\n",
        "* **Interactive Visualization:** Generating interactive 3D visualizations of molecules with property maps, allowing researchers to explore the spatial distribution of key characteristics like lipophilicity and charge. This facilitates a deeper understanding of structure-property relationships.\n",
        "* **Comparative Analysis:**  Comparing the properties and druglikeness scores of optimized molecules with commercially available drugs, providing valuable context and insights into their potential.\n",
        "\n",
        "By integrating computational chemistry techniques with predictive modeling, LigandXplore provides a streamlined workflow for rational drug design, helping researchers make data-driven decisions faster and more efficiently.\n",
        "\n",
        "Let’s get started with the ligand ranking process! 🚀"
      ],
      "metadata": {
        "id": "uycoaJvzlfYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Installing Required Libraries**"
      ],
      "metadata": {
        "id": "pGR8E0W1m9r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block installs all the necessary Python libraries required for LigandXplore to function properly. If these libraries are already installed, you may skip this step.\n",
        "\n",
        "**Libraries Included:**\n",
        "\n",
        "\n",
        "*   RDKit – For molecular representations and cheminformatics.\n",
        "*   admet-ai – For ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) predictions.\n",
        "*   pandas – For data handling and analysis.\n",
        "*   tabulate – For displaying structured tables.\n",
        "*   scikit-learn – For machine learning tasks.\n",
        "*   DEAP – For evolutionary optimization and molecule enhancement.\n",
        "*   py3Dmol & ipywidgets – For interactive 3D molecular visualization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lmCnO5h0nKTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install rdkit\n",
        "!pip install admet-ai\n",
        "!pip install tabulate\n",
        "!pip install --upgrade pandas\n",
        "!pip install rdkit admet-ai pandas scikit-learn deap\n",
        "!pip install py3Dmol ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeE2h8MFr4U1",
        "outputId": "4fb143af-ee5e-4f03-f445-4a483dc15e96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.5\n",
            "Collecting admet-ai\n",
            "  Downloading admet_ai-1.3.1.tar.gz (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chemfunc>=1.0.4 (from admet-ai)\n",
            "  Downloading chemfunc-1.0.11-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting chemprop==1.6.1 (from admet-ai)\n",
            "  Downloading chemprop-1.6.1-py3-none-any.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.26.4)\n",
            "Collecting pandas<2.2.0,>=2.0.0 (from admet-ai)\n",
            "  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rdkit>=2023.3.3 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (2024.9.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from admet-ai) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from admet-ai) (4.67.1)\n",
            "Collecting typed-argument-parser>=1.9.0 (from admet-ai)\n",
            "  Downloading typed_argument_parser-1.10.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (0.2.7)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.10.0)\n",
            "Collecting pandas-flavor>=0.2.0 (from chemprop==1.6.1->admet-ai)\n",
            "  Downloading pandas_flavor-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (1.6.1)\n",
            "Requirement already satisfied: sphinx>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (8.1.3)\n",
            "Collecting tensorboardX>=2.0 (from chemprop==1.6.1->admet-ai)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (2.5.1+cu124)\n",
            "Collecting descriptastorus>=2.7.0.5 (from chemfunc>=1.0.4->admet-ai)\n",
            "  Downloading descriptastorus-2.8.0-py3-none-any.whl.metadata (364 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=2.0.0->admet-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=2.0.0->admet-ai) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=2.0.0->admet-ai) (2025.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit>=2023.3.3->admet-ai) (11.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (0.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (24.2)\n",
            "Collecting typing-inspect>=0.7.1 (from typed-argument-parser>=1.9.0->admet-ai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from descriptastorus>=2.7.0.5->chemfunc>=1.0.4->admet-ai) (1.13.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (1.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (3.2.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pandas-flavor>=0.2.0->chemprop==1.6.1->admet-ai) (2025.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->chemprop==1.6.1->admet-ai) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->chemprop==1.6.1->admet-ai) (3.5.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.32.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.0->chemprop==1.6.1->admet-ai) (4.25.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4.0->chemprop==1.6.1->admet-ai)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.7.1->typed-argument-parser>=1.9.0->admet-ai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2025.1.31)\n",
            "Downloading chemprop-1.6.1-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chemfunc-1.0.11-py3-none-any.whl (31 kB)\n",
            "Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typed_argument_parser-1.10.1-py3-none-any.whl (30 kB)\n",
            "Downloading descriptastorus-2.8.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_flavor-0.6.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: admet-ai\n",
            "  Building wheel for admet-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for admet-ai: filename=admet_ai-1.3.1-py3-none-any.whl size=16957736 sha256=a4f197dcacb398af0a7bb6362e8dfecd83a1c190c64e5cb907fb14f07cade1d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/40/fc/af04994410fe73834302d05325a749da73deeae6cc85be712b\n",
            "Successfully built admet-ai\n",
            "Installing collected packages: tensorboardX, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, typing-inspect, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, typed-argument-parser, nvidia-cusolver-cu12, pandas-flavor, descriptastorus, chemprop, chemfunc, admet-ai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed admet-ai-1.3.1 chemfunc-1.0.11 chemprop-1.6.1 descriptastorus-2.8.0 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-2.1.4 pandas-flavor-0.6.0 tensorboardX-2.6.2.2 typed-argument-parser-1.10.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "admet-ai 1.3.1 requires pandas<2.2.0,>=2.0.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.5)\n",
            "Requirement already satisfied: admet-ai in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Requirement already satisfied: chemfunc>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.0.11)\n",
            "Requirement already satisfied: chemprop==1.6.1 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.6.1)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from admet-ai) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from admet-ai) (4.67.1)\n",
            "Requirement already satisfied: typed-argument-parser>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.10.1)\n",
            "Requirement already satisfied: flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (0.2.7)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.10.0)\n",
            "Requirement already satisfied: pandas-flavor>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (0.6.0)\n",
            "Requirement already satisfied: sphinx>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (8.1.3)\n",
            "Requirement already satisfied: tensorboardX>=2.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (2.6.2.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (2.5.1+cu124)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: descriptastorus>=2.7.0.5 in /usr/local/lib/python3.11/dist-packages (from chemfunc>=1.0.4->admet-ai) (2.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (0.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (24.2)\n",
            "Requirement already satisfied: typing-inspect>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (0.9.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (1.9.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (3.2.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pandas-flavor>=0.2.0->chemprop==1.6.1->admet-ai) (2025.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.32.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.0->chemprop==1.6.1->admet-ai) (4.25.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.7.1->typed-argument-parser>=1.9.0->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2025.1.31)\n",
            "Using cached pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m828.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap, pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deap-1.4.2 pandas-2.1.4\n",
            "Collecting py3Dmol\n",
            "  Downloading py3Dmol-2.4.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading py3Dmol-2.4.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py3Dmol, jedi\n",
            "Successfully installed jedi-0.19.2 py3Dmol-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Uploading Ligand Files**"
      ],
      "metadata": {
        "id": "AsEjh9ZcScgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block allows users to upload their ligand files (.sdf format) to Google Colab for further processing. The uploaded files will be automatically moved to a designated folder (/content/ligands) to keep the workspace organized.\n",
        "\n",
        "***Ensure that your ligand file is in the correct format (.sdf) before uploading.***"
      ],
      "metadata": {
        "id": "p9t7WhzpopyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create the folder where you want to save the .sdf file\n",
        "folder_path = '/content/ligands'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the uploaded file to the specified folder\n",
        "for filename in uploaded.keys():\n",
        "    # Define the full path where you want to save the file\n",
        "    destination = os.path.join(folder_path, filename)\n",
        "    os.rename(filename, destination)\n",
        "\n",
        "    print(f\"File saved to: {destination}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "Ndba1EIVc-WO",
        "outputId": "919ecdb2-c5e6-4b09-b73f-c18c913f8dd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f6a3862-d93b-4b0a-af2f-12b6300db57d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5f6a3862-d93b-4b0a-af2f-12b6300db57d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Structure2D_COMPOUND_CID_36314.sdf to Structure2D_COMPOUND_CID_36314.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_3672.sdf to Conformer3D_COMPOUND_CID_3672.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_237.sdf to Conformer3D_COMPOUND_CID_237.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_162394460 - Copy.sdf to Conformer3D_COMPOUND_CID_162394460 - Copy.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_146502750.sdf to Conformer3D_COMPOUND_CID_146502750.sdf\n",
            "Saving Structure2D_COMPOUND_CID_392622.sdf to Structure2D_COMPOUND_CID_392622.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_5757.sdf to Conformer3D_COMPOUND_CID_5757.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_2153.sdf to Conformer3D_COMPOUND_CID_2153.sdf\n",
            "Saving Conformer3D_COMPOUND_CID_2244.sdf to Conformer3D_COMPOUND_CID_2244.sdf\n",
            "Saving ligand_2153.sdf to ligand_2153.sdf\n",
            "File saved to: /content/ligands/Structure2D_COMPOUND_CID_36314.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_3672.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_237.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_162394460 - Copy.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_146502750.sdf\n",
            "File saved to: /content/ligands/Structure2D_COMPOUND_CID_392622.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_5757.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_2153.sdf\n",
            "File saved to: /content/ligands/Conformer3D_COMPOUND_CID_2244.sdf\n",
            "File saved to: /content/ligands/ligand_2153.sdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Displaying Physicochemical & ADMET Properties of Ligands**"
      ],
      "metadata": {
        "id": "oZeOhU9XltQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section processes molecular data from SDF files, calculates key physicochemical properties, predicts ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity) properties and are displayed in a structured table and saved as a CSV file for further analysis."
      ],
      "metadata": {
        "id": "TQ-AByUoq0WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from admet_ai import ADMETModel\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define the folder containing SDF files\n",
        "ligands_folder = \"/content/ligands\"\n",
        "\n",
        "# Function to calculate molecular descriptors\n",
        "def calculate_descriptors(mol):\n",
        "    descriptors = {\n",
        "        'Molecular Weight': Descriptors.MolWt(mol),\n",
        "        'LogP': Descriptors.MolLogP(mol),\n",
        "        'HBD': Descriptors.NumHDonors(mol),\n",
        "        'HBA': Descriptors.NumHAcceptors(mol),\n",
        "        'Rotatable Bonds': Descriptors.NumRotatableBonds(mol),\n",
        "        'Polar Surface Area': Descriptors.TPSA(mol)\n",
        "    }\n",
        "    return descriptors\n",
        "\n",
        "# Function to apply Lipinski's Rule of 5\n",
        "def lipinski_rules(descriptors):\n",
        "    return sum([\n",
        "        descriptors['Molecular Weight'] <= 500,\n",
        "        descriptors['LogP'] <= 5,\n",
        "        descriptors['HBD'] <= 5,\n",
        "        descriptors['HBA'] <= 10\n",
        "    ])\n",
        "\n",
        "# Function to apply Veber's Rule\n",
        "def veber_rules(descriptors):\n",
        "    return sum([\n",
        "        descriptors['Rotatable Bonds'] <= 10,\n",
        "        descriptors['Polar Surface Area'] <= 140\n",
        "    ])\n",
        "\n",
        "# Function to process all SDF files in the folder\n",
        "def process_sdf_folder(folder_path):\n",
        "    all_molecules = []\n",
        "\n",
        "    # Loop through all .sdf files in the directory\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".sdf\"):\n",
        "            sdf_path = os.path.join(folder_path, filename)\n",
        "            supplier = Chem.SDMolSupplier(sdf_path)\n",
        "\n",
        "            for mol in supplier:\n",
        "                if mol is not None:  # Skip invalid molecules\n",
        "                    mol_name = mol.GetProp(\"_Name\") if mol.HasProp(\"_Name\") else \"Unknown\"\n",
        "                    pubchem_id = mol.GetProp(\"PUBCHEM_COMPOUND_CID\") if mol.HasProp(\"PUBCHEM_COMPOUND_CID\") else \"N/A\"\n",
        "\n",
        "                    # Get descriptors\n",
        "                    descriptors = calculate_descriptors(mol)\n",
        "\n",
        "                    # Apply Lipinski and Veber's rules\n",
        "                    lipinski_score = lipinski_rules(descriptors)\n",
        "                    veber_score = veber_rules(descriptors)\n",
        "\n",
        "                    # Store molecule data\n",
        "                    all_molecules.append({\n",
        "                        'Molecule Name': mol_name,\n",
        "                        'PubChem ID': pubchem_id,\n",
        "                        'Molecular Weight': descriptors['Molecular Weight'],\n",
        "                        'LogP': descriptors['LogP'],\n",
        "                        'HBD': descriptors['HBD'],\n",
        "                        'HBA': descriptors['HBA'],\n",
        "                        'Rotatable Bonds': descriptors['Rotatable Bonds'],\n",
        "                        'Polar Surface Area': descriptors['Polar Surface Area'],\n",
        "                        'Lipinski Score': lipinski_score,\n",
        "                        'Veber Score': veber_score,\n",
        "                        'Total Drug-Like Score': lipinski_score + veber_score,\n",
        "                        'SMILES': Chem.MolToSmiles(mol)  # Extract SMILES from the molecule\n",
        "                    })\n",
        "\n",
        "    return all_molecules\n",
        "\n",
        "# Function to predict all ADMET properties\n",
        "def predict_all_admet_properties(smiles_list):\n",
        "    model = ADMETModel()\n",
        "    preds = model.predict(smiles=smiles_list)\n",
        "\n",
        "    # Insert the SMILES column for reference\n",
        "    preds.insert(0, \"SMILES\", smiles_list)\n",
        "    return preds\n",
        "\n",
        "# Process the ligands folder and extract molecules\n",
        "molecules_data = process_sdf_folder(ligands_folder)\n",
        "\n",
        "# Convert molecule data into a DataFrame\n",
        "df_drug_like = pd.DataFrame(molecules_data)\n",
        "\n",
        "# Extract SMILES strings for ADMET prediction\n",
        "smiles_list = df_drug_like['SMILES'].tolist()\n",
        "\n",
        "# Predict ADMET properties\n",
        "df_admet = predict_all_admet_properties(smiles_list)\n",
        "\n",
        "# Merge drug-likeness and ADMET data\n",
        "df_combined = pd.merge(df_drug_like, df_admet, on=\"SMILES\", how=\"inner\")\n",
        "\n",
        "# Sort molecules based on 'Total Drug-Like Score'\n",
        "df_sorted = df_combined.sort_values(by=['Total Drug-Like Score'], ascending=False)\n",
        "\n",
        "# Save the sorted DataFrame as a CSV file\n",
        "csv_filename = \"/content/ADMETProperties_molecules.csv\"\n",
        "df_sorted.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Display the results as a table\n",
        "print(\"\\nRanked Molecules Based on Drug-Likeness and ADMET Properties:\")\n",
        "print(tabulate(df_sorted, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Print success message\n",
        "print(f\"\\n✅ Results have been saved as a CSV file: {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9c8BXCp2uVP",
        "outputId": "00c3146e-0537-4eaf-db12-54ee7698c0ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 10/10 [00:00<00:00, 77672.30it/s]\n",
            "Computing physchem properties: 100%|██████████| 10/10 [00:00<00:00, 447.88it/s]\n",
            "RDKit fingerprints: 100%|██████████| 10/10 [00:00<00:00, 18.66it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.33it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.68it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.62it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 22.02it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 21.22it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ranked Molecules Based on Drug-Likeness and ADMET Properties:\n",
            "+---------------+------------+--------------------+--------------------+-----+-----+-----------------+--------------------+----------------+-------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------+----------------------+----------+---------------------+----------------+--------------------+-----------------------+---------------------+--------------------+----------------------+----------------------+--------------------------------+----------------------+--------------------------------+-----------------------+--------------------------------+------------------------+---------------------+-----------------------+---------------------+--------------------+------------------------+----------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+-----------------------+---------------------+-----------------------+---------------------+-------------------------+------------------------+---------------------+------------------------------+--------------------+---------------------------+--------------------+---------------------+---------------------+-----------------------------------------------+-----------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------+----------------------------------+---------------------------------------------+-----------------------------------+-----------------------------------+------------------------------------------+-------------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+--------------------------------------------------+--------------------------------------+-----------------------------------+--------------------------------------+----------------------------------------+------------------------------------+-------------------------------------+-------------------------------------------+----------------------------------------+------------------------------------+--------------------------------------------+------------------------------------------+----------------------------------------------+-------------------------------------+---------------------------------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+------------------------------------------------------+-----------------------------------------------------+----------------------------------------------+-----------------------------------------------------------+---------------------------------------+--------------------------------------------------------+--------------------------------------+-------------------------------------------------+--------------------------------------------+\n",
            "| Molecule Name | PubChem ID |  Molecular Weight  |        LogP        | HBD | HBA | Rotatable Bonds | Polar Surface Area | Lipinski Score | Veber Score | Total Drug-Like Score |                                                                              SMILES                                                                              |  molecular_weight  |        logP         | hydrogen_bond_acceptors | hydrogen_bond_donors | Lipinski |         QED         | stereo_centers |        tpsa        |         AMES          |     BBB_Martins     | Bioavailability_Ma |     CYP1A2_Veith     |    CYP2C19_Veith     | CYP2C9_Substrate_CarbonMangels |     CYP2C9_Veith     | CYP2D6_Substrate_CarbonMangels |     CYP2D6_Veith      | CYP3A4_Substrate_CarbonMangels |      CYP3A4_Veith      | Carcinogens_Lagunin |        ClinTox        |        DILI         |      HIA_Hou       |       NR-AR-LBD        |        NR-AR         |        NR-AhR         |     NR-Aromatase      |       NR-ER-LBD       |        NR-ER         |     NR-PPAR-gamma     |     PAMPA_NCATS     |    Pgp_Broccatelli    |        SR-ARE        |       SR-ATAD5        |        SR-HSE        |        SR-MMP        |        SR-p53         |    Skin_Reaction    |         hERG          |     Caco2_Wang      | Clearance_Hepatocyte_AZ | Clearance_Microsome_AZ |   Half_Life_Obach   | HydrationFreeEnergy_FreeSolv |      LD50_Zhu      | Lipophilicity_AstraZeneca |      PPBR_AZ       | Solubility_AqSolDB  |    VDss_Lombardo    | molecular_weight_drugbank_approved_percentile | logP_drugbank_approved_percentile | hydrogen_bond_acceptors_drugbank_approved_percentile | hydrogen_bond_donors_drugbank_approved_percentile | Lipinski_drugbank_approved_percentile | QED_drugbank_approved_percentile | stereo_centers_drugbank_approved_percentile | tpsa_drugbank_approved_percentile | AMES_drugbank_approved_percentile | BBB_Martins_drugbank_approved_percentile | Bioavailability_Ma_drugbank_approved_percentile | CYP1A2_Veith_drugbank_approved_percentile | CYP2C19_Veith_drugbank_approved_percentile | CYP2C9_Substrate_CarbonMangels_drugbank_approved_percentile | CYP2C9_Veith_drugbank_approved_percentile | CYP2D6_Substrate_CarbonMangels_drugbank_approved_percentile | CYP2D6_Veith_drugbank_approved_percentile | CYP3A4_Substrate_CarbonMangels_drugbank_approved_percentile | CYP3A4_Veith_drugbank_approved_percentile | Carcinogens_Lagunin_drugbank_approved_percentile | ClinTox_drugbank_approved_percentile | DILI_drugbank_approved_percentile | HIA_Hou_drugbank_approved_percentile | NR-AR-LBD_drugbank_approved_percentile | NR-AR_drugbank_approved_percentile | NR-AhR_drugbank_approved_percentile | NR-Aromatase_drugbank_approved_percentile | NR-ER-LBD_drugbank_approved_percentile | NR-ER_drugbank_approved_percentile | NR-PPAR-gamma_drugbank_approved_percentile | PAMPA_NCATS_drugbank_approved_percentile | Pgp_Broccatelli_drugbank_approved_percentile | SR-ARE_drugbank_approved_percentile | SR-ATAD5_drugbank_approved_percentile | SR-HSE_drugbank_approved_percentile | SR-MMP_drugbank_approved_percentile | SR-p53_drugbank_approved_percentile | Skin_Reaction_drugbank_approved_percentile | hERG_drugbank_approved_percentile | Caco2_Wang_drugbank_approved_percentile | Clearance_Hepatocyte_AZ_drugbank_approved_percentile | Clearance_Microsome_AZ_drugbank_approved_percentile | Half_Life_Obach_drugbank_approved_percentile | HydrationFreeEnergy_FreeSolv_drugbank_approved_percentile | LD50_Zhu_drugbank_approved_percentile | Lipophilicity_AstraZeneca_drugbank_approved_percentile | PPBR_AZ_drugbank_approved_percentile | Solubility_AqSolDB_drugbank_approved_percentile | VDss_Lombardo_drugbank_approved_percentile |\n",
            "+---------------+------------+--------------------+--------------------+-----+-----+-----------------+--------------------+----------------+-------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------+----------------------+----------+---------------------+----------------+--------------------+-----------------------+---------------------+--------------------+----------------------+----------------------+--------------------------------+----------------------+--------------------------------+-----------------------+--------------------------------+------------------------+---------------------+-----------------------+---------------------+--------------------+------------------------+----------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+-----------------------+---------------------+-----------------------+---------------------+-------------------------+------------------------+---------------------+------------------------------+--------------------+---------------------------+--------------------+---------------------+---------------------+-----------------------------------------------+-----------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------+----------------------------------+---------------------------------------------+-----------------------------------+-----------------------------------+------------------------------------------+-------------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+--------------------------------------------------+--------------------------------------+-----------------------------------+--------------------------------------+----------------------------------------+------------------------------------+-------------------------------------+-------------------------------------------+----------------------------------------+------------------------------------+--------------------------------------------+------------------------------------------+----------------------------------------------+-------------------------------------+---------------------------------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+------------------------------------------------------+-----------------------------------------------------+----------------------------------------------+-----------------------------------------------------------+---------------------------------------+--------------------------------------------------------+--------------------------------------+-------------------------------------------------+--------------------------------------------+\n",
            "|   162394460   | 162394460  | 398.68800000000016 |       2.6714       |  4  |  4  |        6        |       87.38        |       4        |      2      |           6           |                                                          NC[C@@H](C(=O)NNCc1ccc(O)c(Br)c1)c1cccc(Cl)c1                                                           | 398.6880000000001  |  2.671400000000001  |            4            |          4           |   4.0    | 0.5634773769896697  |       1        | 87.38000000000001  |  0.7363239169120789   | 0.34805492162704466 | 0.9237998485565185 |  0.8150797963142395  |  0.7971238851547241  |      0.21525557041168214       | 0.40780357718467714  |       0.2744558334350586       |   0.812786316871643   |       0.5858330011367798       |   0.9601204633712769   | 0.6930452466011048  |  0.29779587090015414  | 0.2888411432504654  | 0.9994607448577881 |  0.010928187752142549  | 0.01478489749133587  |  0.5773596405982971   |  0.12855832278728485  |  0.06473740190267563  | 0.11669883280992507  | 0.021621549129486085  | 0.6722888708114624  |  0.05037892684340477  |  0.2873216956853867  |  0.01727471752092242  | 0.18326676338911058  | 0.41618559658527376  |  0.09918585419654846  |  0.694032096862793  |  0.5572569787502288   | -5.520491914519063  |   25.472926189315455    |   6.854251564624417    | 19.682228027140763  |     -10.595015312167329      | 2.8342429899303543 |    1.9757059077614592     | 88.33957024341315  | -3.1282841442473406 |  4.749792568234403  |                65.141527723924                |         56.72741372625048         |                  46.47150058162078                   |                 86.02171384257464                 |           63.80379992245056           |         56.3396665374176         |              54.80806514152772              |        58.937572702597905         |         93.21442419542458         |            20.860798759208997            |                81.46568437378829                |             92.55525397440869             |             94.18379216750678              |                       65.141527723924                       |             87.01046917409849             |                      72.50872431174874                      |             93.4858472276076              |                     58.898797983714616                      |             97.94493989918573             |                95.50213260953858                 |          76.15354788677782           |         39.31756494765413         |          72.23730127956573           |           50.562233423807676           |          35.5176424970919          |          94.33889104303994          |             77.89841023652578             |           81.85343156262117            |         62.19464908879411          |             72.39240015509887              |             44.280728964715              |              42.69096549050019               |          70.33734005428461          |           65.60682435052345           |          91.46956184567662          |          82.47382706475378          |          75.45560294687863          |             75.10663047692904              |         66.92516479255525         |           22.528111671190384            |                  38.58084528887166                   |                 36.991081814656845                  |              66.84761535478867               |                     40.67468010856921                     |           69.91081814656843           |                   59.557968204730514                   |          70.29856533540132           |                48.23575029081039                |             73.12911981388135              |\n",
            "|     5757      |    5757    |      272.388       | 3.609200000000003  |  2  |  2  |        0        |       40.46        |       4        |      2      |           6           |                                                       C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C@@H]2O                                                       | 272.3879999999999  | 3.6092000000000026  |            2            |          2           |   4.0    |  0.757169743555686  |       5        |       40.46        |  0.03592464253306389  | 0.06576163275167346 | 0.5407576322555542 |  0.6042723417282104  | 0.25900353491306305  |       0.5124844819307327       | 0.11391125619411469  |       0.2891750052571297       | 0.011296311486512422  |       0.8047420978546143       |   0.0391154982149601   | 0.04582204216690115 | 0.0033268909610342234 | 0.03261766778305173 | 0.9995497584342956 |   0.8774696111679077   |  0.8780086636543274  |  0.01724497452378273  |  0.23916601985692978  |  0.9820187091827393   |  0.9820738792419433  | 0.015706560853868724  | 0.9472478747367858  |  0.08396487329155207  | 0.38552348017692567  |  0.02219689227640629  |  0.0809349600225687  |  0.6863088846206665  |  0.4832902193069458   | 0.5416280329227448  |  0.2687480509281158   | -4.969755041288141  |    86.40360194458005    |   36.570177169072274   | -4.861958402147683  |      -7.989803447014729      | 2.0670937227746373 |    3.3168290967651672     | 87.26012968611175  | -4.531254725191634  | 3.3970344561327854  |               32.80341217526173               |         71.38425746413338         |                  16.38231872818922                   |                 60.85692128732067                 |           63.80379992245056           |        83.09422256688639         |              83.96665374176037              |        23.070957735556416         |        14.695618456766187         |            3.8386971694455214            |               21.713842574641333                |             86.19620007754943             |             69.44552151996898              |                      93.87359441644048                      |              67.429236138038              |                      73.98216362931369                      |             26.7157813105855              |                      88.32880961613029                      |            44.125630089181854             |                18.611865063978286                |          6.397828615742536           |         8.491663435440092         |          73.98216362931369           |            98.0612640558356            |         95.96742923613803          |          44.31950368359829          |             87.86351298953082             |           99.96122528111671            |         99.84490112446684          |             67.70065917022102              |            78.86777820860799             |              49.32144241954246               |          77.27801473439317          |           69.56184567661884           |          81.96975571927103          |          92.28383094222566          |          95.61845676618844          |             61.845676618844514             |         48.58472276075998         |           50.756107018224114            |                  85.53702985653354                   |                  67.85575804575417                  |              27.84024815820085               |                     66.4598681659558                      |          22.954633578906552           |                    86.9716944552152                    |          68.16595579682047           |               21.636293136874755                |              64.1721597518418              |\n",
            "|   146502750   | 146502750  | 425.46700000000016 | 4.228580000000004  |  0  |  6  |        2        |       92.42        |       4        |      2      |           6           |                                              C[C@H]1C(=O)C(C#N)=C[C@@]2(C)c3nc(-c4ccnnc4)nc(-c4ccccc4F)c3CC[C@H]12                                               | 425.4670000000002  |  4.228580000000004  |            6            |          0           |   4.0    | 0.6101190507120148  |       3        |       92.42        |  0.40264029800891876  | 0.9247079610824585  | 0.9086232423782349 | 0.47395234405994413  | 0.47111861407756805  |      0.14665960147976875       |  0.774853527545929   |      0.07600068859755993       | 0.029151789657771588  |       0.7489179849624634       |   0.8348206996917724   | 0.09033608064055443 |  0.2105935588479042   | 0.9031418085098266  | 0.9999890565872193 |  0.12577787414193153   | 0.09203146547079086  |  0.5057624816894531   |   0.80758615732193    |  0.0569473072886467   |  0.0791811227798462  |  0.09561691880226135  | 0.9673173666000366  |  0.9127960920333862   |  0.7601207017898559  |  0.3590850651264191   |  0.0845750130712986  |  0.7301181554794312  |  0.4176527261734009   | 0.24050483256578445 |  0.7964029908180237   | -4.608937931772713  |    55.95678606596637    |   51.99338455754862    | -2.8273004908131525 |      -8.176159887814595      | 3.6467069485572745 |    3.5611487094045473     | 95.53415928612779  |  -5.70728095214937  | -1.7303338497019596 |               70.72508724311749               |         80.69018999612253         |                   69.5036835982939                   |                11.768127181077936                 |           63.80379992245056           |        61.65180302442807         |              75.43621558743699              |         62.40791004265219         |         76.38619620007755         |             71.0352849941838             |                76.1923226056611                 |             82.35750290810391             |             82.78402481582009              |                      49.74796432725863                      |             96.20007754943776             |                      41.99302055060101                      |            41.721597518417994             |                      80.34121752617293                      |             92.05118262892594             |                34.58704924389298                 |           67.7782086079876           |         81.62078324932143         |          96.54905001938735           |           91.81853431562621            |         85.45948041876696          |          92.98177588212485          |             99.37960449786739             |           79.25552539744086            |         44.82357502908104          |             90.69406746801086              |            86.39007367196588             |              94.64908879410623               |          94.7654129507561           |           97.51841799146956           |          82.97789841023652          |          93.05932531989143          |          93.95114385420706          |             27.025979061651803             |         78.09228383094222         |            73.2842186894145             |                  67.23536254362156                   |                  78.40248158200853                  |              31.36874757658007               |                     64.67623109732455                     |           94.95928654517255           |                   90.61651803024428                    |          83.59829391236913           |                7.289647150058162                |             23.497479643272584             |\n",
            "|     3672      |    3672    |      206.285       | 3.0732000000000017 |  1  |  1  |        4        |        37.3        |       4        |      2      |           6           |                                                                 CC(C)Cc1ccc([C@@H](C)C(=O)O)cc1                                                                  | 206.28499999999997 |  3.073200000000001  |            1            |          1           |   4.0    | 0.8215995486924976  |       1        |        37.3        | 0.0062664449913427235 | 0.5324256420135498  | 0.9444472432136536 | 0.00468623322667554  | 0.04953954014927149  |       0.6704423904418946       | 0.05575857423245907  |      0.04805629327893257       | 0.007138351025059819  |       0.2663993388414383       |  0.002299748035147786  | 0.4789010167121887  | 0.015424265712499618  | 0.5179961085319519  | 0.9987566113471985 | 0.00029402488435152917 | 0.00712310248054564  | 0.004825279163196683  | 0.0017083241371437907 |  0.01757029090076685  | 0.12881022840738296  |  0.05553418770432472  |  0.593759286403656  | 0.0042208576516713945 | 0.011128368228673935 | 0.0009551323630148545 | 0.005552589148283005 | 0.030205438286066054 | 0.000736696389503777  |  0.365418004989624  | 0.009122536052018404  | -4.301729852050817  |    22.82005581182144    |   0.5674011453972995   |  8.569264415183989  |      -6.987556303069178      | 2.3579548546073235 |    0.8804752670649784     |  90.3542825781805  | -2.972549609732491  | 1.0516209977128805  |              18.456766188445133               |        62.756882512601784         |                  6.649864288483908                   |                 36.68088406359054                 |           63.80379992245056           |        91.70221015897634         |              54.80806514152772              |        19.852656068243505         |        2.3264831329972857         |            32.72586273749515             |                87.66963939511439                |            25.048468398604108             |             37.92167506785576              |                      98.41023652578518                      |             55.60294687863513             |                      32.60953858084529                      |            20.860798759208997             |                      29.89530825901512                      |            22.489336952307095             |                87.55331523846452                 |          15.548662272198527          |        54.788677782086076         |          63.629313687475765          |           4.497867390461419            |         18.650639782861575         |         25.048468398604108          |             19.5424583171772              |           49.360217138425746           |         65.68437378829003          |             85.07173322993408              |            39.12369135323769             |              13.803799922450562              |         11.128344319503682          |          22.954633578906552           |          32.49321442419542          |         47.537805350911206          |          10.35284994183792          |             43.505234587049245             |         7.716169057774331         |            91.31446297014347            |                  35.63396665374176                   |                 29.623885226832105                  |              52.03567274137262               |                     75.10663047692904                     |          39.395114385420705           |                    40.4032570763862                    |          74.21481194261341           |                51.26017836370686                |             45.56029468786351              |\n",
            "|     2153      |    2153    |      180.167       |      -1.0397       |  1  |  5  |        0        |       72.68        |       4        |      2      |           6           |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 180.16699999999997 | -1.0397000000000005 |            5            |          1           |   4.0    | 0.5624722357827983  |       0        |       72.68        |  0.19952020198106765  | 0.8009407162666321  | 0.9000226497650147 |  0.2022198259830475  | 0.003487794508691877 |      0.40416358709335326       | 0.003984411002602428 |      0.22379742413759232       | 0.0012955602767760865 |       0.3405459553003311       |  0.004445872397627682  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  | 0.9993916034698487 |  0.004693216504529118  | 0.01481268610805273  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.47237719893455504 |  0.08504921197891235  | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 | 0.009809199720621109  |  0.664505398273468  |  0.11998217403888703  |  -4.60291462478323  |   -0.5898503502208925   |  0.40092364526881286   | 1.3860505190305836  |      -16.47621426203117      | 2.8962771344050946 |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | -0.8910617458021803 |              14.346645986816595               |         15.27723924001551         |                   59.1120589375727                   |                 36.68088406359054                 |           63.80379992245056           |        56.14579294300116         |             22.489336952307095              |         47.63474214811942         |         52.03567274137262         |            55.75804575416828             |                73.32299340829779                |             69.94959286545172             |             6.940674680108569              |                      87.78596355176424                      |            20.240403257076387             |                      68.51492826677007                      |             8.065141527723924             |                     35.827840248158196                      |            26.560682435052346             |                20.628150445909267                |          50.25203567274137           |         84.02481582008531         |           71.0352849941838           |           30.670802636680882           |         35.55641721597518          |          63.35789065529275          |             19.5424583171772              |           17.797595967429235           |         19.852656068243505         |             21.907716169057775             |            32.80341217526173             |              49.476541295075606              |         14.424195424583171          |          57.270259790616514           |          45.40519581233036          |         15.432338115548662          |          37.4563784412563           |             72.39240015509887              |         32.60953858084529         |            73.90461419154711            |                  14.695618456766187                  |                  29.27491275688251                  |               39.7440868553703               |                     5.971306708026367                     |           73.40054284606437           |                   24.00155098875533                    |          18.262892594028692          |                75.76580069794494                |             29.003489724699495             |\n",
            "|     2153      |    2153    |      180.167       |      -1.0397       |  1  |  5  |        0        |       72.68        |       4        |      2      |           6           |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 180.16699999999997 | -1.0397000000000005 |            5            |          1           |   4.0    | 0.5624722357827983  |       0        |       72.68        |  0.19952020198106765  | 0.8009407162666321  | 0.9000226497650147 |  0.2022198259830475  | 0.003487794508691877 |      0.40416358709335326       | 0.003984411002602428 |      0.22379742413759232       | 0.0012955602767760865 |       0.3405459553003311       |  0.004445872397627682  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  | 0.9993916034698487 |  0.004693216504529118  | 0.01481268610805273  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.47237719893455504 |  0.08504921197891235  | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 | 0.009809199720621109  |  0.664505398273468  |  0.11998217403888703  |  -4.60291462478323  |   -0.5898503502208925   |  0.40092364526881286   | 1.3860505190305836  |      -16.47621426203117      | 2.8962771344050946 |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | -0.8910617458021803 |              14.346645986816595               |         15.27723924001551         |                   59.1120589375727                   |                 36.68088406359054                 |           63.80379992245056           |        56.14579294300116         |             22.489336952307095              |         47.63474214811942         |         52.03567274137262         |            55.75804575416828             |                73.32299340829779                |             69.94959286545172             |             6.940674680108569              |                      87.78596355176424                      |            20.240403257076387             |                      68.51492826677007                      |             8.065141527723924             |                     35.827840248158196                      |            26.560682435052346             |                20.628150445909267                |          50.25203567274137           |         84.02481582008531         |           71.0352849941838           |           30.670802636680882           |         35.55641721597518          |          63.35789065529275          |             19.5424583171772              |           17.797595967429235           |         19.852656068243505         |             21.907716169057775             |            32.80341217526173             |              49.476541295075606              |         14.424195424583171          |          57.270259790616514           |          45.40519581233036          |         15.432338115548662          |          37.4563784412563           |             72.39240015509887              |         32.60953858084529         |            73.90461419154711            |                  14.695618456766187                  |                  29.27491275688251                  |               39.7440868553703               |                     5.971306708026367                     |           73.40054284606437           |                   24.00155098875533                    |          18.262892594028692          |                75.76580069794494                |             29.003489724699495             |\n",
            "|     2153      |    2153    |      180.167       |      -1.0397       |  1  |  5  |        0        |       72.68        |       4        |      2      |           6           |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 180.16699999999997 | -1.0397000000000005 |            5            |          1           |   4.0    | 0.5624722357827983  |       0        |       72.68        |  0.19952020198106765  | 0.8009407162666321  | 0.9000226497650147 |  0.2022198259830475  | 0.003487794508691877 |      0.40416358709335326       | 0.003984411002602428 |      0.22379742413759232       | 0.0012955602767760865 |       0.3405459553003311       |  0.004445872397627682  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  | 0.9993916034698487 |  0.004693216504529118  | 0.01481268610805273  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.47237719893455504 |  0.08504921197891235  | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 | 0.009809199720621109  |  0.664505398273468  |  0.11998217403888703  |  -4.60291462478323  |   -0.5898503502208925   |  0.40092364526881286   | 1.3860505190305836  |      -16.47621426203117      | 2.8962771344050946 |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | -0.8910617458021803 |              14.346645986816595               |         15.27723924001551         |                   59.1120589375727                   |                 36.68088406359054                 |           63.80379992245056           |        56.14579294300116         |             22.489336952307095              |         47.63474214811942         |         52.03567274137262         |            55.75804575416828             |                73.32299340829779                |             69.94959286545172             |             6.940674680108569              |                      87.78596355176424                      |            20.240403257076387             |                      68.51492826677007                      |             8.065141527723924             |                     35.827840248158196                      |            26.560682435052346             |                20.628150445909267                |          50.25203567274137           |         84.02481582008531         |           71.0352849941838           |           30.670802636680882           |         35.55641721597518          |          63.35789065529275          |             19.5424583171772              |           17.797595967429235           |         19.852656068243505         |             21.907716169057775             |            32.80341217526173             |              49.476541295075606              |         14.424195424583171          |          57.270259790616514           |          45.40519581233036          |         15.432338115548662          |          37.4563784412563           |             72.39240015509887              |         32.60953858084529         |            73.90461419154711            |                  14.695618456766187                  |                  29.27491275688251                  |               39.7440868553703               |                     5.971306708026367                     |           73.40054284606437           |                   24.00155098875533                    |          18.262892594028692          |                75.76580069794494                |             29.003489724699495             |\n",
            "|     2153      |    2153    |      180.167       |      -1.0397       |  1  |  5  |        0        |       72.68        |       4        |      2      |           6           |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 180.16699999999997 | -1.0397000000000005 |            5            |          1           |   4.0    | 0.5624722357827983  |       0        |       72.68        |  0.19952020198106765  | 0.8009407162666321  | 0.9000226497650147 |  0.2022198259830475  | 0.003487794508691877 |      0.40416358709335326       | 0.003984411002602428 |      0.22379742413759232       | 0.0012955602767760865 |       0.3405459553003311       |  0.004445872397627682  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  | 0.9993916034698487 |  0.004693216504529118  | 0.01481268610805273  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.47237719893455504 |  0.08504921197891235  | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 | 0.009809199720621109  |  0.664505398273468  |  0.11998217403888703  |  -4.60291462478323  |   -0.5898503502208925   |  0.40092364526881286   | 1.3860505190305836  |      -16.47621426203117      | 2.8962771344050946 |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | -0.8910617458021803 |              14.346645986816595               |         15.27723924001551         |                   59.1120589375727                   |                 36.68088406359054                 |           63.80379992245056           |        56.14579294300116         |             22.489336952307095              |         47.63474214811942         |         52.03567274137262         |            55.75804575416828             |                73.32299340829779                |             69.94959286545172             |             6.940674680108569              |                      87.78596355176424                      |            20.240403257076387             |                      68.51492826677007                      |             8.065141527723924             |                     35.827840248158196                      |            26.560682435052346             |                20.628150445909267                |          50.25203567274137           |         84.02481582008531         |           71.0352849941838           |           30.670802636680882           |         35.55641721597518          |          63.35789065529275          |             19.5424583171772              |           17.797595967429235           |         19.852656068243505         |             21.907716169057775             |            32.80341217526173             |              49.476541295075606              |         14.424195424583171          |          57.270259790616514           |          45.40519581233036          |         15.432338115548662          |          37.4563784412563           |             72.39240015509887              |         32.60953858084529         |            73.90461419154711            |                  14.695618456766187                  |                  29.27491275688251                  |               39.7440868553703               |                     5.971306708026367                     |           73.40054284606437           |                   24.00155098875533                    |          18.262892594028692          |                75.76580069794494                |             29.003489724699495             |\n",
            "|     2244      |    2244    | 180.15899999999996 |       1.3101       |  1  |  3  |        2        |        63.6        |       4        |      2      |           6           |                                                                      CC(=O)Oc1ccccc1C(=O)O                                                                       | 180.15899999999996 |       1.3101        |            3            |          1           |   4.0    | 0.5501217966938848  |       0        | 63.60000000000001  |  0.05085575878620148  | 0.8109905004501343  | 0.8728528618812561 | 0.015933019947260617 | 0.012031708657741547 |      0.36843977123498917       | 0.03166418168693781  |      0.025489537790417672      | 0.0069421239662915465 |      0.09314111992716789       | 0.00023585800809087232 | 0.2533529117703438  |  0.13706251606345177  | 0.8547625064849853  | 0.9971104025840759 |  0.002197452075779438  |  0.0230566693469882  | 0.0074217274319380525 | 0.0006330780393909663 | 0.003623103746213019  | 0.029106763750314714 | 0.004367554653435945  | 0.26736666187644004 | 0.008252930385060609  | 0.008828719053417445 | 0.0012725539738312364 | 0.006973763182759285 | 0.001770349603611976 | 0.0019960077945142983 | 0.3067779362201691  | 0.0033732875715941192 | -4.406400557063573  |   22.272858068277007    |   29.698105175080496   | 3.5387509518797153  |      -8.788301506572092      | 2.214422774937376  |    -1.6280061592252302    | 73.18028314796899  | -1.4189941906350367 |  6.688880537362766  |              14.307871267933308               |         36.25436215587437         |                  31.17487398216363                   |                 36.68088406359054                 |           63.80379992245056           |        54.47848003101977         |             22.489336952307095              |         41.74098487785963         |        19.852656068243505         |            56.88251260178364             |                65.83947266382319                |             38.58084528887166             |             18.37921675067856              |                      85.11050794881737                      |            47.576580069794495             |                     18.611865063978286                      |            20.589375727025978             |                      8.918185343156262                      |            11.554866227219852             |                66.65374176037224                 |          58.12330360604886           |          76.580069794494          |          54.207056998836755          |           17.75882124854595            |         49.12756882512602          |         31.213648701046917          |            12.291585886002327             |           16.789453276463746           |         11.748739821636292         |             48.58472276075998              |            24.27297402093835             |              20.628150445909267              |          8.414113997673518          |          26.793330748352073           |         36.952307095773556          |         17.215975184179914          |         18.689414501744864          |             35.98293912369135              |         3.179526948429624         |            86.3125242341993             |                  35.09112058937573                   |                  61.72935246219465                  |              43.42768514928267               |                     58.58860023264831                     |          30.864676231097324           |                   7.599844901124467                    |          47.30515703761148           |                77.31678945327646                |              82.7452500969368              |\n",
            "|      237      |    237     | 399.96600000000007 | 5.972400000000006  |  1  |  4  |        9        |       37.39        |       3        |      2      |           5           |                                                         CCN(CC)CCC[C@H](C)Nc1c2ccc(Cl)cc2nc2ccc(OC)cc12                                                          | 399.9660000000002  |  5.972400000000006  |            4            |          1           |   3.0    | 0.4493831432285027  |       1        |       37.39        |  0.8656829237937927   | 0.9666250228881836  | 0.9144891142845154 |  0.8928620934486389  | 0.16664258092641832  |      0.14729833751916885       | 0.03724945206195116  |       0.6632840871810913       |  0.9382375478744507   |       0.7245888948440552       |   0.3950761675834656   | 0.4990366905927658  |  0.6714143872261047   | 0.6598935723304749  | 0.9984958052635193 |  0.04410548508167267   | 0.01413212064653635  |  0.7515201330184936   |  0.29046552777290346  |  0.04024618007242679  | 0.07671576887369155  | 0.018241093400865792  | 0.9112822532653808  |  0.8822207450866699   | 0.29513721019029615  |  0.06169416084885597  |   0.18509561419487   | 0.29118165001273155  |  0.32870113253593447  | 0.5213869214057922  |  0.9679293155670166   | -4.7424990566188985 |   27.166197478701292    |   61.84875027227092    | 213.08603338585036  |      -9.102830357109813      | 2.980885871893334  |    2.1027811814395543     | 90.98206486171682  | -3.770542018326757  |  63.97153654947105  |               65.25785188057387               |         94.14501744862349         |                  46.47150058162078                   |                 36.68088406359054                 |          20.918960837533927           |        40.17060876308646         |              54.80806514152772              |         20.70569988367584         |         96.12252811167119         |            80.88406359053897             |                78.48003101977511                |             94.84296238852268             |             59.01512214036448              |                      49.94183792167507                      |             49.78673904614192             |                      92.7879022877084                       |             98.13881349360217             |                       76.580069794494                       |             76.2310973245444              |                88.75533152384645                 |          96.62659945715393           |         62.89259402869329         |          61.53547886777821           |           82.90034897246994            |         33.927879022877086         |          97.16944552151996          |             90.34509499806126             |           71.73322993408297            |         43.505234587049245         |             69.98836758433501              |            69.17409848778595             |              92.12873206669252               |          70.95773555641722          |           82.3962776269872            |          91.46956184567662          |          77.54943776657619          |          91.58588600232648          |             60.06203955021326              |         96.27762698720434         |            65.10275300504071            |                  40.597130670802635                  |                   83.714618069019                   |               99.3408297789841               |                     55.83559519193486                     |           77.78208607987592           |                   62.23342380767739                    |          75.22295463357891           |                36.29313687475766                |             99.92245056223342              |\n",
            "|     36314     |   36314    | 853.9180000000002  | 3.735700000000006  |  4  | 14  |       10        |       221.29       |       2        |      1      |           3           | CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O |      853.918       | 3.7357000000000036  |           14            |          4           |   2.0    | 0.12978619445124503 |       11       | 221.28999999999996 |  0.3690257579088211   |  0.313214099407196  | 0.6864721655845643 | 0.007987610111013056 |  0.0576062198728323  |      0.042367860302329065      | 0.11490030437707902  |      0.024853175133466722      |  0.06152134500443936  |       0.6306055426597595       |   0.6012769103050232   | 0.08611706085503101 |  0.33451235443353655  | 0.7962758541107178  | 0.9790336132049561 |  0.20569149702787398   |  0.4202631890773773  |  0.04734908863902092  |  0.25445756912231443  | 0.028367896378040314  | 0.19605415910482407  |  0.08270415738224983  |  0.496009373664856  |  0.7438596844673157   | 0.29954059720039367  |  0.2619645059108734   | 0.037576324120163915 |  0.8947320818901062  |  0.2986320436000824   | 0.08220251500606537 |  0.6043513178825378   | -5.646112000638434  |   127.14940708554795    |   97.40182882142126    |  45.36265677255511  |     -11.105821218558324      | 4.349726015371661  |     2.591540834807309     | 99.71100285099035  | -5.970252675199414  | 3.3081267603121303  |               95.15316013958899               |         73.20666925164792         |                  94.59092671578131                   |                 86.02171384257464                 |           9.848778596355176           |        7.212097712291586         |              96.47150058162077              |         93.67972082202404         |         73.36176812718108         |            19.038386971694454            |                34.58704924389298                |            31.058549825513765             |             40.635905389685924             |                      15.66498642884839                      |             67.54556029468786             |                     18.224117875145403                      |             54.01318340442032             |                      64.24970918960837                      |             83.86971694455215             |                33.307483520744476                |          78.55758045754168           |         71.22915858860023         |          36.02171384257464           |            93.136874757658             |         94.53276463745638          |         60.178363706863124          |             88.9104303993796              |           63.43544009305933            |         79.41062427297402          |             89.41450174486235              |             34.1993020550601             |              85.80845288871656               |          71.38425746413338          |           95.81233036060489           |           69.716944552152           |          97.55719271035285          |          90.30632027917797          |             3.179526948429624              |         68.43737882900349         |           19.658782473827063            |                  96.47150058162077                   |                  95.23070957735555                  |              85.49825513765025               |                     35.09112058937573                     |           99.10818146568437           |                   72.04342768514928                    |          90.50019387359441           |                5.661108956960062                |             63.24156649864288              |\n",
            "|    392622     |   392622   | 720.9620000000001  | 5.905200000000006  |  4  |  9  |       17        | 145.77999999999997 |       2        |      0      |           2           |                               CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)C[C@H](O)[C@H](Cc2ccccc2)NC(=O)OCc2cncs2)C(C)C)cs1                               | 720.9620000000001  |  5.905200000000005  |            9            |          4           |   2.0    | 0.10624046585910327 |       4        | 145.77999999999997 |  0.1494164913892746   | 0.21091121435165405 | 0.6501427412033081 |  0.2530102521181107  |  0.9651636362075806  |       0.5533619165420532       |  0.9725804209709168  |      0.19899119734764098       |  0.4466812551021576   |       0.7313636660575866       |    0.9960165143013     | 0.13090016543865204 |  0.13391368836164474  | 0.8784408926963806  | 0.9992312788963318 |  0.002778747561387718  | 0.006491873692721128 |  0.22553056478500366  |  0.09251121878623962  |  0.00802364144474268  | 0.046837984770536426 |  0.06080225184559822  | 0.7044352889060974  |  0.9145047664642334   |  0.4040724217891693  | 0.012484434805810452  | 0.08004205450415611  |  0.6769227504730224  |  0.07775581479072571  | 0.10142785906791688 |   0.876216185092926   | -5.213374414288153  |    63.33160546515101    |   112.39471772456488   |  69.04919204080818  |     -11.242016714513223      | 2.8012802232423804 |     4.091072569121021     | 104.67844832732646 | -5.502664501109723  | -3.641342687285788  |               92.59402869329197               |         93.71849554090733         |                  86.25436215587436                   |                 86.02171384257464                 |           9.848778596355176           |        5.854982551376502         |              80.1085692128732               |         83.90849166343544         |         43.93175649476541         |            13.260953858084529            |                30.12795657231485                |             73.40054284606437             |              98.6816595579682              |                      95.50213260953858                      |             99.49592865451726             |                      65.83947266382319                      |             84.3350135711516              |                      77.70453664210935                      |             99.92245056223342             |                45.327646374563784                |          57.58045754168282           |         78.98410236525785         |          68.55370298565336           |           21.675067855758044           |         17.060876308646762         |          83.36564559906941          |              72.004652966266              |           30.283055447848003           |         24.389298177588213         |              86.3125242341993              |            46.72353625436215             |              94.80418766963939               |          78.20860798759209          |           59.5967429236138            |          81.85343156262117          |          92.08995734780923          |          72.15975184179915          |             5.854982551376502              |         84.17991469561845         |            36.17681271810779            |                  72.78014734393176                   |                  97.20822024040325                  |              91.66343544009305               |                     33.81155486622722                     |           68.01085692128731           |                    96.0837533927879                    |          96.70414889492051           |                8.76308646762311                 |             13.338503295851105             |\n",
            "+---------------+------------+--------------------+--------------------+-----+-----+-----------------+--------------------+----------------+-------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------+-------------------------+----------------------+----------+---------------------+----------------+--------------------+-----------------------+---------------------+--------------------+----------------------+----------------------+--------------------------------+----------------------+--------------------------------+-----------------------+--------------------------------+------------------------+---------------------+-----------------------+---------------------+--------------------+------------------------+----------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+-----------------------+---------------------+-----------------------+---------------------+-------------------------+------------------------+---------------------+------------------------------+--------------------+---------------------------+--------------------+---------------------+---------------------+-----------------------------------------------+-----------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------+----------------------------------+---------------------------------------------+-----------------------------------+-----------------------------------+------------------------------------------+-------------------------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+-------------------------------------------------------------+-------------------------------------------+--------------------------------------------------+--------------------------------------+-----------------------------------+--------------------------------------+----------------------------------------+------------------------------------+-------------------------------------+-------------------------------------------+----------------------------------------+------------------------------------+--------------------------------------------+------------------------------------------+----------------------------------------------+-------------------------------------+---------------------------------------+-------------------------------------+-------------------------------------+-------------------------------------+--------------------------------------------+-----------------------------------+-----------------------------------------+------------------------------------------------------+-----------------------------------------------------+----------------------------------------------+-----------------------------------------------------------+---------------------------------------+--------------------------------------------------------+--------------------------------------+-------------------------------------------------+--------------------------------------------+\n",
            "\n",
            "✅ Results have been saved as a CSV file: /content/OverallRanked_molecules.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ranking Ligands Based on Drug-**Likeness**"
      ],
      "metadata": {
        "id": "cMZ7049bdJRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section processes molecular structures from SDF files, computes essential physicochemical properties and ranks ligands based on their drug-likeness potential. The results are displayed in a structured format and saved as a CSV file for further analysis.\n",
        "\n",
        "**Working Logic & Calculation Methods**\n",
        "\n",
        "1️⃣ Extracting Molecular Data\n",
        "The script scans the /content/ligands folder for SDF files containing molecular structures.\n",
        "Each molecule's name, PubChem ID, and SMILES representation are extracted.\n",
        "\n",
        "2️⃣ Computing Physicochemical Properties\n",
        "Each ligand’s key descriptors are calculated using RDKit, including:\n",
        "\n",
        "✔ Molecular Weight (MW)\n",
        "\n",
        "*   LogP (Hydrophobicity)\n",
        "*   Hydrogen Bond Donors (HBD) & Acceptors (HBA)\n",
        "*   Rotatable Bonds (Flexibility)\n",
        "*   Topological Polar Surface Area (TPSA)\n",
        "*   Stereo Centers (Chirality Information)\n",
        "\n",
        "3️⃣ Drug-Likeness Evaluation\n",
        "\n",
        "The script applies two fundamental drug-likeness rules:\n",
        "\n",
        "🔹 *Lipinski’s Rule of 5 (RO5)*\n",
        "\n",
        "A molecule is considered \"drug-like\" if it meets at least three of these conditions:\n",
        "\n",
        "1.   ✅ MW ≤ 500\n",
        "2.   ✅ LogP ≤ 5\n",
        "3.   ✅ HBD ≤ 5\n",
        "2.   ✅ HBA ≤ 10\n",
        "\n",
        "\n",
        "\n",
        "Lipinski Score Calculation:\n",
        "\n",
        "*Lipinski Score = (MW≤500) + (LogP≤5) + (HBD≤5) + (HBA≤10)*\n",
        "\n",
        "*(maximum score = 4)*\n",
        "\n",
        "*🔹 Veber’s Rule*\n",
        "\n",
        "A molecule is considered orally bioavailable if:\n",
        "\n",
        "1.   ✅ Rotatable Bonds ≤ 10\n",
        "2.   ✅ TPSA ≤ 140\n",
        "\n",
        "Veber Score Calculation:\n",
        "\n",
        "*Veber Score=(Rotatable Bonds≤10) + (TPSA≤140)*\n",
        "*(maximum score = 2)*\n",
        "\n",
        "*4️⃣ ADMET Property Prediction*\n",
        "\n",
        "SMILES strings are extracted from molecules and passed into admet-ai. QED (Quantitative Estimation of Drug-Likeness) is extracted from the model's output.\n",
        "\n",
        "*5️⃣ Total Drug-Like Score Calculation (VLQ Score)*\n",
        "\n",
        "To rank molecules, the script calculates an overall Total Drug-Like Score (VLQ Score) as follows:\n",
        "\n",
        "*VLQ Score = QED + Lipinski Score + Veber Score*\n",
        "\n",
        "This score integrates:\n",
        "\n",
        "✔ QED value (higher = more drug-like)\n",
        "✔ Lipinski & Veber rule compliance\n",
        "\n",
        "*6️⃣ Sorting & Saving Results*\n",
        "\n",
        "Molecules are ranked in descending order based on their VLQ Score.The final ranked dataset is displayed in tabular format.\n",
        "The results are saved as a CSV file (VLQ_score.csv).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pug9MkS0r3Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from admet_ai import ADMETModel\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Function to calculate descriptors\n",
        "def calculate_descriptors(mol):\n",
        "    descriptors = {}\n",
        "    descriptors['Molecular Weight'] = Descriptors.MolWt(mol)\n",
        "    descriptors['LogP'] = Descriptors.MolLogP(mol)\n",
        "    descriptors['HBD'] = Descriptors.NumHDonors(mol)\n",
        "    descriptors['HBA'] = Descriptors.NumHAcceptors(mol)\n",
        "    descriptors['Rotatable Bonds'] = Descriptors.NumRotatableBonds(mol)\n",
        "    descriptors['Polar Surface Area'] = Descriptors.TPSA(mol)\n",
        "    descriptors['Stereo Centers'] = Chem.FindMolChiralCenters(mol, includeUnassigned=True)\n",
        "    return descriptors\n",
        "\n",
        "# Function to apply Lipinski's Rule of 5\n",
        "def lipinski_rules(descriptors):\n",
        "    rules = sum([\n",
        "        descriptors['Molecular Weight'] <= 500,\n",
        "        descriptors['LogP'] <= 5,\n",
        "        descriptors['HBD'] <= 5,\n",
        "        descriptors['HBA'] <= 10\n",
        "    ])\n",
        "    return rules\n",
        "\n",
        "# Function to apply Veber's Rule\n",
        "def veber_rules(descriptors):\n",
        "    rules = sum([\n",
        "        descriptors['Rotatable Bonds'] <= 10,\n",
        "        descriptors['Polar Surface Area'] <= 140\n",
        "    ])\n",
        "    return rules\n",
        "\n",
        "# Function to process SDF files and calculate descriptors\n",
        "def process_sdf_from_folder(folder_path):\n",
        "    all_molecules = []\n",
        "\n",
        "    # List all .sdf files in the folder\n",
        "    sdf_files = [f for f in os.listdir(folder_path) if f.endswith('.sdf')]\n",
        "\n",
        "    for sdf_file in sdf_files:\n",
        "        sdf_file_path = os.path.join(folder_path, sdf_file)\n",
        "        supplier = Chem.SDMolSupplier(sdf_file_path)\n",
        "\n",
        "        for mol in supplier:\n",
        "            if mol is not None:  # Skip invalid molecules\n",
        "                mol_name = mol.GetProp(\"_Name\") if mol.HasProp(\"_Name\") else \"Unknown\"\n",
        "                pubchem_id = mol.GetProp(\"PUBCHEM_COMPOUND_CID\") if mol.HasProp(\"PUBCHEM_COMPOUND_CID\") else \"N/A\"\n",
        "\n",
        "                # Get descriptors\n",
        "                descriptors = calculate_descriptors(mol)\n",
        "\n",
        "                # Apply Lipinski and Veber's rules\n",
        "                lipinski_score = lipinski_rules(descriptors)\n",
        "                veber_score = veber_rules(descriptors)\n",
        "\n",
        "                # Store the data\n",
        "                data = {\n",
        "                    'Molecule Name': mol_name,\n",
        "                    'PubChem ID': pubchem_id,\n",
        "                    'SMILES': Chem.MolToSmiles(mol),\n",
        "                    'Molecular Weight': descriptors['Molecular Weight'],\n",
        "                    'LogP': descriptors['LogP'],\n",
        "                    'tPSA': descriptors['Polar Surface Area'],\n",
        "                    'Rotatable Bonds': descriptors['Rotatable Bonds'],\n",
        "                    'Stereo Centers': len(descriptors['Stereo Centers']),\n",
        "                    'HBD': descriptors['HBD'],\n",
        "                    'HBA': descriptors['HBA'],\n",
        "                    'Lipinski Score': lipinski_score,\n",
        "                    'Veber Score': veber_score\n",
        "                }\n",
        "                all_molecules.append(data)\n",
        "\n",
        "    return all_molecules\n",
        "\n",
        "# Function to predict ADMET properties\n",
        "def predict_admet(smiles_list):\n",
        "    model = ADMETModel()\n",
        "    preds = model.predict(smiles=smiles_list)\n",
        "\n",
        "    # Ensure SMILES column is present\n",
        "    preds.insert(0, \"SMILES\", smiles_list)\n",
        "\n",
        "    # Extract specific properties of interest\n",
        "    selected_columns = [\"SMILES\", \"QED\"]\n",
        "\n",
        "    # Ensure only available columns are selected\n",
        "    preds = preds[[col for col in selected_columns if col in preds.columns]]\n",
        "\n",
        "    return preds\n",
        "\n",
        "# Define the folder path where your SDF files are stored\n",
        "folder_path = '/content/ligands'\n",
        "\n",
        "# Process the SDF files from the folder and get the data\n",
        "molecules_data = process_sdf_from_folder(folder_path)\n",
        "\n",
        "# Create a DataFrame for drug-likeness scores\n",
        "df_drug_like = pd.DataFrame(molecules_data)\n",
        "\n",
        "# Extract SMILES strings for ADMET prediction\n",
        "smiles_list = df_drug_like['SMILES'].tolist()\n",
        "\n",
        "# Predict ADMET properties\n",
        "df_admet = predict_admet(smiles_list)\n",
        "\n",
        "# Merge drug-likeness and ADMET data\n",
        "df_combined = pd.merge(df_drug_like, df_admet, on=\"SMILES\", how=\"inner\")\n",
        "\n",
        "# Calculate Total Drug-Like Score as the sum of QED, Lipinski Score, and Veber Score\n",
        "df_combined['Total Drug-Like Score'] = df_combined['QED'] + df_combined['Lipinski Score'] + df_combined['Veber Score']\n",
        "\n",
        "# Sort molecules based on 'Total Drug-Like Score'\n",
        "df_sorted_VLQscore = df_combined.sort_values(by=['Total Drug-Like Score'], ascending=False)\n",
        "\n",
        "# Display the results in a table format\n",
        "print(\"Ranked Molecules Based on Drug-Likeness score:\")\n",
        "print(tabulate(df_sorted_VLQscore, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Ensure columns are valid before saving\n",
        "df_sorted_VLQscore.columns = [str(col) for col in df_sorted_VLQscore.columns]\n",
        "\n",
        "# Now try saving the DataFrame to CSV\n",
        "df_sorted_VLQscore.to_csv('/content/VLQ_score.csv', index=False)\n",
        "\n",
        "print(\"CSV file saved successfully to /content/VLQ_score.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw_6KQ2hZ9iY",
        "outputId": "6301df37-1390-43ba-e6d1-263d7066af8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 10/10 [00:00<00:00, 12584.17it/s]\n",
            "Computing physchem properties: 100%|██████████| 10/10 [00:00<00:00, 556.76it/s]\n",
            "RDKit fingerprints: 100%|██████████| 10/10 [00:00<00:00, 18.49it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.41it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.48it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.72it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 22.39it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 22.64it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Molecules Based on Drug-Likeness score:\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+--------------------+-----------------+----------------+-----+-----+----------------+-------------+---------------------+-----------------------+\n",
            "| Molecule Name | PubChem ID |                                                                              SMILES                                                                              |  Molecular Weight  |        LogP        |        tPSA        | Rotatable Bonds | Stereo Centers | HBD | HBA | Lipinski Score | Veber Score |         QED         | Total Drug-Like Score |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+--------------------+-----------------+----------------+-----+-----+----------------+-------------+---------------------+-----------------------+\n",
            "|     3672      |    3672    |                                                                 CC(C)Cc1ccc([C@@H](C)C(=O)O)cc1                                                                  |      206.285       | 3.0732000000000017 |        37.3        |        4        |       1        |  1  |  1  |       4        |      2      | 0.8215995486924976  |   6.821599548692498   |\n",
            "|     5757      |    5757    |                                                       C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C@@H]2O                                                       |      272.388       | 3.609200000000003  |       40.46        |        0        |       5        |  2  |  2  |       4        |      2      |  0.757169743555686  |   6.757169743555686   |\n",
            "|   146502750   | 146502750  |                                              C[C@H]1C(=O)C(C#N)=C[C@@]2(C)c3nc(-c4ccnnc4)nc(-c4ccccc4F)c3CC[C@H]12                                               | 425.46700000000016 | 4.228580000000004  |       92.42        |        2        |       3        |  0  |  6  |       4        |      2      | 0.6101190507120148  |   6.610119050712015   |\n",
            "|   162394460   | 162394460  |                                                          NC[C@@H](C(=O)NNCc1ccc(O)c(Br)c1)c1cccc(Cl)c1                                                           | 398.68800000000016 |       2.6714       |       87.38        |        6        |       1        |  4  |  4  |       4        |      2      | 0.5634773769896697  |  6.5634773769896695   |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    |      180.167       |      -1.0397       |       72.68        |        0        |       0        |  1  |  5  |       4        |      2      | 0.5624722357827983  |   6.562472235782798   |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    |      180.167       |      -1.0397       |       72.68        |        0        |       0        |  1  |  5  |       4        |      2      | 0.5624722357827983  |   6.562472235782798   |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    |      180.167       |      -1.0397       |       72.68        |        0        |       0        |  1  |  5  |       4        |      2      | 0.5624722357827983  |   6.562472235782798   |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    |      180.167       |      -1.0397       |       72.68        |        0        |       0        |  1  |  5  |       4        |      2      | 0.5624722357827983  |   6.562472235782798   |\n",
            "|     2244      |    2244    |                                                                      CC(=O)Oc1ccccc1C(=O)O                                                                       | 180.15899999999996 |       1.3101       |        63.6        |        2        |       0        |  1  |  3  |       4        |      2      | 0.5501217966938848  |   6.550121796693885   |\n",
            "|      237      |    237     |                                                         CCN(CC)CCC[C@H](C)Nc1c2ccc(Cl)cc2nc2ccc(OC)cc12                                                          | 399.96600000000007 | 5.972400000000006  |       37.39        |        9        |       1        |  1  |  4  |       3        |      2      | 0.4493831432285027  |   5.449383143228503   |\n",
            "|     36314     |   36314    | CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O | 853.9180000000002  | 3.735700000000006  |       221.29       |       10        |       11       |  4  | 14  |       2        |      1      | 0.12978619445124503 |   3.129786194451245   |\n",
            "|    392622     |   392622   |                               CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)C[C@H](O)[C@H](Cc2ccccc2)NC(=O)OCc2cncs2)C(C)C)cs1                               | 720.9620000000001  | 5.905200000000006  | 145.77999999999997 |       17        |       4        |  4  |  9  |       2        |      0      | 0.10624046585910327 |  2.1062404658591034   |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+--------------------+-----------------+----------------+-----+-----+----------------+-------------+---------------------+-----------------------+\n",
            "CSV file saved successfully to /content/VLQ_score.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Ranking** Ligands Based on ADMET Properties"
      ],
      "metadata": {
        "id": "C66tT0ojdPnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section analyzes ligand ADMET properties (Absorption, Distribution, Metabolism, Excretion, and Toxicity) by:\n",
        "\n",
        "\n",
        "1.   Extracting molecular data from SDF files.\n",
        "2.   Predicting ADMET properties using admet-ai.\n",
        "2.   Normalizing ADMET scores for fair comparison.\n",
        "2.   Calculating category-wise ADMET scores.\n",
        "2.   Ranking ligands based on overall ADMET performance.\n",
        "2.   Saving ranked molecules in a CSV file for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**🚀 Working Logic & Calculation Methods**\n",
        "\n",
        "1️⃣ Extracting Molecular Information\n",
        "\n",
        "The script scans the /content/ligands folder for SDF files containing molecular structures.\n",
        "\n",
        "2️⃣ ADMET Property Prediction\n",
        "\n",
        "The extracted SMILES strings are input into the ADMETModel to predict:\n",
        "\n",
        "✔ Absorption (Bioavailability, Solubility, Lipophilicity, etc.)\n",
        "\n",
        "✔ Distribution (Plasma Binding, BBB Penetration, etc.\n",
        ")\n",
        "✔ Metabolism (CYP Enzyme Interactions, Half-life, etc.)\n",
        "\n",
        "✔ Excretion (Clearance, P-gp Interaction, etc.)\n",
        "\n",
        "✔ Toxicity (AMES Test, Carcinogenicity, hERG Inhibition, etc.)\n",
        "\n",
        "The output includes numerical scores for each ADMET property.\n",
        "\n",
        "3️⃣ Normalization of ADMET Properties\n",
        "Raw ADMET values are normalized using Min-Max Scaling to bring them into a 0-1 range:\n",
        "\n",
        "*Normalized Value = Value−Min / Max−Min*\n",
        "\n",
        "This ensures fair ranking, avoiding bias due to different property scales.\n",
        "\n",
        "\n",
        "4️⃣ ADMET Category Scores Calculation\n",
        "\n",
        "For each ADMET category (Absorption, Distribution, Metabolism, Excretion, Toxicity), an average score is calculated:\n",
        "\n",
        "*Category Score = ∑(Normalized ADMET Properties in Category)/Number of Properties in Category*\n",
        "\n",
        "5️⃣ Overall ADMET Score & Ranking\n",
        "\n",
        "The Overall ADMET Score is computed as the average of all category scores:\n",
        "\n",
        "*Overall ADMET Score = ∑(Absorption, Distribution, Metabolism, Excretion, Toxicity Scores)/5*\n",
        "\n",
        " ✔ Higher scores indicate better pharmacokinetic & safety profiles.\n",
        "\n",
        "✔ Molecules are ranked in descending order based on the Overall ADMET Score.\n",
        "\n",
        "6️⃣ Saving & Displaying Results\n",
        "\n",
        "Ranked ADMET data is displayed in tabular format.\n",
        "Top-scoring molecules can be selected for further drug development.\n",
        "\n",
        "Results are saved as:\n",
        "\n",
        "✅ Raw ADMET Predictions → /content/ADMET_Predictions.csv\n",
        "\n",
        "✅ Normalized & Ranked ADMET Scores → /content/Normalized_ADMET_Scores.csv\n"
      ],
      "metadata": {
        "id": "P_DeWjIOwdIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from admet_ai import ADMETModel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define the ADMET properties categorized by ADMET classification\n",
        "admet_categories = {\n",
        "    \"Absorption\": [\"Bioavailability_Ma\", \"HIA_Hou\", \"Caco2_Wang\", \"AMPA_NCATS\", \"HydrationFreeEnergy_FreeSolv\", \"Lipophilicity_AstraZeneca\", \"PPBR_AZ\", \"Solubility_AqSolDB\"],\n",
        "    \"Distribution\": [\"PPBR_AZ\", \"VDss_Lombardo\", \"BBB_Martins\", \"Solubility_AqSolDB\", \"Lipophilicity_AstraZeneca\"],\n",
        "    \"Metabolism\": [\"CYP1A2_Veith\", \"CYP2C19_Veith\", \"CYP2C9_Veith\", \"CYP2D6_Veith\", \"CYP3A4_Veith\", \"CYP2C9_Substrate_CarbonMangels\", \"CYP2D6_Substrate_CarbonMangels\", \"CYP3A4_Substrate_CarbonMangels\", \"Clearance_Hepatocyte_AZ\", \"Half_Life_Obach\"],\n",
        "    \"Excretion\": [\"Clearance_Hepatocyte_AZ\", \"Half_Life_Obach\", \"Pgp_Broccatelli\"],\n",
        "    \"Toxicity\": [\"AMES\", \"Carcinogens_Lagunin\", \"ClinTox\", \"DILI\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n",
        "                 \"NR-ER-LBD\", \"NR-ER\", \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"Skin_Reaction\", \"hERG\", \"LD50_Zhu\"]\n",
        "}\n",
        "\n",
        "# Flatten list of all ADMET properties\n",
        "admet_columns = [prop for category in admet_categories.values() for prop in category]\n",
        "\n",
        "# Function to process SDF files and extract molecular information\n",
        "def process_sdf_from_folder(folder_path):\n",
        "    all_molecules = []\n",
        "    sdf_files = [f for f in os.listdir(folder_path) if f.endswith('.sdf')]\n",
        "\n",
        "    for sdf_file in sdf_files:\n",
        "        sdf_file_path = os.path.join(folder_path, sdf_file)\n",
        "        supplier = Chem.SDMolSupplier(sdf_file_path)\n",
        "\n",
        "        for mol in supplier:\n",
        "            if mol is not None:\n",
        "                mol_name = mol.GetProp(\"_Name\") if mol.HasProp(\"_Name\") else \"Unknown\"\n",
        "                pubchem_id = mol.GetProp(\"PUBCHEM_COMPOUND_CID\") if mol.HasProp(\"PUBCHEM_COMPOUND_CID\") else \"N/A\"\n",
        "                smiles = Chem.MolToSmiles(mol)\n",
        "\n",
        "                all_molecules.append({\"Molecule Name\": mol_name, \"PubChem ID\": pubchem_id, \"SMILES\": smiles})\n",
        "\n",
        "    return pd.DataFrame(all_molecules)\n",
        "\n",
        "# Function to predict ADMET properties\n",
        "def predict_admet(smiles_list):\n",
        "    model = ADMETModel()\n",
        "    preds = model.predict(smiles=smiles_list)\n",
        "\n",
        "    # Keep only relevant ADMET columns\n",
        "    preds = preds[[col for col in admet_columns if col in preds.columns]]\n",
        "    preds.insert(0, \"SMILES\", smiles_list)\n",
        "    return preds\n",
        "\n",
        "# Define the folder path where SDF files are stored\n",
        "folder_path = '/content/ligands'\n",
        "\n",
        "# Process the SDF files\n",
        "df_molecules = process_sdf_from_folder(folder_path)\n",
        "\n",
        "# Predict ADMET properties\n",
        "df_admet = predict_admet(df_molecules['SMILES'].tolist())\n",
        "\n",
        "# Merge molecular info with ADMET predictions\n",
        "df_combined = pd.merge(df_molecules, df_admet, on=\"SMILES\", how=\"inner\")\n",
        "\n",
        "# Display the ADMET predictions table\n",
        "print(\"ADMET Predictions:\")\n",
        "print(tabulate(df_combined, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Save predicted results to a CSV file\n",
        "predicted_csv_path = \"/content/ADMET_Predictions.csv\"\n",
        "df_combined.to_csv(predicted_csv_path, index=False)\n",
        "print(f\"Predicted results saved successfully to {predicted_csv_path}\")\n",
        "\n",
        "# Normalize ADMET properties\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Filter admet_existing_columns to include only columns present in df_combined\n",
        "admet_existing_columns = [col for col in admet_columns if col in df_combined.columns]\n",
        "\n",
        "# Rename duplicate columns to make them unique\n",
        "df_combined = df_combined.loc[:, ~df_combined.columns.duplicated()]  # Remove duplicates first\n",
        "for col in df_combined.columns:\n",
        "    if df_combined.columns.tolist().count(col) > 1:  # Check if column is duplicated\n",
        "        df_combined.rename(columns={col: f\"{col}_1\"}, inplace=True)  # Rename the first occurrence\n",
        "        df_combined.rename(columns={col: f\"{col}_2\"}, inplace=True)  # Rename the second occurrence\n",
        "\n",
        "# Debug: Print columns to verify\n",
        "print(\"Columns in df_combined (after renaming duplicates):\", df_combined.columns.tolist())\n",
        "print(\"Columns in admet_existing_columns:\", admet_existing_columns)\n",
        "\n",
        "# Verify the shape of the data being normalized\n",
        "print(\"Shape of df_combined[admet_existing_columns]:\", df_combined[admet_existing_columns].shape)\n",
        "\n",
        "# Normalize the data\n",
        "df_norm = pd.DataFrame(scaler.fit_transform(df_combined[admet_existing_columns]), columns=admet_existing_columns)\n",
        "\n",
        "# Combine normalized scores with molecule info\n",
        "df_normalized = pd.concat([df_combined[['Molecule Name', 'PubChem ID', 'SMILES']], df_norm], axis=1)\n",
        "\n",
        "# Calculate ADMET category scores\n",
        "for category, properties in admet_categories.items():\n",
        "    existing_props = [prop for prop in properties if prop in df_norm.columns]\n",
        "    if existing_props:  # Only calculate if there are existing properties\n",
        "        df_normalized[f\"{category} Score\"] = df_norm[existing_props].mean(axis=1)\n",
        "    else:\n",
        "        df_normalized[f\"{category} Score\"] = 0  # Default score if no properties exist\n",
        "\n",
        "# Compute overall ADMET Score as the mean of the category scores\n",
        "df_normalized[\"Overall ADMET Score\"] = df_normalized[[f\"{cat} Score\" for cat in admet_categories]].mean(axis=1)\n",
        "\n",
        "# Rank molecules based on the Overall ADMET Score\n",
        "df_ranked = df_normalized.sort_values(by=[\"Overall ADMET Score\"], ascending=False)\n",
        "\n",
        "# Display normalized results in a table format\n",
        "print(\"Normalized ADMET Results:\")\n",
        "print(tabulate(df_ranked, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Save normalized results to a CSV file\n",
        "normalized_csv_path = \"/content/Normalized_ADMET_Scores.csv\"\n",
        "df_ranked.to_csv(normalized_csv_path, index=False)\n",
        "print(f\"Normalized results saved successfully to {normalized_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_YZQsvCDdkr",
        "outputId": "78dbfb3f-1cf5-4cee-e101-a60d5cecc8a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.5)\n",
            "Requirement already satisfied: admet-ai in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Requirement already satisfied: chemfunc>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.0.11)\n",
            "Requirement already satisfied: chemprop==1.6.1 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from admet-ai) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from admet-ai) (4.67.1)\n",
            "Requirement already satisfied: typed-argument-parser>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from admet-ai) (1.10.1)\n",
            "Requirement already satisfied: flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (0.2.7)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (3.10.0)\n",
            "Requirement already satisfied: pandas-flavor>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (0.6.0)\n",
            "Requirement already satisfied: sphinx>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (8.1.3)\n",
            "Requirement already satisfied: tensorboardX>=2.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (2.6.2.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from chemprop==1.6.1->admet-ai) (2.5.1+cu124)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: descriptastorus>=2.7.0.5 in /usr/local/lib/python3.11/dist-packages (from chemfunc>=1.0.4->admet-ai) (2.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (0.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (24.2)\n",
            "Requirement already satisfied: typing-inspect>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from typed-argument-parser>=1.9.0->admet-ai) (0.9.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=1.1.2->chemprop==1.6.1->admet-ai) (1.9.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt>=0.2.3->chemprop==1.6.1->admet-ai) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.3->chemprop==1.6.1->admet-ai) (3.2.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pandas-flavor>=0.2.0->chemprop==1.6.1->admet-ai) (2025.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.18.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.32.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.0->chemprop==1.6.1->admet-ai) (4.25.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->chemprop==1.6.1->admet-ai) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.7.1->typed-argument-parser>=1.9.0->admet-ai) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=1.1.2->chemprop==1.6.1->admet-ai) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=3.1.2->chemprop==1.6.1->admet-ai) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 10/10 [00:00<00:00, 97997.76it/s]\n",
            "Computing physchem properties: 100%|██████████| 10/10 [00:00<00:00, 429.48it/s]\n",
            "RDKit fingerprints: 100%|██████████| 10/10 [00:00<00:00, 19.24it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.27it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.18it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.64it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  40%|████      | 2/5 [00:00<00:00, 18.71it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 18.38it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 17.97it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADMET Predictions:\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------------+----------------------+----------------------+----------------------+-----------------------+------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+---------------------+-------------------------+---------------------+-----------------------+-----------------------+---------------------+-----------------------+---------------------+------------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+---------------------+-----------------------+--------------------+\n",
            "| Molecule Name | PubChem ID |                                                                              SMILES                                                                              | Bioavailability_Ma |      HIA_Hou       |     Caco2_Wang      | HydrationFreeEnergy_FreeSolv | Lipophilicity_AstraZeneca |      PPBR_AZ       | Solubility_AqSolDB  |      PPBR_AZ       |    VDss_Lombardo    |     BBB_Martins     | Solubility_AqSolDB  | Lipophilicity_AstraZeneca |     CYP1A2_Veith     |    CYP2C19_Veith     |     CYP2C9_Veith     |     CYP2D6_Veith      |      CYP3A4_Veith      | CYP2C9_Substrate_CarbonMangels | CYP2D6_Substrate_CarbonMangels | CYP3A4_Substrate_CarbonMangels | Clearance_Hepatocyte_AZ |   Half_Life_Obach   | Clearance_Hepatocyte_AZ |   Half_Life_Obach   |    Pgp_Broccatelli    |         AMES          | Carcinogens_Lagunin |        ClinTox        |        DILI         |       NR-AR-LBD        |        NR-AhR         |     NR-Aromatase      |       NR-ER-LBD       |        NR-ER         |     NR-PPAR-gamma     |        SR-ARE        |       SR-ATAD5        |        SR-HSE        |        SR-MMP        |    Skin_Reaction    |         hERG          |      LD50_Zhu      |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------------+----------------------+----------------------+----------------------+-----------------------+------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+---------------------+-------------------------+---------------------+-----------------------+-----------------------+---------------------+-----------------------+---------------------+------------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+---------------------+-----------------------+--------------------+\n",
            "|   162394460   | 162394460  |                                                          NC[C@@H](C(=O)NNCc1ccc(O)c(Br)c1)c1cccc(Cl)c1                                                           | 0.9237998485565185 | 0.9994607448577881 | -5.520491914519063  |     -10.595015312167329      |    1.9757059077614592     | 88.33957024341315  | -3.1282841442473406 | 88.33957024341315  |  4.749792568234403  | 0.34805492162704466 | -3.1282841442473406 |    1.9757059077614592     |  0.8150797963142395  |  0.7971238851547241  | 0.40780357718467714  |   0.812786316871643   |   0.9601204633712769   |      0.21525557041168214       |       0.2744558334350586       |       0.5858330011367798       |   25.472926189315455    | 19.682228027140763  |   25.472926189315455    | 19.682228027140763  |  0.05037892684340477  |  0.7363239169120789   | 0.6930452466011048  |  0.29779587090015414  | 0.2888411432504654  |  0.010928187752142549  |  0.5773596405982971   |  0.12855832278728485  |  0.06473740190267563  | 0.11669883280992507  | 0.021621549129486085  |  0.2873216956853867  |  0.01727471752092242  | 0.18326676338911058  | 0.41618559658527376  |  0.694032096862793  |  0.5572569787502288   | 2.8342429899303543 |\n",
            "|     5757      |    5757    |                                                       C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C@@H]2O                                                       | 0.5407576322555542 | 0.9995497584342956 | -4.969755041288141  |      -7.989803447014729      |    3.3168290967651672     | 87.26012968611175  | -4.531254725191634  | 87.26012968611175  | 3.3970344561327854  | 0.06576163275167346 | -4.531254725191634  |    3.3168290967651672     |  0.6042723417282104  | 0.25900353491306305  | 0.11391125619411469  | 0.011296311486512422  |   0.0391154982149601   |       0.5124844819307327       |       0.2891750052571297       |       0.8047420978546143       |    86.40360194458005    | -4.861958402147683  |    86.40360194458005    | -4.861958402147683  |  0.08396487329155207  |  0.03592464253306389  | 0.04582204216690115 | 0.0033268909610342234 | 0.03261766778305173 |   0.8774696111679077   |  0.01724497452378273  |  0.23916601985692978  |  0.9820187091827393   |  0.9820738792419433  | 0.015706560853868724  | 0.38552348017692567  |  0.02219689227640629  |  0.0809349600225687  |  0.6863088846206665  | 0.5416280329227448  |  0.2687480509281158   | 2.0670937227746373 |\n",
            "|   146502750   | 146502750  |                                              C[C@H]1C(=O)C(C#N)=C[C@@]2(C)c3nc(-c4ccnnc4)nc(-c4ccccc4F)c3CC[C@H]12                                               | 0.9086232423782349 | 0.9999890565872193 | -4.608937931772713  |      -8.176159887814595      |    3.5611487094045473     | 95.53415928612779  |  -5.70728095214937  | 95.53415928612779  | -1.7303338497019596 | 0.9247079610824585  |  -5.70728095214937  |    3.5611487094045473     | 0.47395234405994413  | 0.47111861407756805  |  0.774853527545929   | 0.029151789657771588  |   0.8348206996917724   |      0.14665960147976875       |      0.07600068859755993       |       0.7489179849624634       |    55.95678606596637    | -2.8273004908131525 |    55.95678606596637    | -2.8273004908131525 |  0.9127960920333862   |  0.40264029800891876  | 0.09033608064055443 |  0.2105935588479042   | 0.9031418085098266  |  0.12577787414193153   |  0.5057624816894531   |   0.80758615732193    |  0.0569473072886467   |  0.0791811227798462  |  0.09561691880226135  |  0.7601207017898559  |  0.3590850651264191   |  0.0845750130712986  |  0.7301181554794312  | 0.24050483256578445 |  0.7964029908180237   | 3.6467069485572745 |\n",
            "|     3672      |    3672    |                                                                 CC(C)Cc1ccc([C@@H](C)C(=O)O)cc1                                                                  | 0.9444472432136536 | 0.9987566113471985 | -4.301729852050817  |      -6.987556303069178      |    0.8804752670649784     |  90.3542825781805  | -2.972549609732491  |  90.3542825781805  | 1.0516209977128805  | 0.5324256420135498  | -2.972549609732491  |    0.8804752670649784     | 0.00468623322667554  | 0.04953954014927149  | 0.05575857423245907  | 0.007138351025059819  |  0.002299748035147786  |       0.6704423904418946       |      0.04805629327893257       |       0.2663993388414383       |    22.82005581182144    |  8.569264415183989  |    22.82005581182144    |  8.569264415183989  | 0.0042208576516713945 | 0.0062664449913427235 | 0.4789010167121887  | 0.015424265712499618  | 0.5179961085319519  | 0.00029402488435152917 | 0.004825279163196683  | 0.0017083241371437907 |  0.01757029090076685  | 0.12881022840738296  |  0.05553418770432472  | 0.011128368228673935 | 0.0009551323630148545 | 0.005552589148283005 | 0.030205438286066054 |  0.365418004989624  | 0.009122536052018404  | 2.3579548546073235 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.9000226497650147 | 0.9993916034698487 |  -4.60291462478323  |      -16.47621426203117      |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | 44.85910080355618  | -0.8910617458021803 | 0.8009407162666321  | -1.5467033104197516 |   -0.13593688807430643    |  0.2022198259830475  | 0.003487794508691877 | 0.003984411002602428 | 0.0012955602767760865 |  0.004445872397627682  |      0.40416358709335326       |      0.22379742413759232       |       0.3405459553003311       |   -0.5898503502208925   | 1.3860505190305836  |   -0.5898503502208925   | 1.3860505190305836  |  0.08504921197891235  |  0.19952020198106765  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  |  0.004693216504529118  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 |  0.664505398273468  |  0.11998217403888703  | 2.8962771344050946 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.9000226497650147 | 0.9993916034698487 |  -4.60291462478323  |      -16.47621426203117      |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | 44.85910080355618  | -0.8910617458021803 | 0.8009407162666321  | -1.5467033104197516 |   -0.13593688807430643    |  0.2022198259830475  | 0.003487794508691877 | 0.003984411002602428 | 0.0012955602767760865 |  0.004445872397627682  |      0.40416358709335326       |      0.22379742413759232       |       0.3405459553003311       |   -0.5898503502208925   | 1.3860505190305836  |   -0.5898503502208925   | 1.3860505190305836  |  0.08504921197891235  |  0.19952020198106765  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  |  0.004693216504529118  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 |  0.664505398273468  |  0.11998217403888703  | 2.8962771344050946 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.9000226497650147 | 0.9993916034698487 |  -4.60291462478323  |      -16.47621426203117      |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | 44.85910080355618  | -0.8910617458021803 | 0.8009407162666321  | -1.5467033104197516 |   -0.13593688807430643    |  0.2022198259830475  | 0.003487794508691877 | 0.003984411002602428 | 0.0012955602767760865 |  0.004445872397627682  |      0.40416358709335326       |      0.22379742413759232       |       0.3405459553003311       |   -0.5898503502208925   | 1.3860505190305836  |   -0.5898503502208925   | 1.3860505190305836  |  0.08504921197891235  |  0.19952020198106765  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  |  0.004693216504529118  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 |  0.664505398273468  |  0.11998217403888703  | 2.8962771344050946 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.9000226497650147 | 0.9993916034698487 |  -4.60291462478323  |      -16.47621426203117      |   -0.13593688807430643    | 44.85910080355618  | -1.5467033104197516 | 44.85910080355618  | -0.8910617458021803 | 0.8009407162666321  | -1.5467033104197516 |   -0.13593688807430643    |  0.2022198259830475  | 0.003487794508691877 | 0.003984411002602428 | 0.0012955602767760865 |  0.004445872397627682  |      0.40416358709335326       |      0.22379742413759232       |       0.3405459553003311       |   -0.5898503502208925   | 1.3860505190305836  |   -0.5898503502208925   | 1.3860505190305836  |  0.08504921197891235  |  0.19952020198106765  | 0.05025481265038252 |  0.09532190263271331  | 0.9182566285133362  |  0.004693216504529118  | 0.057926833629608154  | 0.001693321153288707  | 0.0038891625357791782 | 0.04093920225277543  | 0.0006940638646483422 | 0.016227655485272406 | 0.011122903507202863  | 0.01110487785190344  | 0.001418138199369423 |  0.664505398273468  |  0.11998217403888703  | 2.8962771344050946 |\n",
            "|     2244      |    2244    |                                                                      CC(=O)Oc1ccccc1C(=O)O                                                                       | 0.8728528618812561 | 0.9971104025840759 | -4.406400557063573  |      -8.788301506572092      |    -1.6280061592252302    | 73.18028314796899  | -1.4189941906350367 | 73.18028314796899  |  6.688880537362766  | 0.8109905004501343  | -1.4189941906350367 |    -1.6280061592252302    | 0.015933019947260617 | 0.012031708657741547 | 0.03166418168693781  | 0.0069421239662915465 | 0.00023585800809087232 |      0.36843977123498917       |      0.025489537790417672      |      0.09314111992716789       |   22.272858068277007    | 3.5387509518797153  |   22.272858068277007    | 3.5387509518797153  | 0.008252930385060609  |  0.05085575878620148  | 0.2533529117703438  |  0.13706251606345177  | 0.8547625064849853  |  0.002197452075779438  | 0.0074217274319380525 | 0.0006330780393909663 | 0.003623103746213019  | 0.029106763750314714 | 0.004367554653435945  | 0.008828719053417445 | 0.0012725539738312364 | 0.006973763182759285 | 0.001770349603611976 | 0.3067779362201691  | 0.0033732875715941192 | 2.214422774937376  |\n",
            "|     36314     |   36314    | CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O | 0.6864721655845643 | 0.9790336132049561 | -5.646112000638434  |     -11.105821218558324      |     2.591540834807309     | 99.71100285099035  | -5.970252675199414  | 99.71100285099035  | 3.3081267603121303  |  0.313214099407196  | -5.970252675199414  |     2.591540834807309     | 0.007987610111013056 |  0.0576062198728323  | 0.11490030437707902  |  0.06152134500443936  |   0.6012769103050232   |      0.042367860302329065      |      0.024853175133466722      |       0.6306055426597595       |   127.14940708554795    |  45.36265677255511  |   127.14940708554795    |  45.36265677255511  |  0.7438596844673157   |  0.3690257579088211   | 0.08611706085503101 |  0.33451235443353655  | 0.7962758541107178  |  0.20569149702787398   |  0.04734908863902092  |  0.25445756912231443  | 0.028367896378040314  | 0.19605415910482407  |  0.08270415738224983  | 0.29954059720039367  |  0.2619645059108734   | 0.037576324120163915 |  0.8947320818901062  | 0.08220251500606537 |  0.6043513178825378   | 4.349726015371661  |\n",
            "|    392622     |   392622   |                               CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)C[C@H](O)[C@H](Cc2ccccc2)NC(=O)OCc2cncs2)C(C)C)cs1                               | 0.6501427412033081 | 0.9992312788963318 | -5.213374414288153  |     -11.242016714513223      |     4.091072569121021     | 104.67844832732646 | -5.502664501109723  | 104.67844832732646 | -3.641342687285788  | 0.21091121435165405 | -5.502664501109723  |     4.091072569121021     |  0.2530102521181107  |  0.9651636362075806  |  0.9725804209709168  |  0.4466812551021576   |    0.9960165143013     |       0.5533619165420532       |      0.19899119734764098       |       0.7313636660575866       |    63.33160546515101    |  69.04919204080818  |    63.33160546515101    |  69.04919204080818  |  0.9145047664642334   |  0.1494164913892746   | 0.13090016543865204 |  0.13391368836164474  | 0.8784408926963806  |  0.002778747561387718  |  0.22553056478500366  |  0.09251121878623962  |  0.00802364144474268  | 0.046837984770536426 |  0.06080225184559822  |  0.4040724217891693  | 0.012484434805810452  | 0.08004205450415611  |  0.6769227504730224  | 0.10142785906791688 |   0.876216185092926   | 2.8012802232423804 |\n",
            "|      237      |    237     |                                                         CCN(CC)CCC[C@H](C)Nc1c2ccc(Cl)cc2nc2ccc(OC)cc12                                                          | 0.9144891142845154 | 0.9984958052635193 | -4.7424990566188985 |      -9.102830357109813      |    2.1027811814395543     | 90.98206486171682  | -3.770542018326757  | 90.98206486171682  |  63.97153654947105  | 0.9666250228881836  | -3.770542018326757  |    2.1027811814395543     |  0.8928620934486389  | 0.16664258092641832  | 0.03724945206195116  |  0.9382375478744507   |   0.3950761675834656   |      0.14729833751916885       |       0.6632840871810913       |       0.7245888948440552       |   27.166197478701292    | 213.08603338585036  |   27.166197478701292    | 213.08603338585036  |  0.8822207450866699   |  0.8656829237937927   | 0.4990366905927658  |  0.6714143872261047   | 0.6598935723304749  |  0.04410548508167267   |  0.7515201330184936   |  0.29046552777290346  |  0.04024618007242679  | 0.07671576887369155  | 0.018241093400865792  | 0.29513721019029615  |  0.06169416084885597  |   0.18509561419487   | 0.29118165001273155  | 0.5213869214057922  |  0.9679293155670166   | 2.980885871893334  |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------------+----------------------+----------------------+----------------------+-----------------------+------------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+---------------------+-------------------------+---------------------+-----------------------+-----------------------+---------------------+-----------------------+---------------------+------------------------+-----------------------+-----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+----------------------+---------------------+-----------------------+--------------------+\n",
            "Predicted results saved successfully to /content/ADMET_Predictions.csv\n",
            "Columns in df_combined (after renaming duplicates): ['Molecule Name', 'PubChem ID', 'SMILES', 'Bioavailability_Ma', 'HIA_Hou', 'Caco2_Wang', 'HydrationFreeEnergy_FreeSolv', 'Lipophilicity_AstraZeneca', 'PPBR_AZ', 'Solubility_AqSolDB', 'VDss_Lombardo', 'BBB_Martins', 'CYP1A2_Veith', 'CYP2C19_Veith', 'CYP2C9_Veith', 'CYP2D6_Veith', 'CYP3A4_Veith', 'CYP2C9_Substrate_CarbonMangels', 'CYP2D6_Substrate_CarbonMangels', 'CYP3A4_Substrate_CarbonMangels', 'Clearance_Hepatocyte_AZ', 'Half_Life_Obach', 'Pgp_Broccatelli', 'AMES', 'Carcinogens_Lagunin', 'ClinTox', 'DILI', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER-LBD', 'NR-ER', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'Skin_Reaction', 'hERG', 'LD50_Zhu']\n",
            "Columns in admet_existing_columns: ['Bioavailability_Ma', 'HIA_Hou', 'Caco2_Wang', 'HydrationFreeEnergy_FreeSolv', 'Lipophilicity_AstraZeneca', 'PPBR_AZ', 'Solubility_AqSolDB', 'PPBR_AZ', 'VDss_Lombardo', 'BBB_Martins', 'Solubility_AqSolDB', 'Lipophilicity_AstraZeneca', 'CYP1A2_Veith', 'CYP2C19_Veith', 'CYP2C9_Veith', 'CYP2D6_Veith', 'CYP3A4_Veith', 'CYP2C9_Substrate_CarbonMangels', 'CYP2D6_Substrate_CarbonMangels', 'CYP3A4_Substrate_CarbonMangels', 'Clearance_Hepatocyte_AZ', 'Half_Life_Obach', 'Clearance_Hepatocyte_AZ', 'Half_Life_Obach', 'Pgp_Broccatelli', 'AMES', 'Carcinogens_Lagunin', 'ClinTox', 'DILI', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER-LBD', 'NR-ER', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'Skin_Reaction', 'hERG', 'LD50_Zhu']\n",
            "Shape of df_combined[admet_existing_columns]: (12, 42)\n",
            "Normalized ADMET Results:\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+----------------------+--------------------+----------------------+---------------------+----------------------+---------------------------+-----------------------+----------------------+----------------------+----------------------+-----------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+----------------------+-------------------------+----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+-----------------------+-----------------------+-----------------------+------------------------+----------------------+---------------------+-----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+---------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "| Molecule Name | PubChem ID |                                                                              SMILES                                                                              | Bioavailability_Ma  |      HIA_Hou       |     Caco2_Wang      | HydrationFreeEnergy_FreeSolv | Lipophilicity_AstraZeneca |      PPBR_AZ       |  Solubility_AqSolDB  |      PPBR_AZ       |    VDss_Lombardo     |     BBB_Martins     |  Solubility_AqSolDB  | Lipophilicity_AstraZeneca |     CYP1A2_Veith      |    CYP2C19_Veith     |     CYP2C9_Veith     |     CYP2D6_Veith     |     CYP3A4_Veith      | CYP2C9_Substrate_CarbonMangels | CYP2D6_Substrate_CarbonMangels | CYP3A4_Substrate_CarbonMangels | Clearance_Hepatocyte_AZ |   Half_Life_Obach    | Clearance_Hepatocyte_AZ |   Half_Life_Obach    |   Pgp_Broccatelli    |         AMES         |  Carcinogens_Lagunin  |       ClinTox        |        DILI         |       NR-AR-LBD       |        NR-AhR         |     NR-Aromatase      |       NR-ER-LBD        |        NR-ER         |    NR-PPAR-gamma    |        SR-ARE         |       SR-ATAD5       |        SR-HSE        |        SR-MMP         |    Skin_Reaction     |        hERG         |      LD50_Zhu       |  Absorption Score   | Distribution Score |  Metabolism Score   |   Excretion Score   |   Toxicity Score    | Overall ADMET Score |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+----------------------+--------------------+----------------------+---------------------+----------------------+---------------------------+-----------------------+----------------------+----------------------+----------------------+-----------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+----------------------+-------------------------+----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+-----------------------+-----------------------+-----------------------+------------------------+----------------------+---------------------+-----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+---------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "|      237      |    237     |                                                         CCN(CC)CCC[C@H](C)Nc1c2ccc(Cl)cc2nc2ccc(OC)cc12                                                          | 0.9257892001281955  | 0.9287416020524901 | 0.6721399454529018  |      0.7770734214270241      |    0.6523406160109972     | 0.7710375650592453 |  0.4833192103531342  | 0.7710375650592453 |         1.0          |         1.0         |  0.4833192103531342  |    0.6523406160109972     |  1.0000000000000002   | 0.16965673810574103  | 0.03434356606572944  |         1.0          |   0.396513335622693   |       0.1670669198980621       |       0.9999999999999999       |       0.8873621516878644       |   0.2172867479120799    |         1.0          |   0.2172867479120799    |         1.0          |   0.96453411835031   |         1.0          |  0.7002447460487152   |         1.0          |  0.708274965715353  |  0.04994605513696847  |  0.9999999999999999   |  0.35916889987110795  |  0.03743176698946213   | 0.049958707230748155 | 0.18485568673371697 |  0.3810881757237104   |  0.1696005358087951  |         1.0          |  0.3243691804654933   |  0.7178214643805353  | 0.9999999999999999  | 0.4003238507061715  | 0.7117138951907365  | 0.7266743478558442 | 0.5907930172670207  | 0.6798215228348939  | 0.5342990608712223  | 0.6486603688039435  |\n",
            "|    392622     |   392622   |                               CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)C[C@H](O)[C@H](Cc2ccccc2)NC(=O)OCc2cncs2)C(C)C)cs1                               | 0.27096339855797624 | 0.9638386228788249 | 0.3218858468218335  |       0.55162674955253       |            1.0            | 0.9999999999999999 | 0.10273821530363936  | 0.9999999999999999 |         0.0          | 0.16112274423537812 | 0.10273821530363936  |            1.0            |  0.2795887954322207   |         1.0          |  0.9999999999999999  | 0.47536101564554006  |  1.0000000000000002   |       0.813588247442824        |      0.27275938387078635       |       0.8968826152955215       |   0.5004057256831443    | 0.33912287897954374  |   0.5004057256831443    | 0.33912287897954374  |  0.9999999999999999  |  0.1665665599028351  |  0.13145097809978148  | 0.19546361536573179  | 0.9550429265395866  | 0.0028326400277093166 |  0.29557627788951896  |  0.11385809547754301  |  0.004497707955838879  | 0.018606330409496143 | 0.6332319863372768  |  0.5260853460676522   | 0.03219307125163521  | 0.41488364884430856  |  0.7561782921273993   | 0.031422710885452926 | 0.9049167411615349  | 0.32164028470500416 | 0.6313791048418443  | 0.5458248968553321 | 0.6181031055843558  | 0.5358114418650752  | 0.3237910125322532  | 0.5309819123357722  |\n",
            "|   146502750   | 146502750  |                                              C[C@H]1C(=O)C(C#N)=C[C@@]2(C)c3nc(-c4ccnnc4)nc(-c4ccccc4F)c3CC[C@H]12                                               | 0.9112585514638447  |        1.0         | 0.7714875342218406  |      0.874734278558035       |     0.907341044792767     | 0.847134925074784  | 0.057780001716429386 | 0.847134925074784  | 0.028263976614457415 | 0.9534701240335969  | 0.057780001716429386 |     0.907341044792767     |  0.5283481930211373   |  0.4862665768360816  |  0.7958623704928786  | 0.029731007628785066 |  0.8381211629380526   |      0.16604994498704612       |      0.08011440627153998       |       0.9215513825532676       |   0.4426723432662869    | 0.009335520344292405 |   0.4426723432662869    | 0.009335520344292405 |  0.9981229214157193  | 0.46121276795844246  |  0.06877695077784954  |  0.3102388071107303  | 0.9829334292260063  |  0.1430544251570358   |   0.670872713183763   |          1.0          |  0.05450167932698581   | 0.052545736589975085 | 0.9999999999999999  |  0.9999999999999998   |         1.0          |  0.4401308483162251  |  0.8157266797710885   |  0.2587359654616844  | 0.8221706984658367  | 0.6920138784094134  | 0.7181992307411681  | 0.5757807554770019 | 0.39583839766249557 | 0.3804277297273756  | 0.5748773282208844  | 0.5290246883657852  |\n",
            "|     36314     |   36314    | CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O | 0.36095685738153516 |        0.0         |         0.0         |      0.5659802541834207      |    0.7378018723747624     | 0.9169592166754709 |         0.0          | 0.9169592166754709 | 0.10278322009129781  | 0.27468367497765167 |         0.0          |    0.7378018723747624     | 0.0037170306379554962 | 0.05627512204999893  | 0.11451202795900942  | 0.06427909681162072  |  0.6035877966683005   |              0.0               |              0.0               |       0.7552890445681634       |   0.9999999999999999    | 0.23044311976756907  |   0.9999999999999999    | 0.23044311976756907  |  0.8125364182043828  | 0.42209955459891074  |  0.06225830349107367  |  0.4957216911317573  | 0.8622680575140519  |  0.2341577619752923   | 0.056949380669047686  |  0.3145467779968056   |  0.02529119355640097   | 0.17518694259285375  | 0.8639657285013362  |  0.38694926184106654  |  0.7288119469206513  | 0.17836245637262452  |          1.0          |         0.0          | 0.6230618158697524  |         1.0         | 0.42364592896654224 | 0.460873634146177  | 0.3382121965191822  | 0.6546845315479042  | 0.4370371101783309  | 0.4628906802716273  |\n",
            "|   162394460   | 162394460  |                                                          NC[C@@H](C(=O)NNCc1ccc(O)c(Br)c1)c1cccc(Cl)c1                                                           | 0.9488532919929955  | 0.9747888069083643 | 0.09344075734071922 |      0.6198135685046038      |     0.630121080363017     | 0.7268629839631605 |  0.6244357556466257  | 0.7268629839631605 | 0.12410557500646212  | 0.3133585979474586  |  0.6244357556466257  |     0.630121080363017     |  0.9124246665351152   |   0.82526362442879   | 0.41691186214496667  |  0.8661056579132874  |  0.9639518495331633   |       0.2752662332461331       |       0.3909626767617612       |       0.6923710007321631       |   0.20403106345472138   | 0.11261487765009104  |   0.20403106345472138   | 0.11261487765009104  | 0.05070733289347626  |  0.8494804206431222  |  0.9999999999999999   | 0.44076409390288357  | 0.28930917318286864 | 0.012123186091904539  |  0.7667581455517205   |  0.1585287274219612   |  0.062463790533069226  | 0.09191510140874301  | 0.22046835062632808 |  0.3706854099755083   | 0.04556889459638934  |  0.9898138576796014  |  0.4643020086222855   |  0.9999999999999999  | 0.5742369288072743  | 0.3360809665418807  | 0.6599736064692289  | 0.5500379766124409 | 0.4980457877920837  | 0.13679984302062023 | 0.45132347385797306 | 0.4592361375504693  |\n",
            "|     5757      |    5757    |                                                       C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C@@H]2O                                                       |         0.0         | 0.9790365612929293 | 0.5030987357730545  |      0.8943741940872751      |    0.8646209452374969     | 0.7088179767542059 |  0.3161758346374196  | 0.7088179767542059 | 0.10409817216587713  |         0.0         |  0.3161758346374196  |    0.8646209452374969     |  0.6750758890831516   | 0.26569840826299546  | 0.11349091268206678  | 0.010673821156610057 |  0.03904438187381404  |       0.7485045150994074       |       0.4140179072405966       |       1.0000000000000002       |   0.6810236261044798    |         0.0          |   0.6810236261044798    |         0.0          | 0.08760345521641082  | 0.03450969148630737  |          0.0          |         0.0          |         0.0         |          1.0          |  0.01663289266888784  |  0.29559704020166594  |          1.0           |         1.0          | 0.15815471415274202 |  0.5013959549408061   | 0.059312997797993726 | 0.41985686079827284  |  0.7666853867652207   |  0.7509043883142336  | 0.2751263334158346  |         0.0         | 0.6155739004411505  | 0.4854159606780152 | 0.38571275730063354 | 0.28993014148507407 | 0.36930448591423326 | 0.4291874491638213  |\n",
            "|     3672      |    3672    |                                                                 CC(C)Cc1ccc([C@@H](C)C(=O)O)cc1                                                                  | 0.9999999999999998  | 0.9411873460494817 |         1.0         |             1.0              |    0.43861634809415706    | 0.7605429289670201 |  0.6586536615385947  | 0.7605429289670201 | 0.06940931576904955  | 0.5180186189952303  |  0.6586536615385947  |    0.43861634809415706    |          0.0          | 0.04788697359728302  | 0.053452794247573175 | 0.006236021894231346 | 0.0020726351872908826 |              1.0               |      0.036343976627082525      |      0.24347664532290106       |   0.18326320844485525   | 0.06162581589830139  |   0.18326320844485525   | 0.06162581589830139  |         0.0          |         0.0          |  0.6691338808284556   | 0.018107470681752797 | 0.5480545259082374  |          0.0          |          0.0          | 0.0013324766028637307 |  0.014255161283488265  | 0.10462424467357614  | 0.5777336119496136  | 0.0030609260155824575 |         0.0          |         0.0          |  0.03222528909350902  |  0.4628993078825654  | 0.00596051272664025 | 0.1274235595351909  | 0.7656813223249024  | 0.5378817264954779 | 0.15660392463022296 | 0.09795560973726267 | 0.1508712333636162  | 0.3417987633102964  |\n",
            "|     2244      |    2244    |                                                                      CC(=O)Oc1ccccc1C(=O)O                                                                       | 0.8226499285862758  | 0.8626297735327455 | 0.9221421489992854  |      0.8102212967006448      |            0.0            | 0.4734451898386033 |         1.0          | 0.4734451898386033 | 0.15278484426725414  | 0.8272384868315432  |         1.0          |            0.0            |  0.01266279261156051  | 0.008884401352909167 | 0.028577209073203668 | 0.006026588373943286 |          0.0          |       0.5191611748054854       |     0.0009967604089061999      |              0.0               |   0.1789794999395073    | 0.03854455957639159  |   0.1789794999395073    | 0.03854455957639159  | 0.004429467218253842 | 0.051883242752095625 |  0.32064806728439865  |  0.2001768119446387  | 0.9283069909481008  | 0.0021699500318886054 | 0.0034772548054074846 |          0.0          |          0.0           |         0.0          | 0.03869975035203023 |          0.0          | 0.000886330858655381 | 0.007915506793469255 | 0.0003942750549570373 | 0.36705551328947117  |         0.0         | 0.06454348895376294 | 0.6364533527496159  | 0.4908642138470005 | 0.08427975380481717 | 0.08789551725001031 | 0.11683277547463974 | 0.2832651226252167  |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.8899535875020328  | 0.9714893592814065 | 0.7759678875170786  |             0.0              |    0.2608932910392677     |        0.0         |  0.9719398227505993  |        0.0         | 0.04067687950180436  | 0.8160827618975115  |  0.9719398227505993  |    0.2608932910392677     |   0.222403694587023   |         0.0          |         0.0          |         0.0          | 0.0042278531551401865 |       0.5760394816688856       |       0.3116143740065097       |      0.34767354605629586       |           0.0           | 0.02866743056414953  |           0.0           | 0.02866743056414953  | 0.08879466454886488  | 0.22486624559377022  | 0.0068489053747021605 |  0.1376990471846507  |         1.0         | 0.005015177906188903  |  0.07111546864457445  | 0.0013138844638159153 | 0.00027193375367569545 | 0.012416418478780826 |         0.0         | 0.009848283492798286  | 0.02839129102036067  | 0.03092455806723626  |          0.0          |  0.9517403220358845  | 0.12089384450754405 | 0.3632575488919718  | 0.5103077061880251  | 0.4153032336223812 | 0.12660781755017944 | 0.02922590513543279 | 0.1743884076127032  | 0.25116661402174434 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.8899535875020328  | 0.9714893592814065 | 0.7759678875170786  |             0.0              |    0.2608932910392677     |        0.0         |  0.9719398227505993  |        0.0         | 0.04067687950180436  | 0.8160827618975115  |  0.9719398227505993  |    0.2608932910392677     |   0.222403694587023   |         0.0          |         0.0          |         0.0          | 0.0042278531551401865 |       0.5760394816688856       |       0.3116143740065097       |      0.34767354605629586       |           0.0           | 0.02866743056414953  |           0.0           | 0.02866743056414953  | 0.08879466454886488  | 0.22486624559377022  | 0.0068489053747021605 |  0.1376990471846507  |         1.0         | 0.005015177906188903  |  0.07111546864457445  | 0.0013138844638159153 | 0.00027193375367569545 | 0.012416418478780826 |         0.0         | 0.009848283492798286  | 0.02839129102036067  | 0.03092455806723626  |          0.0          |  0.9517403220358845  | 0.12089384450754405 | 0.3632575488919718  | 0.5103077061880251  | 0.4153032336223812 | 0.12660781755017944 | 0.02922590513543279 | 0.1743884076127032  | 0.25116661402174434 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.8899535875020328  | 0.9714893592814065 | 0.7759678875170786  |             0.0              |    0.2608932910392677     |        0.0         |  0.9719398227505993  |        0.0         | 0.04067687950180436  | 0.8160827618975115  |  0.9719398227505993  |    0.2608932910392677     |   0.222403694587023   |         0.0          |         0.0          |         0.0          | 0.0042278531551401865 |       0.5760394816688856       |       0.3116143740065097       |      0.34767354605629586       |           0.0           | 0.02866743056414953  |           0.0           | 0.02866743056414953  | 0.08879466454886488  | 0.22486624559377022  | 0.0068489053747021605 |  0.1376990471846507  |         1.0         | 0.005015177906188903  |  0.07111546864457445  | 0.0013138844638159153 | 0.00027193375367569545 | 0.012416418478780826 |         0.0         | 0.009848283492798286  | 0.02839129102036067  | 0.03092455806723626  |          0.0          |  0.9517403220358845  | 0.12089384450754405 | 0.3632575488919718  | 0.5103077061880251  | 0.4153032336223812 | 0.12660781755017944 | 0.02922590513543279 | 0.1743884076127032  | 0.25116661402174434 |\n",
            "|     2153      |    2153    |                                                                    Cn1c(=O)c2[nH]cnc2n(C)c1=O                                                                    | 0.8899535875020328  | 0.9714893592814065 | 0.7759678875170786  |             0.0              |    0.2608932910392677     |        0.0         |  0.9719398227505993  |        0.0         | 0.04067687950180436  | 0.8160827618975115  |  0.9719398227505993  |    0.2608932910392677     |   0.222403694587023   |         0.0          |         0.0          |         0.0          | 0.0042278531551401865 |       0.5760394816688856       |       0.3116143740065097       |      0.34767354605629586       |           0.0           | 0.02866743056414953  |           0.0           | 0.02866743056414953  | 0.08879466454886488  | 0.22486624559377022  | 0.0068489053747021605 |  0.1376990471846507  |         1.0         | 0.005015177906188903  |  0.07111546864457445  | 0.0013138844638159153 | 0.00027193375367569545 | 0.012416418478780826 |         0.0         | 0.009848283492798286  | 0.02839129102036067  | 0.03092455806723626  |          0.0          |  0.9517403220358845  | 0.12089384450754405 | 0.3632575488919718  | 0.5103077061880251  | 0.4153032336223812 | 0.12660781755017944 | 0.02922590513543279 | 0.1743884076127032  | 0.25116661402174434 |\n",
            "+---------------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------------------+---------------------+------------------------------+---------------------------+--------------------+----------------------+--------------------+----------------------+---------------------+----------------------+---------------------------+-----------------------+----------------------+----------------------+----------------------+-----------------------+--------------------------------+--------------------------------+--------------------------------+-------------------------+----------------------+-------------------------+----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+-----------------------+-----------------------+-----------------------+------------------------+----------------------+---------------------+-----------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+---------------------+---------------------+--------------------+---------------------+---------------------+---------------------+---------------------+\n",
            "Normalized results saved successfully to /content/Normalized_ADMET_Scores.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Optimizing Ligand Structure for ADMET Properties Using Genetic Algorithm**"
      ],
      "metadata": {
        "id": "sPIzV9dB7VaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sectionk provides a framework for optimizing the molecular structure of a ligand to enhance specific ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties using a genetic algorithm (GA). The process involves applying various mutation operations to the molecular structure (SMILES format) and evaluating the optimized molecules based on their predicted ADMET properties.\n",
        "\n",
        "Users can specify the ADMET property they want to optimize by selecting from the properties listed in the provided .txt file. The default target property in this notebook is Lipophilicity (AstraZeneca model), but users can modify it as needed.\n",
        "\n",
        "---\n",
        "\n",
        "**Mutation Operators and Their Working Logic**\n",
        "\n",
        "A mutation operator introduces structural changes to a molecule, which helps explore new chemical variants. Below are the mutation operators used in this notebook:\n",
        "\n",
        "1. *Atom Deletion* - Removes a random atom from the molecule while maintaining chemical validity.\n",
        "2. Atom Insertion - Adds a new carbon, nitrogen, or oxygen atom and connects it to a random existing atom.\n",
        "3. *Bond Order Modification* - Changes a randomly selected bond type (single ↔ double, double ↔ triple).\n",
        "4. *Substructure Replacement* - Replaces specific functional groups (e.g., hydroxyl groups with amines, methyl groups with chlorine).\n",
        "5. *Ring Modification* - Breaks a randomly selected ring in the molecule, potentially altering its pharmacokinetic properties.\n",
        "6. *Atom Swapping* - Exchanges the atomic identities of two randomly chosen atoms (e.g., swapping carbon and nitrogen).\n",
        "7. *Stereochemistry Alteration* - Flips the chirality of a randomly chosen chiral center in the molecule.\n",
        "\n",
        "\n",
        "Each mutation operator generates a new molecular structure, which is then assessed using the ADMET prediction model. The GA iteratively selects and refines structures with improved ADMET properties over multiple generations, optimizing the molecular design for better drug-like characteristics."
      ],
      "metadata": {
        "id": "Vp7z7ZLq1ji3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from deap import base, creator, tools, algorithms\n",
        "from admet_ai import ADMETModel\n",
        "\n",
        "# Define the target ADMET property to optimize\n",
        "target_property = \"Lipophilicity_AstraZeneca\"  # Change if needed\n",
        "\n",
        "# Prompt user for the initial ligand SMILES\n",
        "initial_smiles = input(\"Enter the SMILES of the ligand to enhance: \")\n",
        "\n",
        "# Global list to store mutation details (operator used, mutated SMILES, predicted lipophilicity)\n",
        "mutation_results = []\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper Function: Check if a SMILES corresponds to a chemically valid molecule\n",
        "def is_valid_molecule(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return False\n",
        "    try:\n",
        "        Chem.SanitizeMol(mol)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Sanitization failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Mutation Operator 1: Atom Deletion\n",
        "def mutate_atom_deletion(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None or mol.GetNumAtoms() < 2:  # Avoid deleting if only one atom\n",
        "        return None\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    atom_idx = random.randint(0, rw_mol.GetNumAtoms() - 1)\n",
        "    rw_mol.RemoveAtom(atom_idx)\n",
        "    return rw_mol\n",
        "\n",
        "# Mutation Operator 2: Atom Insertion (Addition)\n",
        "def mutate_atom_insertion(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    # Choose a random atom to attach the new atom\n",
        "    idx = random.randint(0, rw_mol.GetNumAtoms() - 1)\n",
        "    # Choose a new atom type from a simple allowed set (can be extended)\n",
        "    new_atom_symbol = random.choice([\"C\", \"N\", \"O\"])\n",
        "    new_atom = Chem.Atom(new_atom_symbol)\n",
        "    new_idx = rw_mol.AddAtom(new_atom)\n",
        "    # Attach the new atom via a single bond\n",
        "    rw_mol.AddBond(idx, new_idx, Chem.BondType.SINGLE)\n",
        "    return rw_mol\n",
        "\n",
        "# Mutation Operator 3: Bond Order Modification\n",
        "def mutate_bond_order_modification(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None or mol.GetNumBonds() == 0:\n",
        "        return None\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    bond_idx = random.randint(0, rw_mol.GetNumBonds() - 1)\n",
        "    bond = rw_mol.GetBondWithIdx(bond_idx)\n",
        "    current_order = bond.GetBondType()\n",
        "    # Toggle between single and double (or modify triple bonds slightly)\n",
        "    if current_order == Chem.BondType.SINGLE:\n",
        "        new_order = Chem.BondType.DOUBLE\n",
        "    elif current_order == Chem.BondType.DOUBLE:\n",
        "        new_order = Chem.BondType.SINGLE\n",
        "    elif current_order == Chem.BondType.TRIPLE:\n",
        "        new_order = Chem.BondType.DOUBLE\n",
        "    else:\n",
        "        new_order = Chem.BondType.SINGLE\n",
        "    bond.SetBondType(new_order)\n",
        "    return rw_mol\n",
        "\n",
        "# Mutation Operator 4: Substructure Replacement\n",
        "def mutate_substructure_replacement(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    # Example 1: Replace a hydroxyl group (-OH) with an amine (-NH2)\n",
        "    hydroxyl_smarts = \"[OX2H]\"  # Hydroxyl group\n",
        "    hydroxyl = Chem.MolFromSmarts(hydroxyl_smarts)\n",
        "    amine = Chem.MolFromSmiles(\"N\")  # Simple amine fragment\n",
        "    if mol.HasSubstructMatch(hydroxyl):\n",
        "        mutated = Chem.ReplaceSubstructs(mol, hydroxyl, amine, replaceAll=True)\n",
        "        if mutated:\n",
        "            return mutated[0]\n",
        "    # Example 2: Replace a methyl group with chlorine\n",
        "    methyl_smarts = \"[CH3]\"\n",
        "    methyl = Chem.MolFromSmarts(methyl_smarts)\n",
        "    chlorine = Chem.MolFromSmiles(\"Cl\")\n",
        "    if mol.HasSubstructMatch(methyl):\n",
        "        mutated = Chem.ReplaceSubstructs(mol, methyl, chlorine, replaceAll=True)\n",
        "        if mutated:\n",
        "            return mutated[0]\n",
        "    return None\n",
        "\n",
        "# Mutation Operator 5: Ring Modification\n",
        "def mutate_ring_modification(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    ring_info = mol.GetRingInfo()\n",
        "    if not ring_info.NumRings():\n",
        "        return None\n",
        "    rings = ring_info.AtomRings()\n",
        "    selected_ring = random.choice(rings)\n",
        "    if len(selected_ring) < 2:\n",
        "        return None\n",
        "    # Find bonds in the selected ring\n",
        "    bond_indices = []\n",
        "    for i in range(len(selected_ring)):\n",
        "        a1 = selected_ring[i]\n",
        "        a2 = selected_ring[(i + 1) % len(selected_ring)]\n",
        "        bond = mol.GetBondBetweenAtoms(a1, a2)\n",
        "        if bond:\n",
        "            bond_indices.append(bond.GetIdx())\n",
        "    if not bond_indices:\n",
        "        return None\n",
        "    bond_to_break = random.choice(bond_indices)\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    bond = rw_mol.GetBondWithIdx(bond_to_break)\n",
        "    idx1 = bond.GetBeginAtomIdx()\n",
        "    idx2 = bond.GetEndAtomIdx()\n",
        "    rw_mol.RemoveBond(idx1, idx2)\n",
        "    return rw_mol\n",
        "\n",
        "# Mutation Operator 6: Atom Swapping\n",
        "def mutate_atom_swapping(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None or mol.GetNumAtoms() < 2:\n",
        "        return None\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    idx1, idx2 = random.sample(range(rw_mol.GetNumAtoms()), 2)\n",
        "    atom1 = rw_mol.GetAtomWithIdx(idx1)\n",
        "    atom2 = rw_mol.GetAtomWithIdx(idx2)\n",
        "    # Swap the atomic numbers (i.e., swap element types)\n",
        "    num1 = atom1.GetAtomicNum()\n",
        "    num2 = atom2.GetAtomicNum()\n",
        "    atom1.SetAtomicNum(num2)\n",
        "    atom2.SetAtomicNum(num1)\n",
        "    return rw_mol\n",
        "\n",
        "# Mutation Operator 7: Stereochemistry Alteration\n",
        "def mutate_stereochemistry_alteration(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    rw_mol = Chem.RWMol(mol)\n",
        "    chiral_centers = Chem.FindMolChiralCenters(rw_mol, includeUnassigned=True)\n",
        "    if not chiral_centers:\n",
        "        return None\n",
        "    # Randomly pick one chiral center and flip its configuration\n",
        "    center_idx, current_config = random.choice(chiral_centers)\n",
        "    atom = rw_mol.GetAtomWithIdx(center_idx)\n",
        "    if atom.GetChiralTag() == Chem.CHI_TETRAHEDRAL_CW:\n",
        "        atom.SetChiralTag(Chem.CHI_TETRAHEDRAL_CCW)\n",
        "    elif atom.GetChiralTag() == Chem.CHI_TETRAHEDRAL_CCW:\n",
        "        atom.SetChiralTag(Chem.CHI_TETRAHEDRAL_CW)\n",
        "    Chem.AssignStereochemistry(rw_mol, force=True, cleanIt=True)\n",
        "    return rw_mol\n",
        "\n",
        "# List of mutation operators (name, function)\n",
        "mutation_operators = [\n",
        "    (\"Atom Deletion\", mutate_atom_deletion),\n",
        "    (\"Atom Insertion\", mutate_atom_insertion),\n",
        "    (\"Bond Order Modification\", mutate_bond_order_modification),\n",
        "    (\"Substructure Replacement\", mutate_substructure_replacement),\n",
        "    (\"Ring Modification\", mutate_ring_modification),\n",
        "    (\"Atom Swapping\", mutate_atom_swapping),\n",
        "    (\"Stereochemistry Alteration\", mutate_stereochemistry_alteration)\n",
        "]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Prompt the user to select a mutation operator\n",
        "print(\"\\nAvailable mutation operators:\")\n",
        "for i, (op_name, _) in enumerate(mutation_operators, start=1):\n",
        "    print(f\"{i}: {op_name}\")\n",
        "print(\"0: Random selection (default)\")\n",
        "operator_choice_input = input(\"Enter the number of the mutation operator to use (0 for random): \")\n",
        "try:\n",
        "    operator_choice = int(operator_choice_input)\n",
        "except:\n",
        "    operator_choice = 0\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ADMET Prediction Function\n",
        "def predict_admet(smiles_list):\n",
        "    model = ADMETModel()\n",
        "    # Ensure at least two SMILES are provided for robustness.\n",
        "    if len(smiles_list) == 1:\n",
        "        smiles_list = smiles_list * 2\n",
        "    preds = model.predict(smiles=smiles_list)\n",
        "    if not isinstance(preds, pd.DataFrame):\n",
        "        print(\"Warning: predict_admet did not return a DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "    return preds\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Master Mutation Function: now chooses the operator based on user input (if provided)\n",
        "def mutate_smiles(smiles):\n",
        "    # Use user-selected operator if valid; otherwise, use random selection\n",
        "    if operator_choice == 0:\n",
        "        operator_name, operator_func = random.choice(mutation_operators)\n",
        "    else:\n",
        "        if 1 <= operator_choice <= len(mutation_operators):\n",
        "            operator_name, operator_func = mutation_operators[operator_choice - 1]\n",
        "        else:\n",
        "            operator_name, operator_func = random.choice(mutation_operators)\n",
        "\n",
        "    mutated_mol = operator_func(smiles)\n",
        "    if mutated_mol is None:\n",
        "        print(f\"Mutation operator '{operator_name}' failed. Returning original SMILES.\")\n",
        "        return smiles\n",
        "    mutated_smiles = Chem.MolToSmiles(mutated_mol)\n",
        "    if not is_valid_molecule(mutated_smiles):\n",
        "        print(f\"Mutated SMILES from '{operator_name}' is chemically invalid. Returning original SMILES.\")\n",
        "        return smiles\n",
        "    preds = predict_admet([mutated_smiles])\n",
        "    if not preds.empty and target_property in preds.columns:\n",
        "        predicted_value = preds[target_property].iloc[0]\n",
        "    else:\n",
        "        predicted_value = \"N/A\"\n",
        "    print(f\"Mutation Operator: {operator_name}\")\n",
        "    print(f\"Mutated SMILES: {mutated_smiles}\")\n",
        "    print(f\"Predicted {target_property}: {predicted_value}\\n\")\n",
        "\n",
        "    # Save the mutation details in the global list\n",
        "    mutation_results.append({\n",
        "        'Mutation Operator': operator_name,\n",
        "        'Mutated SMILES': mutated_smiles,\n",
        "        'Predicted Lipophilicity': predicted_value\n",
        "    })\n",
        "\n",
        "    return mutated_smiles\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Evaluation Function for the GA (uses ADMET predictions as fitness)\n",
        "def evaluate(individual):\n",
        "    smiles = individual[0]\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        print(f\"Warning: Invalid SMILES: {smiles}. Fitness set to -1.0.\")\n",
        "        return (-1.0,)\n",
        "    preds = predict_admet([smiles])\n",
        "    if not isinstance(preds, pd.DataFrame) or preds.empty:\n",
        "        print(\"Warning: No valid predictions received. Fitness set to 0.0.\")\n",
        "        return (0.0,)\n",
        "    if target_property not in preds.columns:\n",
        "        print(f\"Warning: Target property '{target_property}' not found. Fitness set to 0.0.\")\n",
        "        return (0.0,)\n",
        "    try:\n",
        "        fitness_value = float(preds[target_property].iloc[0])\n",
        "        return (fitness_value,)\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing target property value: {e}. Fitness set to 0.0.\")\n",
        "        return (0.0,)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# GA Setup using DEAP\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "# Initialize individuals with the user-provided SMILES string.\n",
        "toolbox.register(\"attr_smiles\", lambda: initial_smiles)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_smiles, n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Crossover operator is not used (each individual is a single SMILES string)\n",
        "def mate_operator(ind1, ind2):\n",
        "    return ind1, ind2\n",
        "toolbox.register(\"mate\", mate_operator)\n",
        "\n",
        "# Mutation operator: updates the individual's SMILES using our master mutation function.\n",
        "def mutate_operator(individual):\n",
        "    individual[0] = mutate_smiles(individual[0])\n",
        "    return (individual,)\n",
        "toolbox.register(\"mutate\", mutate_operator)\n",
        "\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Run the Genetic Algorithm\n",
        "def run_ga():\n",
        "    pop = toolbox.population(n=10)  # Population size\n",
        "    hof = tools.HallOfFame(1)         # To keep track of the best individual\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "    stats.register(\"max\", np.max)\n",
        "\n",
        "    # Run the GA for 5 generations with a high mutation probability.\n",
        "    algorithms.eaSimple(pop, toolbox, cxpb=0.0, mutpb=0.8, ngen=5,\n",
        "                         stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "    best_ind = hof[0]\n",
        "    print(\"Best molecule:\", best_ind[0])\n",
        "    return best_ind[0]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    best_smiles = run_ga()\n",
        "    print(\"Optimized SMILES:\", best_smiles)\n",
        "\n",
        "    # Create a DataFrame from the mutation results\n",
        "    df_results = pd.DataFrame(mutation_results)\n",
        "\n",
        "    # Print the table\n",
        "    print(\"\\nAll chemically feasible mutations generated:\")\n",
        "    print(df_results.to_string(index=False))\n",
        "\n",
        "    # Save the data as a CSV file\n",
        "    df_results.to_csv(\"mutation_results.csv\", index=False)\n",
        "    print(\"\\nResults saved as 'mutation_results.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYKg2j793SXd",
        "outputId": "79ce75a7-24f4-4c55-d8c3-684fc196bcff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the SMILES of the ligand to enhance: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "\n",
            "Available mutation operators:\n",
            "1: Atom Deletion\n",
            "2: Atom Insertion\n",
            "3: Bond Order Modification\n",
            "4: Substructure Replacement\n",
            "5: Ring Modification\n",
            "6: Atom Swapping\n",
            "7: Stereochemistry Alteration\n",
            "0: Random selection (default)\n",
            "Enter the number of the mutation operator to use (0 for random): 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 4481.09it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 103.62it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.07it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.06it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.76it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.45it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.12it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 4457.28it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 183.43it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 14.10it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 38.13it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 36.09it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.92it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 35.83it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 34.36it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.68it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20020.54it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 214.03it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 15.08it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 36.75it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 35.41it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.77it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 35.31it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 34.37it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.66it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 23301.69it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 153.94it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.25it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.30it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.69it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.06it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.70it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 33.14it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.17it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 23301.69it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 148.32it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 14.34it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.95it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.07it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.92it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.83it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.65it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.66it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22429.43it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 151.61it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.16it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.85it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.05it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.78it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.17it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.12it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 3499.63it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 153.10it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.48it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.93it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.93it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.64it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.90it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.27it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.13it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 23301.69it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 132.96it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 14.08it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.23it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.78it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.13it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.70it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.85it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.35it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 26379.27it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 142.48it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.73it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 34.03it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 34.25it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.48it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.55it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.57it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.64it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 3153.61it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 130.38it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  7.02it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.16it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.72it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.79it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.26it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.83it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen\tnevals\tavg    \tmax    \n",
            "0  \t10    \t2.59154\t2.59154\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1629.81it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 150.92it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 14.04it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.65it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.85it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.51it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 23.57it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.34it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Stereochemistry Alteration\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.5651896011993918\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 16810.84it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 153.24it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 14.07it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.38it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.83it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.35it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.97it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.77it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.19it/s]\n",
            "[22:11:09] non-ring atom 0 marked aromatic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Bond Order Modification\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.5915405640321625\n",
            "\n",
            "Mutated SMILES from 'Atom Deletion' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1382.66it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 163.99it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.74it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.31it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.37it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.80it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.66it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.86it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Ring Modification\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@@](C)(OC(C)=O)[C@H](O)C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.590862197364279\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 3869.28it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 160.26it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  8.24it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.93it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.15it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.80it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 22.88it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.98it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Swapping\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.5915405640321625\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 728.75it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 128.85it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.03it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 23.20it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.34it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.42it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.58it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.08it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Substructure Replacement\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H](N)C[C@H]3OC[C@@]3(OC(C)=O)[C@H]2[C@H](OC(=O)c2ccccc2)[C@]2(N)C[C@H](OC(=O)[C@@H](N)[C@@H](NC(=O)c3ccccc3)c3ccccc3)C(C)=C1C2(C)C\n",
            "Predicted Lipophilicity_AstraZeneca: 1.9332146997090818\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1512.01it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 135.29it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.49it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.09it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.33it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.63it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.49it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.26it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Swapping\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](NC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](OC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.5826164106661977\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22369.62it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 151.43it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.55it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.39it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.91it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.18it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.45it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.83it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Bond Order Modification\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.5915405640321625\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1826.39it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 137.15it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.04it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.83it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.03it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.12it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.61it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.75it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.89it/s]\n",
            "[22:11:27] Explicit valence for atom # 15 C, 5, is greater than permitted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Deletion\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.899766333139005\n",
            "\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 24314.81it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 150.48it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.01it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.53it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.66it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.12it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.33it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.73it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 14847.09it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 149.78it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.63it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.49it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.08it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.80it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.03it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.83it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.22it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 25653.24it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 140.76it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.70it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 23.84it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.64it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.57it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.76it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.56it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 3695.42it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 113.17it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  7.11it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.33it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.14it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.43it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.96it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.82it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.46it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20410.24it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 128.96it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.26it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.69it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.44it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.45it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  40%|████      | 2/5 [00:00<00:00,  9.17it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00,  7.81it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00,  8.31it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 14665.40it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 103.64it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.55it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.07it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.51it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.26it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.98it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.89it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.18it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20712.61it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 142.76it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.29it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.29it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.25it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.44it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 25.98it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.78it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.26it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 2093.49it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 186.60it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  8.87it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.39it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.86it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.53it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 21.31it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 20.48it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 2663.90it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 183.95it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.38it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.97it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.89it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.97it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.78it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.41it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.16it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 25970.92it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 140.05it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.78it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.46it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.48it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.68it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.12it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.88it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  \t10    \t2.55294\t2.89977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22610.80it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 148.33it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.23it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.89it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.21it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.95it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.27it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.21it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Bond Order Modification\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.899766333139005\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22857.24it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 107.97it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  8.56it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.19it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.71it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.82it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.56it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.06it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.65it/s]\n",
            "[22:12:06] Explicit valence for atom # 11 O, 3, is greater than permitted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Swapping\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.899766333139005\n",
            "\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 896.03it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 145.98it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.25it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.37it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.72it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.09it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.39it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.51it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Deletion\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](CO)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1\n",
            "Predicted Lipophilicity_AstraZeneca: 2.6587998165174236\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1937.77it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 146.19it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.31it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 29.14it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.95it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.59it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 21.73it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 21.37it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.71it/s]\n",
            "[22:12:13] Explicit valence for atom # 15 C, 5, is greater than permitted\n",
            "[22:12:13] Can't kekulize mol.  Unkekulized atoms: 52 53 54 56 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Deletion\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.0489380170496325\n",
            "\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n",
            "Mutated SMILES from 'Atom Swapping' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 847.93it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 113.88it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.95it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.02it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 22.78it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.40it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.79it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.15it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Deletion\n",
            "Mutated SMILES: C.CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@](C)(C(=O)[C@H](O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C)[C@@H]2COC(=O)c1ccccc1.O.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.7037459444592296\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 18935.91it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 127.08it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.93it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.60it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.66it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.08it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 22.67it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 22.00it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.57it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21845.33it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 130.78it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.85it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.34it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.34it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.23it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.67it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.54it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.30it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 3650.40it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 153.13it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.15it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.06it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.77it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.74it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.27it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.55it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.65it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 25970.92it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 157.91it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.27it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 34.68it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 33.82it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.47it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.27it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.33it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.05it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 25266.89it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 176.81it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.67it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.62it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.58it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.02it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.66it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.43it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.66it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21732.15it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 143.94it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 13.42it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.93it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.40it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.24it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.42it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.30it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.17it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 23967.45it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 146.87it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.98it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.85it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.32it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.56it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.20it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.41it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.53it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 17924.38it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 111.24it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.70it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.74it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.87it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.73it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.22it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.66it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n",
            "[22:12:41] Can't kekulize mol.  Unkekulized atoms: 33 34 35 36 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2  \t8     \t2.71687\t3.04894\n",
            "Mutated SMILES from 'Atom Swapping' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20510.04it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 109.91it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.54it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.16it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.35it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.02it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.08it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.18it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.16it/s]\n",
            "[22:12:44] Explicit valence for atom # 30 C, 5, is greater than permitted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Swapping\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.899766333139005\n",
            "\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1550.00it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 128.79it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.41it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.83it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.11it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.41it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.82it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.79it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Stereochemistry Alteration\n",
            "Mutated SMILES: CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.6116831709050974\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1261.07it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 133.61it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.84it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.59it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.98it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.37it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.67it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.86it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Insertion\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.1722188191682354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 769.53it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 124.61it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.95it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.60it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.13it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.45it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.21it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.56it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.25it/s]\n",
            "[22:12:54] non-ring atom 1 marked aromatic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Substructure Replacement\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COC(=O)c1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.28245440663882\n",
            "\n",
            "Mutated SMILES from 'Ring Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22369.62it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 125.89it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.10it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.14it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.58it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.25it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.73it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.21it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 25420.02it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 159.54it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.24it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.57it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.83it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.17it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.31it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.26it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.41it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 27235.74it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 182.44it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.96it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.12it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.59it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.44it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.44it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 19737.90it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 132.57it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.56it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.62it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.14it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.58it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.78it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.53it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.40it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 17050.02it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 117.63it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.68it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 25.75it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.15it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.99it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.94it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.28it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 18001.30it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 127.38it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.95it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 29.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.14it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.61it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.48it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.91it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.59it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 18157.16it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 134.56it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.88it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.80it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.24it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.20it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.90it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.24it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3  \t7     \t2.75277\t3.17222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21732.15it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 156.59it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.56it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.48it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.74it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.70it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.82it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.18it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.59it/s]\n",
            "[22:13:21] non-ring atom 1 marked aromatic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Swapping\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.1722188191682354\n",
            "\n",
            "Mutated SMILES from 'Atom Deletion' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1642.57it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 139.12it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.81it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 25.78it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.28it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.75it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 23.15it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 23.98it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Substructure Replacement\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.4490575919455644\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20360.70it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 106.85it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.40it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.25it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.94it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.91it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.77it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.73it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Bond Order Modification\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.899766333139005\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1306.03it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 171.55it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.57it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.69it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.58it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.87it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.87it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.01it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n",
            "[22:13:31] non-ring atom 8 marked aromatic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Stereochemistry Alteration\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.0501066888038673\n",
            "\n",
            "Mutated SMILES from 'Ring Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1844.06it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 155.34it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.27it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.10it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.74it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.73it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.03it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.22it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Atom Deletion\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.337291464296257\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21454.24it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 146.28it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.64it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.91it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.74it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.90it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 33.75it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.82it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.01it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 19691.57it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 139.97it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.55it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 22.32it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 20.77it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  3.94it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.91it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.89it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.44it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 17331.83it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 148.07it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.30it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.30it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.91it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.89it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.72it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.45it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.40it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 23831.27it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 140.34it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.37it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.65it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.99it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.74it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.89it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.33it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.65it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 17772.47it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 155.23it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 12.20it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.79it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.36it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.80it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.58it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.33it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.39it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 15196.75it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 143.07it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 20.86it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 20.24it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  3.83it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 21.95it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 21.25it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 24105.20it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 144.12it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.50it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.51it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.96it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 28.50it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.11it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.92it/s]\n",
            "[22:13:59] Explicit valence for atom # 25 O, 3, is greater than permitted\n",
            "[22:13:59] non-ring atom 1 marked aromatic\n",
            "[22:13:59] non-ring atom 1 marked aromatic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4  \t7     \t2.90277\t3.33729\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n",
            "Mutated SMILES from 'Ring Modification' is chemically invalid. Returning original SMILES.\n",
            "Mutated SMILES from 'Ring Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1473.50it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 148.02it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.77it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.07it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.46it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.63it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.34it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.23it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Substructure Replacement\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 2.4545020967229823\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1741.46it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 99.24it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.28it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.02it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.41it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.77it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.21it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.77it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.74it/s]\n",
            "[22:14:05] non-ring atom 1 marked aromatic\n",
            "[22:14:05] Explicit valence for atom # 30 C, 5, is greater than permitted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Substructure Replacement\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1CN)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1\n",
            "Predicted Lipophilicity_AstraZeneca: 1.765294380206284\n",
            "\n",
            "Mutated SMILES from 'Ring Modification' is chemically invalid. Returning original SMILES.\n",
            "Mutated SMILES from 'Bond Order Modification' is chemically invalid. Returning original SMILES.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 1673.70it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 149.19it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.11it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.45it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.47it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.66it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.38it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.72it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutation Operator: Stereochemistry Alteration\n",
            "Mutated SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](O)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O\n",
            "Predicted Lipophilicity_AstraZeneca: 3.0811005454724656\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21564.54it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 142.78it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.54it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.05it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.71it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.24it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.62it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 25.91it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.05it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 21290.88it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 175.94it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.44it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.24it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.31it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.62it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.37it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.73it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.43it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 2857.16it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 187.45it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 27.54it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.03it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.41it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 31.57it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.19it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 18724.57it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 139.30it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 11.07it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.33it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 31.56it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.09it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.76it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.78it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 20712.61it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 148.40it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.96it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 30.18it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.68it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.03it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 26.53it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 17439.93it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 126.97it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  9.11it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 26.52it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 27.80it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.33it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 29.99it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.19it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.41it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 2729.78it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 167.07it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00,  7.65it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 23.21it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 22.50it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  4.34it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 24.32it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 24.79it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  4.46it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 2/2 [00:00<00:00, 22017.34it/s]\n",
            "Computing physchem properties: 100%|██████████| 2/2 [00:00<00:00, 137.35it/s]\n",
            "RDKit fingerprints: 100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 30.60it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 29.64it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  5.66it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00, 28.93it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 28.77it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5  \t8     \t2.91831\t3.33729\n",
            "Best molecule: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n",
            "Optimized SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n",
            "\n",
            "All chemically feasible mutations generated:\n",
            "         Mutation Operator                                                                                                                                                     Mutated SMILES  Predicted Lipophilicity\n",
            "Stereochemistry Alteration    CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.565190\n",
            "   Bond Order Modification   CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.591541\n",
            "         Ring Modification CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@@](C)(OC(C)=O)[C@H](O)C[C@@H]2O                 2.590862\n",
            "             Atom Swapping   CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.591541\n",
            "  Substructure Replacement    CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H](N)C[C@H]3OC[C@@]3(OC(C)=O)[C@H]2[C@H](OC(=O)c2ccccc2)[C@]2(N)C[C@H](OC(=O)[C@@H](N)[C@@H](NC(=O)c3ccccc3)c3ccccc3)C(C)=C1C2(C)C                 1.933215\n",
            "             Atom Swapping   CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](NC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](OC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.582616\n",
            "   Bond Order Modification   CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.591541\n",
            "             Atom Deletion             CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.899766\n",
            "   Bond Order Modification             CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.899766\n",
            "             Atom Swapping             CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.899766\n",
            "             Atom Deletion               CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](CO)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1                 2.658800\n",
            "             Atom Deletion                 CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 3.048938\n",
            "             Atom Deletion               C.CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@](C)(C(=O)[C@H](O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C)[C@@H]2COC(=O)c1ccccc1.O.O                 2.703746\n",
            "             Atom Swapping             CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.899766\n",
            "Stereochemistry Alteration  CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O                 2.611683\n",
            "            Atom Insertion              CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O                 3.172219\n",
            "  Substructure Replacement              CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COC(=O)c1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.282454\n",
            "             Atom Swapping              CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O                 3.172219\n",
            "  Substructure Replacement                  CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.449058\n",
            "   Bond Order Modification             CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.899766\n",
            "Stereochemistry Alteration                CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 3.050107\n",
            "             Atom Deletion                     CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O                 3.337291\n",
            "  Substructure Replacement                 CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 2.454502\n",
            "  Substructure Replacement                CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1CN)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1                 1.765294\n",
            "Stereochemistry Alteration                   CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](O)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O                 3.081101\n",
            "\n",
            "Results saved as 'mutation_results.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 3D Visualization of Initial and Optimized Molecules**"
      ],
      "metadata": {
        "id": "cK1B9kqJ7gJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section generates 3D molecular structures for both the initial ligand (before optimization) and the optimized ligand (after applying the genetic algorithm). The visualization helps compare structural changes and assess molecular modifications introduced during the optimization process.\n",
        "\n",
        "Using py3Dmol, the script:\n",
        "\n",
        "Converts the SMILES representation of the molecule into a 3D structure.\n",
        "\n",
        "Generates a conformer using the ETKDG method and optimizes the geometry with the UFF force field.\n",
        "\n",
        "Displays interactive 3D molecular views side by side, allowing users to visually analyze differences between the original and optimized ligands.\n",
        "\n",
        "Both structures are displayed with stick representation and a light grey background for clarity."
      ],
      "metadata": {
        "id": "Xv9Y3-fW3A67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import py3Dmol\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "def generate_view_with_smiles(smiles, title=\"Molecule\"):\n",
        "    \"\"\"\n",
        "    Generates a 3D conformer for a given SMILES string and returns an HTML string that\n",
        "    contains the title, the SMILES, and the 3D viewer.\n",
        "    \"\"\"\n",
        "    # Convert SMILES to molecule and add hydrogens\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    mol = Chem.AddHs(mol)\n",
        "    # Embed molecule in 3D space using ETKDG method and optimize geometry with UFF\n",
        "    AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "    AllChem.UFFOptimizeMolecule(mol)\n",
        "    # Convert molecule to MolBlock (SDF format)\n",
        "    mb = Chem.MolToMolBlock(mol)\n",
        "\n",
        "    # Create the py3Dmol view\n",
        "    view = py3Dmol.view(width=400, height=400)\n",
        "    view.addModel(mb, 'sdf')\n",
        "    view.setStyle({'stick':{}})\n",
        "    view.setBackgroundColor('0xeeeeee')\n",
        "    view.zoomTo()\n",
        "\n",
        "    # Get the HTML representation of the 3D viewer\n",
        "    html_view = view._make_html()\n",
        "\n",
        "    # Create header with title and SMILES (centered)\n",
        "    header = f\"\"\"\n",
        "    <div style=\"text-align:center;\">\n",
        "        <h3>{title}</h3>\n",
        "        <p>SMILES: {smiles}</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine header and viewer into one HTML block\n",
        "    combined_html = f\"<div>{header}{html_view}</div>\"\n",
        "    return combined_html\n",
        "\n",
        "# Generate HTML for both the initial and optimized molecules\n",
        "initial_html = generate_view_with_smiles(initial_smiles, \"Initial Molecule\")\n",
        "optimized_html = generate_view_with_smiles(best_smiles, \"Optimized Molecule\")\n",
        "\n",
        "# Display the two views side by side using ipywidgets HBox\n",
        "views_box = widgets.HBox([widgets.HTML(initial_html), widgets.HTML(optimized_html)])\n",
        "display(views_box)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588,
          "referenced_widgets": [
            "3d461ffaa9bf4636a590f9521827a10f",
            "bf6e4455f8f34b62bb8868e806aad73d",
            "d0e8224961354c088e37ac15fbd7272b",
            "3f03b66ffe704ec1aeed52c2d69ba823",
            "e24785c055464f5987eeb42d3b76d05a",
            "e182e18bfd324945920e0e1e4392d16e",
            "cfe4c1af3d884dc2b2d7bfe49c4f0f8e",
            "a0012579288c46d2bfbaf9d72d874d95"
          ]
        },
        "id": "4510om9p9gE1",
        "outputId": "65d13c1a-10cc-41dd-8590-4fc0a8925668"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value='<div>\\n    <div style=\"text-align:center;\">\\n        <h3>Initial Molecule</h3>\\n   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d461ffaa9bf4636a590f9521827a10f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Save Optimized Molecule as Image and SDF File**\n"
      ],
      "metadata": {
        "id": "hi9Xi7bC7syC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section generates and saves the optimized molecule in two formats:\n",
        "\n",
        "PNG Image – A 2D molecular representation is created using RDKit's MolToImage function, allowing users to visualize the optimized structure.\n",
        "\n",
        "SDF File – The optimized molecule is converted to 3D using ETKDG embedding and UFF optimization, then saved as an SDF (Structure Data File) for further analysis in molecular modeling tools.\n",
        "\n",
        "\n",
        "The script ensures successful file creation by checking if both the image and SDF file exist in the directory."
      ],
      "metadata": {
        "id": "UGopvh8z3r7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.rdmolfiles import SDWriter\n",
        "import os\n",
        "\n",
        "# Convert SMILES to RDKit Molecule\n",
        "optimized_mol = Chem.MolFromSmiles(best_smiles)\n",
        "\n",
        "# Ensure 3D coordinates exist\n",
        "optimized_mol = Chem.AddHs(optimized_mol)\n",
        "AllChem.EmbedMolecule(optimized_mol, AllChem.ETKDG())\n",
        "AllChem.UFFOptimizeMolecule(optimized_mol)\n",
        "\n",
        "# ------------------- Save as PNG Image -------------------\n",
        "image_path = \"optimized_molecule.png\"\n",
        "img = Draw.MolToImage(optimized_mol, size=(500, 500))\n",
        "img.save(image_path)\n",
        "print(f\"Optimized molecule image saved as '{image_path}'.\")\n",
        "\n",
        "# ------------------- Save as SDF File -------------------\n",
        "sdf_filename = \"optimized_molecule.sdf\"\n",
        "writer = SDWriter(sdf_filename)\n",
        "writer.write(optimized_mol)\n",
        "writer.close()\n",
        "print(f\"Optimized molecule SDF file saved as '{sdf_filename}'.\")\n",
        "\n",
        "# Verify if the files are saved successfully\n",
        "if os.path.exists(image_path) and os.path.exists(sdf_filename):\n",
        "    print(\"Files saved successfully!\")\n",
        "else:\n",
        "    print(\"Error: Files not saved correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIz0fsE_Egp",
        "outputId": "1f0c02fb-9d01-4446-9d7a-ff279e725e5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized molecule image saved as 'optimized_molecule.png'.\n",
            "Optimized molecule SDF file saved as 'optimized_molecule.sdf'.\n",
            "Files saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Predict ADMET Properties for Generated Molecules**"
      ],
      "metadata": {
        "id": "CjadWkRL74AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section loads the ADMET prediction model from admet_ai and evaluates the Absorption, Distribution, Metabolism, Excretion, and Toxicity (ADMET) properties of the mutated molecules generated by the Genetic Algorithm (GA).\n",
        "\n",
        "Key Steps:\n",
        "\n",
        "Load Mutated Molecules – Reads the mutation_results.csv file containing the SMILES representations of the generated molecules.\n",
        "\n",
        "Predict ADMET Properties – Uses the ADMETModel to predict key pharmacokinetic and toxicity properties.\n",
        "\n",
        "Merge & Display Data – Combines SMILES data with ADMET predictions for clear visualization.\n",
        "\n",
        "Save Predictions – Exports the results to a new CSV file (generated_molecules_admet.csv) for further analysis.\n",
        "\n",
        "This ensures a streamlined drug discovery workflow, allowing for the selection of promising drug-like candidates based on ADMET properties."
      ],
      "metadata": {
        "id": "zJtg68_b4KbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from admet_ai import ADMETModel\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the ADMET prediction model\n",
        "model = ADMETModel()\n",
        "\n",
        "# CSV file generated by GA\n",
        "csv_filename = \"mutation_results.csv\"\n",
        "\n",
        "try:\n",
        "    # Load the CSV file\n",
        "    smiles_data = pd.read_csv(csv_filename)\n",
        "\n",
        "    # Check if \"Mutated SMILES\" column exists\n",
        "    if \"Mutated SMILES\" not in smiles_data.columns:\n",
        "        print(\"Error: The CSV file does not contain a 'Mutated SMILES' column.\")\n",
        "    else:\n",
        "        generated_smiles_list = smiles_data[\"Mutated SMILES\"].dropna().tolist()\n",
        "\n",
        "        # Ensure at least two SMILES for robustness in prediction\n",
        "        if len(generated_smiles_list) == 1:\n",
        "            generated_smiles_list = generated_smiles_list * 2\n",
        "\n",
        "        # Predict ADMET properties\n",
        "        admet_predictions = model.predict(smiles=generated_smiles_list)\n",
        "\n",
        "        # Debug: Check the column names in admet_predictions\n",
        "        print(\"ADMET Predictions Columns:\", admet_predictions.columns)\n",
        "\n",
        "        # Check if the prediction was successful\n",
        "        if isinstance(admet_predictions, pd.DataFrame) and not admet_predictions.empty:\n",
        "            print(\"\\n### ADMET Predictions Table ###\")\n",
        "\n",
        "            # Ensure the ADMET output has a SMILES column, or manually add it\n",
        "            if \"SMILES\" not in admet_predictions.columns:\n",
        "                admet_predictions.insert(0, \"SMILES\", generated_smiles_list)\n",
        "\n",
        "            # Merge ADMET predictions with original SMILES data for better readability\n",
        "            result_df = smiles_data.merge(admet_predictions, left_on=\"Mutated SMILES\", right_on=\"SMILES\", how=\"inner\")\n",
        "\n",
        "            # Display table in Colab\n",
        "            display(result_df.style.set_properties(**{\"text-align\": \"center\"}).set_table_styles(\n",
        "                [dict(selector=\"th\", props=[(\"text-align\", \"center\")])])\n",
        "            )\n",
        "\n",
        "            # Save predictions to a new CSV file\n",
        "            result_df.to_csv(\"generated_molecules_admet.csv\", index=False)\n",
        "            print(\"ADMET predictions saved as 'generated_molecules_admet.csv'.\")\n",
        "        else:\n",
        "            print(\"Error: ADMET prediction failed or returned an empty dataset.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file '{csv_filename}' not found. Make sure the GA execution has completed and generated this file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ysscD7fXALMX",
        "outputId": "a825e615-3fe9-4df3-f6c3-880c57ee906f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 25/25 [00:00<00:00, 227456.83it/s]\n",
            "Computing physchem properties: 100%|██████████| 25/25 [00:00<00:00, 291.34it/s]\n",
            "RDKit fingerprints: 100%|██████████| 25/25 [00:01<00:00, 13.14it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  20%|██        | 1/5 [00:00<00:00,  5.48it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  40%|████      | 2/5 [00:00<00:00,  5.54it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00,  6.10it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00,  6.13it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00,  5.79it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  20%|██        | 1/5 [00:00<00:00,  6.68it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  40%|████      | 2/5 [00:00<00:00,  6.60it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  60%|██████    | 3/5 [00:00<00:00,  5.11it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00,  4.45it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                             \u001b[A\u001b[A\n",
            "individual models: 100%|██████████| 5/5 [00:01<00:00,  4.58it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADMET Predictions Columns: Index(['molecular_weight', 'logP', 'hydrogen_bond_acceptors',\n",
            "       'hydrogen_bond_donors', 'Lipinski', 'QED', 'stereo_centers', 'tpsa',\n",
            "       'AMES', 'BBB_Martins', 'Bioavailability_Ma', 'CYP1A2_Veith',\n",
            "       'CYP2C19_Veith', 'CYP2C9_Substrate_CarbonMangels', 'CYP2C9_Veith',\n",
            "       'CYP2D6_Substrate_CarbonMangels', 'CYP2D6_Veith',\n",
            "       'CYP3A4_Substrate_CarbonMangels', 'CYP3A4_Veith', 'Carcinogens_Lagunin',\n",
            "       'ClinTox', 'DILI', 'HIA_Hou', 'NR-AR-LBD', 'NR-AR', 'NR-AhR',\n",
            "       'NR-Aromatase', 'NR-ER-LBD', 'NR-ER', 'NR-PPAR-gamma', 'PAMPA_NCATS',\n",
            "       'Pgp_Broccatelli', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53',\n",
            "       'Skin_Reaction', 'hERG', 'Caco2_Wang', 'Clearance_Hepatocyte_AZ',\n",
            "       'Clearance_Microsome_AZ', 'Half_Life_Obach',\n",
            "       'HydrationFreeEnergy_FreeSolv', 'LD50_Zhu', 'Lipophilicity_AstraZeneca',\n",
            "       'PPBR_AZ', 'Solubility_AqSolDB', 'VDss_Lombardo',\n",
            "       'molecular_weight_drugbank_approved_percentile',\n",
            "       'logP_drugbank_approved_percentile',\n",
            "       'hydrogen_bond_acceptors_drugbank_approved_percentile',\n",
            "       'hydrogen_bond_donors_drugbank_approved_percentile',\n",
            "       'Lipinski_drugbank_approved_percentile',\n",
            "       'QED_drugbank_approved_percentile',\n",
            "       'stereo_centers_drugbank_approved_percentile',\n",
            "       'tpsa_drugbank_approved_percentile',\n",
            "       'AMES_drugbank_approved_percentile',\n",
            "       'BBB_Martins_drugbank_approved_percentile',\n",
            "       'Bioavailability_Ma_drugbank_approved_percentile',\n",
            "       'CYP1A2_Veith_drugbank_approved_percentile',\n",
            "       'CYP2C19_Veith_drugbank_approved_percentile',\n",
            "       'CYP2C9_Substrate_CarbonMangels_drugbank_approved_percentile',\n",
            "       'CYP2C9_Veith_drugbank_approved_percentile',\n",
            "       'CYP2D6_Substrate_CarbonMangels_drugbank_approved_percentile',\n",
            "       'CYP2D6_Veith_drugbank_approved_percentile',\n",
            "       'CYP3A4_Substrate_CarbonMangels_drugbank_approved_percentile',\n",
            "       'CYP3A4_Veith_drugbank_approved_percentile',\n",
            "       'Carcinogens_Lagunin_drugbank_approved_percentile',\n",
            "       'ClinTox_drugbank_approved_percentile',\n",
            "       'DILI_drugbank_approved_percentile',\n",
            "       'HIA_Hou_drugbank_approved_percentile',\n",
            "       'NR-AR-LBD_drugbank_approved_percentile',\n",
            "       'NR-AR_drugbank_approved_percentile',\n",
            "       'NR-AhR_drugbank_approved_percentile',\n",
            "       'NR-Aromatase_drugbank_approved_percentile',\n",
            "       'NR-ER-LBD_drugbank_approved_percentile',\n",
            "       'NR-ER_drugbank_approved_percentile',\n",
            "       'NR-PPAR-gamma_drugbank_approved_percentile',\n",
            "       'PAMPA_NCATS_drugbank_approved_percentile',\n",
            "       'Pgp_Broccatelli_drugbank_approved_percentile',\n",
            "       'SR-ARE_drugbank_approved_percentile',\n",
            "       'SR-ATAD5_drugbank_approved_percentile',\n",
            "       'SR-HSE_drugbank_approved_percentile',\n",
            "       'SR-MMP_drugbank_approved_percentile',\n",
            "       'SR-p53_drugbank_approved_percentile',\n",
            "       'Skin_Reaction_drugbank_approved_percentile',\n",
            "       'hERG_drugbank_approved_percentile',\n",
            "       'Caco2_Wang_drugbank_approved_percentile',\n",
            "       'Clearance_Hepatocyte_AZ_drugbank_approved_percentile',\n",
            "       'Clearance_Microsome_AZ_drugbank_approved_percentile',\n",
            "       'Half_Life_Obach_drugbank_approved_percentile',\n",
            "       'HydrationFreeEnergy_FreeSolv_drugbank_approved_percentile',\n",
            "       'LD50_Zhu_drugbank_approved_percentile',\n",
            "       'Lipophilicity_AstraZeneca_drugbank_approved_percentile',\n",
            "       'PPBR_AZ_drugbank_approved_percentile',\n",
            "       'Solubility_AqSolDB_drugbank_approved_percentile',\n",
            "       'VDss_Lombardo_drugbank_approved_percentile'],\n",
            "      dtype='object')\n",
            "\n",
            "### ADMET Predictions Table ###\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7b0508c8d590>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4b03d th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_4b03d_row0_col0, #T_4b03d_row0_col1, #T_4b03d_row0_col2, #T_4b03d_row0_col3, #T_4b03d_row0_col4, #T_4b03d_row0_col5, #T_4b03d_row0_col6, #T_4b03d_row0_col7, #T_4b03d_row0_col8, #T_4b03d_row0_col9, #T_4b03d_row0_col10, #T_4b03d_row0_col11, #T_4b03d_row0_col12, #T_4b03d_row0_col13, #T_4b03d_row0_col14, #T_4b03d_row0_col15, #T_4b03d_row0_col16, #T_4b03d_row0_col17, #T_4b03d_row0_col18, #T_4b03d_row0_col19, #T_4b03d_row0_col20, #T_4b03d_row0_col21, #T_4b03d_row0_col22, #T_4b03d_row0_col23, #T_4b03d_row0_col24, #T_4b03d_row0_col25, #T_4b03d_row0_col26, #T_4b03d_row0_col27, #T_4b03d_row0_col28, #T_4b03d_row0_col29, #T_4b03d_row0_col30, #T_4b03d_row0_col31, #T_4b03d_row0_col32, #T_4b03d_row0_col33, #T_4b03d_row0_col34, #T_4b03d_row0_col35, #T_4b03d_row0_col36, #T_4b03d_row0_col37, #T_4b03d_row0_col38, #T_4b03d_row0_col39, #T_4b03d_row0_col40, #T_4b03d_row0_col41, #T_4b03d_row0_col42, #T_4b03d_row0_col43, #T_4b03d_row0_col44, #T_4b03d_row0_col45, #T_4b03d_row0_col46, #T_4b03d_row0_col47, #T_4b03d_row0_col48, #T_4b03d_row0_col49, #T_4b03d_row0_col50, #T_4b03d_row0_col51, #T_4b03d_row0_col52, #T_4b03d_row0_col53, #T_4b03d_row0_col54, #T_4b03d_row0_col55, #T_4b03d_row0_col56, #T_4b03d_row0_col57, #T_4b03d_row0_col58, #T_4b03d_row0_col59, #T_4b03d_row0_col60, #T_4b03d_row0_col61, #T_4b03d_row0_col62, #T_4b03d_row0_col63, #T_4b03d_row0_col64, #T_4b03d_row0_col65, #T_4b03d_row0_col66, #T_4b03d_row0_col67, #T_4b03d_row0_col68, #T_4b03d_row0_col69, #T_4b03d_row0_col70, #T_4b03d_row0_col71, #T_4b03d_row0_col72, #T_4b03d_row0_col73, #T_4b03d_row0_col74, #T_4b03d_row0_col75, #T_4b03d_row0_col76, #T_4b03d_row0_col77, #T_4b03d_row0_col78, #T_4b03d_row0_col79, #T_4b03d_row0_col80, #T_4b03d_row0_col81, #T_4b03d_row0_col82, #T_4b03d_row0_col83, #T_4b03d_row0_col84, #T_4b03d_row0_col85, #T_4b03d_row0_col86, #T_4b03d_row0_col87, #T_4b03d_row0_col88, #T_4b03d_row0_col89, #T_4b03d_row0_col90, #T_4b03d_row0_col91, #T_4b03d_row0_col92, #T_4b03d_row0_col93, #T_4b03d_row0_col94, #T_4b03d_row0_col95, #T_4b03d_row0_col96, #T_4b03d_row0_col97, #T_4b03d_row0_col98, #T_4b03d_row0_col99, #T_4b03d_row0_col100, #T_4b03d_row0_col101, #T_4b03d_row1_col0, #T_4b03d_row1_col1, #T_4b03d_row1_col2, #T_4b03d_row1_col3, #T_4b03d_row1_col4, #T_4b03d_row1_col5, #T_4b03d_row1_col6, #T_4b03d_row1_col7, #T_4b03d_row1_col8, #T_4b03d_row1_col9, #T_4b03d_row1_col10, #T_4b03d_row1_col11, #T_4b03d_row1_col12, #T_4b03d_row1_col13, #T_4b03d_row1_col14, #T_4b03d_row1_col15, #T_4b03d_row1_col16, #T_4b03d_row1_col17, #T_4b03d_row1_col18, #T_4b03d_row1_col19, #T_4b03d_row1_col20, #T_4b03d_row1_col21, #T_4b03d_row1_col22, #T_4b03d_row1_col23, #T_4b03d_row1_col24, #T_4b03d_row1_col25, #T_4b03d_row1_col26, #T_4b03d_row1_col27, #T_4b03d_row1_col28, #T_4b03d_row1_col29, #T_4b03d_row1_col30, #T_4b03d_row1_col31, #T_4b03d_row1_col32, #T_4b03d_row1_col33, #T_4b03d_row1_col34, #T_4b03d_row1_col35, #T_4b03d_row1_col36, #T_4b03d_row1_col37, #T_4b03d_row1_col38, #T_4b03d_row1_col39, #T_4b03d_row1_col40, #T_4b03d_row1_col41, #T_4b03d_row1_col42, #T_4b03d_row1_col43, #T_4b03d_row1_col44, #T_4b03d_row1_col45, #T_4b03d_row1_col46, #T_4b03d_row1_col47, #T_4b03d_row1_col48, #T_4b03d_row1_col49, #T_4b03d_row1_col50, #T_4b03d_row1_col51, #T_4b03d_row1_col52, #T_4b03d_row1_col53, #T_4b03d_row1_col54, #T_4b03d_row1_col55, #T_4b03d_row1_col56, #T_4b03d_row1_col57, #T_4b03d_row1_col58, #T_4b03d_row1_col59, #T_4b03d_row1_col60, #T_4b03d_row1_col61, #T_4b03d_row1_col62, #T_4b03d_row1_col63, #T_4b03d_row1_col64, #T_4b03d_row1_col65, #T_4b03d_row1_col66, #T_4b03d_row1_col67, #T_4b03d_row1_col68, #T_4b03d_row1_col69, #T_4b03d_row1_col70, #T_4b03d_row1_col71, #T_4b03d_row1_col72, #T_4b03d_row1_col73, #T_4b03d_row1_col74, #T_4b03d_row1_col75, #T_4b03d_row1_col76, #T_4b03d_row1_col77, #T_4b03d_row1_col78, #T_4b03d_row1_col79, #T_4b03d_row1_col80, #T_4b03d_row1_col81, #T_4b03d_row1_col82, #T_4b03d_row1_col83, #T_4b03d_row1_col84, #T_4b03d_row1_col85, #T_4b03d_row1_col86, #T_4b03d_row1_col87, #T_4b03d_row1_col88, #T_4b03d_row1_col89, #T_4b03d_row1_col90, #T_4b03d_row1_col91, #T_4b03d_row1_col92, #T_4b03d_row1_col93, #T_4b03d_row1_col94, #T_4b03d_row1_col95, #T_4b03d_row1_col96, #T_4b03d_row1_col97, #T_4b03d_row1_col98, #T_4b03d_row1_col99, #T_4b03d_row1_col100, #T_4b03d_row1_col101, #T_4b03d_row2_col0, #T_4b03d_row2_col1, #T_4b03d_row2_col2, #T_4b03d_row2_col3, #T_4b03d_row2_col4, #T_4b03d_row2_col5, #T_4b03d_row2_col6, #T_4b03d_row2_col7, #T_4b03d_row2_col8, #T_4b03d_row2_col9, #T_4b03d_row2_col10, #T_4b03d_row2_col11, #T_4b03d_row2_col12, #T_4b03d_row2_col13, #T_4b03d_row2_col14, #T_4b03d_row2_col15, #T_4b03d_row2_col16, #T_4b03d_row2_col17, #T_4b03d_row2_col18, #T_4b03d_row2_col19, #T_4b03d_row2_col20, #T_4b03d_row2_col21, #T_4b03d_row2_col22, #T_4b03d_row2_col23, #T_4b03d_row2_col24, #T_4b03d_row2_col25, #T_4b03d_row2_col26, #T_4b03d_row2_col27, #T_4b03d_row2_col28, #T_4b03d_row2_col29, #T_4b03d_row2_col30, #T_4b03d_row2_col31, #T_4b03d_row2_col32, #T_4b03d_row2_col33, #T_4b03d_row2_col34, #T_4b03d_row2_col35, #T_4b03d_row2_col36, #T_4b03d_row2_col37, #T_4b03d_row2_col38, #T_4b03d_row2_col39, #T_4b03d_row2_col40, #T_4b03d_row2_col41, #T_4b03d_row2_col42, #T_4b03d_row2_col43, #T_4b03d_row2_col44, #T_4b03d_row2_col45, #T_4b03d_row2_col46, #T_4b03d_row2_col47, #T_4b03d_row2_col48, #T_4b03d_row2_col49, #T_4b03d_row2_col50, #T_4b03d_row2_col51, #T_4b03d_row2_col52, #T_4b03d_row2_col53, #T_4b03d_row2_col54, #T_4b03d_row2_col55, #T_4b03d_row2_col56, #T_4b03d_row2_col57, #T_4b03d_row2_col58, #T_4b03d_row2_col59, #T_4b03d_row2_col60, #T_4b03d_row2_col61, #T_4b03d_row2_col62, #T_4b03d_row2_col63, #T_4b03d_row2_col64, #T_4b03d_row2_col65, #T_4b03d_row2_col66, #T_4b03d_row2_col67, #T_4b03d_row2_col68, #T_4b03d_row2_col69, #T_4b03d_row2_col70, #T_4b03d_row2_col71, #T_4b03d_row2_col72, #T_4b03d_row2_col73, #T_4b03d_row2_col74, #T_4b03d_row2_col75, #T_4b03d_row2_col76, #T_4b03d_row2_col77, #T_4b03d_row2_col78, #T_4b03d_row2_col79, #T_4b03d_row2_col80, #T_4b03d_row2_col81, #T_4b03d_row2_col82, #T_4b03d_row2_col83, #T_4b03d_row2_col84, #T_4b03d_row2_col85, #T_4b03d_row2_col86, #T_4b03d_row2_col87, #T_4b03d_row2_col88, #T_4b03d_row2_col89, #T_4b03d_row2_col90, #T_4b03d_row2_col91, #T_4b03d_row2_col92, #T_4b03d_row2_col93, #T_4b03d_row2_col94, #T_4b03d_row2_col95, #T_4b03d_row2_col96, #T_4b03d_row2_col97, #T_4b03d_row2_col98, #T_4b03d_row2_col99, #T_4b03d_row2_col100, #T_4b03d_row2_col101, #T_4b03d_row3_col0, #T_4b03d_row3_col1, #T_4b03d_row3_col2, #T_4b03d_row3_col3, #T_4b03d_row3_col4, #T_4b03d_row3_col5, #T_4b03d_row3_col6, #T_4b03d_row3_col7, #T_4b03d_row3_col8, #T_4b03d_row3_col9, #T_4b03d_row3_col10, #T_4b03d_row3_col11, #T_4b03d_row3_col12, #T_4b03d_row3_col13, #T_4b03d_row3_col14, #T_4b03d_row3_col15, #T_4b03d_row3_col16, #T_4b03d_row3_col17, #T_4b03d_row3_col18, #T_4b03d_row3_col19, #T_4b03d_row3_col20, #T_4b03d_row3_col21, #T_4b03d_row3_col22, #T_4b03d_row3_col23, #T_4b03d_row3_col24, #T_4b03d_row3_col25, #T_4b03d_row3_col26, #T_4b03d_row3_col27, #T_4b03d_row3_col28, #T_4b03d_row3_col29, #T_4b03d_row3_col30, #T_4b03d_row3_col31, #T_4b03d_row3_col32, #T_4b03d_row3_col33, #T_4b03d_row3_col34, #T_4b03d_row3_col35, #T_4b03d_row3_col36, #T_4b03d_row3_col37, #T_4b03d_row3_col38, #T_4b03d_row3_col39, #T_4b03d_row3_col40, #T_4b03d_row3_col41, #T_4b03d_row3_col42, #T_4b03d_row3_col43, #T_4b03d_row3_col44, #T_4b03d_row3_col45, #T_4b03d_row3_col46, #T_4b03d_row3_col47, #T_4b03d_row3_col48, #T_4b03d_row3_col49, #T_4b03d_row3_col50, #T_4b03d_row3_col51, #T_4b03d_row3_col52, #T_4b03d_row3_col53, #T_4b03d_row3_col54, #T_4b03d_row3_col55, #T_4b03d_row3_col56, #T_4b03d_row3_col57, #T_4b03d_row3_col58, #T_4b03d_row3_col59, #T_4b03d_row3_col60, #T_4b03d_row3_col61, #T_4b03d_row3_col62, #T_4b03d_row3_col63, #T_4b03d_row3_col64, #T_4b03d_row3_col65, #T_4b03d_row3_col66, #T_4b03d_row3_col67, #T_4b03d_row3_col68, #T_4b03d_row3_col69, #T_4b03d_row3_col70, #T_4b03d_row3_col71, #T_4b03d_row3_col72, #T_4b03d_row3_col73, #T_4b03d_row3_col74, #T_4b03d_row3_col75, #T_4b03d_row3_col76, #T_4b03d_row3_col77, #T_4b03d_row3_col78, #T_4b03d_row3_col79, #T_4b03d_row3_col80, #T_4b03d_row3_col81, #T_4b03d_row3_col82, #T_4b03d_row3_col83, #T_4b03d_row3_col84, #T_4b03d_row3_col85, #T_4b03d_row3_col86, #T_4b03d_row3_col87, #T_4b03d_row3_col88, #T_4b03d_row3_col89, #T_4b03d_row3_col90, #T_4b03d_row3_col91, #T_4b03d_row3_col92, #T_4b03d_row3_col93, #T_4b03d_row3_col94, #T_4b03d_row3_col95, #T_4b03d_row3_col96, #T_4b03d_row3_col97, #T_4b03d_row3_col98, #T_4b03d_row3_col99, #T_4b03d_row3_col100, #T_4b03d_row3_col101, #T_4b03d_row4_col0, #T_4b03d_row4_col1, #T_4b03d_row4_col2, #T_4b03d_row4_col3, #T_4b03d_row4_col4, #T_4b03d_row4_col5, #T_4b03d_row4_col6, #T_4b03d_row4_col7, #T_4b03d_row4_col8, #T_4b03d_row4_col9, #T_4b03d_row4_col10, #T_4b03d_row4_col11, #T_4b03d_row4_col12, #T_4b03d_row4_col13, #T_4b03d_row4_col14, #T_4b03d_row4_col15, #T_4b03d_row4_col16, #T_4b03d_row4_col17, #T_4b03d_row4_col18, #T_4b03d_row4_col19, #T_4b03d_row4_col20, #T_4b03d_row4_col21, #T_4b03d_row4_col22, #T_4b03d_row4_col23, #T_4b03d_row4_col24, #T_4b03d_row4_col25, #T_4b03d_row4_col26, #T_4b03d_row4_col27, #T_4b03d_row4_col28, #T_4b03d_row4_col29, #T_4b03d_row4_col30, #T_4b03d_row4_col31, #T_4b03d_row4_col32, #T_4b03d_row4_col33, #T_4b03d_row4_col34, #T_4b03d_row4_col35, #T_4b03d_row4_col36, #T_4b03d_row4_col37, #T_4b03d_row4_col38, #T_4b03d_row4_col39, #T_4b03d_row4_col40, #T_4b03d_row4_col41, #T_4b03d_row4_col42, #T_4b03d_row4_col43, #T_4b03d_row4_col44, #T_4b03d_row4_col45, #T_4b03d_row4_col46, #T_4b03d_row4_col47, #T_4b03d_row4_col48, #T_4b03d_row4_col49, #T_4b03d_row4_col50, #T_4b03d_row4_col51, #T_4b03d_row4_col52, #T_4b03d_row4_col53, #T_4b03d_row4_col54, #T_4b03d_row4_col55, #T_4b03d_row4_col56, #T_4b03d_row4_col57, #T_4b03d_row4_col58, #T_4b03d_row4_col59, #T_4b03d_row4_col60, #T_4b03d_row4_col61, #T_4b03d_row4_col62, #T_4b03d_row4_col63, #T_4b03d_row4_col64, #T_4b03d_row4_col65, #T_4b03d_row4_col66, #T_4b03d_row4_col67, #T_4b03d_row4_col68, #T_4b03d_row4_col69, #T_4b03d_row4_col70, #T_4b03d_row4_col71, #T_4b03d_row4_col72, #T_4b03d_row4_col73, #T_4b03d_row4_col74, #T_4b03d_row4_col75, #T_4b03d_row4_col76, #T_4b03d_row4_col77, #T_4b03d_row4_col78, #T_4b03d_row4_col79, #T_4b03d_row4_col80, #T_4b03d_row4_col81, #T_4b03d_row4_col82, #T_4b03d_row4_col83, #T_4b03d_row4_col84, #T_4b03d_row4_col85, #T_4b03d_row4_col86, #T_4b03d_row4_col87, #T_4b03d_row4_col88, #T_4b03d_row4_col89, #T_4b03d_row4_col90, #T_4b03d_row4_col91, #T_4b03d_row4_col92, #T_4b03d_row4_col93, #T_4b03d_row4_col94, #T_4b03d_row4_col95, #T_4b03d_row4_col96, #T_4b03d_row4_col97, #T_4b03d_row4_col98, #T_4b03d_row4_col99, #T_4b03d_row4_col100, #T_4b03d_row4_col101, #T_4b03d_row5_col0, #T_4b03d_row5_col1, #T_4b03d_row5_col2, #T_4b03d_row5_col3, #T_4b03d_row5_col4, #T_4b03d_row5_col5, #T_4b03d_row5_col6, #T_4b03d_row5_col7, #T_4b03d_row5_col8, #T_4b03d_row5_col9, #T_4b03d_row5_col10, #T_4b03d_row5_col11, #T_4b03d_row5_col12, #T_4b03d_row5_col13, #T_4b03d_row5_col14, #T_4b03d_row5_col15, #T_4b03d_row5_col16, #T_4b03d_row5_col17, #T_4b03d_row5_col18, #T_4b03d_row5_col19, #T_4b03d_row5_col20, #T_4b03d_row5_col21, #T_4b03d_row5_col22, #T_4b03d_row5_col23, #T_4b03d_row5_col24, #T_4b03d_row5_col25, #T_4b03d_row5_col26, #T_4b03d_row5_col27, #T_4b03d_row5_col28, #T_4b03d_row5_col29, #T_4b03d_row5_col30, #T_4b03d_row5_col31, #T_4b03d_row5_col32, #T_4b03d_row5_col33, #T_4b03d_row5_col34, #T_4b03d_row5_col35, #T_4b03d_row5_col36, #T_4b03d_row5_col37, #T_4b03d_row5_col38, #T_4b03d_row5_col39, #T_4b03d_row5_col40, #T_4b03d_row5_col41, #T_4b03d_row5_col42, #T_4b03d_row5_col43, #T_4b03d_row5_col44, #T_4b03d_row5_col45, #T_4b03d_row5_col46, #T_4b03d_row5_col47, #T_4b03d_row5_col48, #T_4b03d_row5_col49, #T_4b03d_row5_col50, #T_4b03d_row5_col51, #T_4b03d_row5_col52, #T_4b03d_row5_col53, #T_4b03d_row5_col54, #T_4b03d_row5_col55, #T_4b03d_row5_col56, #T_4b03d_row5_col57, #T_4b03d_row5_col58, #T_4b03d_row5_col59, #T_4b03d_row5_col60, #T_4b03d_row5_col61, #T_4b03d_row5_col62, #T_4b03d_row5_col63, #T_4b03d_row5_col64, #T_4b03d_row5_col65, #T_4b03d_row5_col66, #T_4b03d_row5_col67, #T_4b03d_row5_col68, #T_4b03d_row5_col69, #T_4b03d_row5_col70, #T_4b03d_row5_col71, #T_4b03d_row5_col72, #T_4b03d_row5_col73, #T_4b03d_row5_col74, #T_4b03d_row5_col75, #T_4b03d_row5_col76, #T_4b03d_row5_col77, #T_4b03d_row5_col78, #T_4b03d_row5_col79, #T_4b03d_row5_col80, #T_4b03d_row5_col81, #T_4b03d_row5_col82, #T_4b03d_row5_col83, #T_4b03d_row5_col84, #T_4b03d_row5_col85, #T_4b03d_row5_col86, #T_4b03d_row5_col87, #T_4b03d_row5_col88, #T_4b03d_row5_col89, #T_4b03d_row5_col90, #T_4b03d_row5_col91, #T_4b03d_row5_col92, #T_4b03d_row5_col93, #T_4b03d_row5_col94, #T_4b03d_row5_col95, #T_4b03d_row5_col96, #T_4b03d_row5_col97, #T_4b03d_row5_col98, #T_4b03d_row5_col99, #T_4b03d_row5_col100, #T_4b03d_row5_col101, #T_4b03d_row6_col0, #T_4b03d_row6_col1, #T_4b03d_row6_col2, #T_4b03d_row6_col3, #T_4b03d_row6_col4, #T_4b03d_row6_col5, #T_4b03d_row6_col6, #T_4b03d_row6_col7, #T_4b03d_row6_col8, #T_4b03d_row6_col9, #T_4b03d_row6_col10, #T_4b03d_row6_col11, #T_4b03d_row6_col12, #T_4b03d_row6_col13, #T_4b03d_row6_col14, #T_4b03d_row6_col15, #T_4b03d_row6_col16, #T_4b03d_row6_col17, #T_4b03d_row6_col18, #T_4b03d_row6_col19, #T_4b03d_row6_col20, #T_4b03d_row6_col21, #T_4b03d_row6_col22, #T_4b03d_row6_col23, #T_4b03d_row6_col24, #T_4b03d_row6_col25, #T_4b03d_row6_col26, #T_4b03d_row6_col27, #T_4b03d_row6_col28, #T_4b03d_row6_col29, #T_4b03d_row6_col30, #T_4b03d_row6_col31, #T_4b03d_row6_col32, #T_4b03d_row6_col33, #T_4b03d_row6_col34, #T_4b03d_row6_col35, #T_4b03d_row6_col36, #T_4b03d_row6_col37, #T_4b03d_row6_col38, #T_4b03d_row6_col39, #T_4b03d_row6_col40, #T_4b03d_row6_col41, #T_4b03d_row6_col42, #T_4b03d_row6_col43, #T_4b03d_row6_col44, #T_4b03d_row6_col45, #T_4b03d_row6_col46, #T_4b03d_row6_col47, #T_4b03d_row6_col48, #T_4b03d_row6_col49, #T_4b03d_row6_col50, #T_4b03d_row6_col51, #T_4b03d_row6_col52, #T_4b03d_row6_col53, #T_4b03d_row6_col54, #T_4b03d_row6_col55, #T_4b03d_row6_col56, #T_4b03d_row6_col57, #T_4b03d_row6_col58, #T_4b03d_row6_col59, #T_4b03d_row6_col60, #T_4b03d_row6_col61, #T_4b03d_row6_col62, #T_4b03d_row6_col63, #T_4b03d_row6_col64, #T_4b03d_row6_col65, #T_4b03d_row6_col66, #T_4b03d_row6_col67, #T_4b03d_row6_col68, #T_4b03d_row6_col69, #T_4b03d_row6_col70, #T_4b03d_row6_col71, #T_4b03d_row6_col72, #T_4b03d_row6_col73, #T_4b03d_row6_col74, #T_4b03d_row6_col75, #T_4b03d_row6_col76, #T_4b03d_row6_col77, #T_4b03d_row6_col78, #T_4b03d_row6_col79, #T_4b03d_row6_col80, #T_4b03d_row6_col81, #T_4b03d_row6_col82, #T_4b03d_row6_col83, #T_4b03d_row6_col84, #T_4b03d_row6_col85, #T_4b03d_row6_col86, #T_4b03d_row6_col87, #T_4b03d_row6_col88, #T_4b03d_row6_col89, #T_4b03d_row6_col90, #T_4b03d_row6_col91, #T_4b03d_row6_col92, #T_4b03d_row6_col93, #T_4b03d_row6_col94, #T_4b03d_row6_col95, #T_4b03d_row6_col96, #T_4b03d_row6_col97, #T_4b03d_row6_col98, #T_4b03d_row6_col99, #T_4b03d_row6_col100, #T_4b03d_row6_col101, #T_4b03d_row7_col0, #T_4b03d_row7_col1, #T_4b03d_row7_col2, #T_4b03d_row7_col3, #T_4b03d_row7_col4, #T_4b03d_row7_col5, #T_4b03d_row7_col6, #T_4b03d_row7_col7, #T_4b03d_row7_col8, #T_4b03d_row7_col9, #T_4b03d_row7_col10, #T_4b03d_row7_col11, #T_4b03d_row7_col12, #T_4b03d_row7_col13, #T_4b03d_row7_col14, #T_4b03d_row7_col15, #T_4b03d_row7_col16, #T_4b03d_row7_col17, #T_4b03d_row7_col18, #T_4b03d_row7_col19, #T_4b03d_row7_col20, #T_4b03d_row7_col21, #T_4b03d_row7_col22, #T_4b03d_row7_col23, #T_4b03d_row7_col24, #T_4b03d_row7_col25, #T_4b03d_row7_col26, #T_4b03d_row7_col27, #T_4b03d_row7_col28, #T_4b03d_row7_col29, #T_4b03d_row7_col30, #T_4b03d_row7_col31, #T_4b03d_row7_col32, #T_4b03d_row7_col33, #T_4b03d_row7_col34, #T_4b03d_row7_col35, #T_4b03d_row7_col36, #T_4b03d_row7_col37, #T_4b03d_row7_col38, #T_4b03d_row7_col39, #T_4b03d_row7_col40, #T_4b03d_row7_col41, #T_4b03d_row7_col42, #T_4b03d_row7_col43, #T_4b03d_row7_col44, #T_4b03d_row7_col45, #T_4b03d_row7_col46, #T_4b03d_row7_col47, #T_4b03d_row7_col48, #T_4b03d_row7_col49, #T_4b03d_row7_col50, #T_4b03d_row7_col51, #T_4b03d_row7_col52, #T_4b03d_row7_col53, #T_4b03d_row7_col54, #T_4b03d_row7_col55, #T_4b03d_row7_col56, #T_4b03d_row7_col57, #T_4b03d_row7_col58, #T_4b03d_row7_col59, #T_4b03d_row7_col60, #T_4b03d_row7_col61, #T_4b03d_row7_col62, #T_4b03d_row7_col63, #T_4b03d_row7_col64, #T_4b03d_row7_col65, #T_4b03d_row7_col66, #T_4b03d_row7_col67, #T_4b03d_row7_col68, #T_4b03d_row7_col69, #T_4b03d_row7_col70, #T_4b03d_row7_col71, #T_4b03d_row7_col72, #T_4b03d_row7_col73, #T_4b03d_row7_col74, #T_4b03d_row7_col75, #T_4b03d_row7_col76, #T_4b03d_row7_col77, #T_4b03d_row7_col78, #T_4b03d_row7_col79, #T_4b03d_row7_col80, #T_4b03d_row7_col81, #T_4b03d_row7_col82, #T_4b03d_row7_col83, #T_4b03d_row7_col84, #T_4b03d_row7_col85, #T_4b03d_row7_col86, #T_4b03d_row7_col87, #T_4b03d_row7_col88, #T_4b03d_row7_col89, #T_4b03d_row7_col90, #T_4b03d_row7_col91, #T_4b03d_row7_col92, #T_4b03d_row7_col93, #T_4b03d_row7_col94, #T_4b03d_row7_col95, #T_4b03d_row7_col96, #T_4b03d_row7_col97, #T_4b03d_row7_col98, #T_4b03d_row7_col99, #T_4b03d_row7_col100, #T_4b03d_row7_col101, #T_4b03d_row8_col0, #T_4b03d_row8_col1, #T_4b03d_row8_col2, #T_4b03d_row8_col3, #T_4b03d_row8_col4, #T_4b03d_row8_col5, #T_4b03d_row8_col6, #T_4b03d_row8_col7, #T_4b03d_row8_col8, #T_4b03d_row8_col9, #T_4b03d_row8_col10, #T_4b03d_row8_col11, #T_4b03d_row8_col12, #T_4b03d_row8_col13, #T_4b03d_row8_col14, #T_4b03d_row8_col15, #T_4b03d_row8_col16, #T_4b03d_row8_col17, #T_4b03d_row8_col18, #T_4b03d_row8_col19, #T_4b03d_row8_col20, #T_4b03d_row8_col21, #T_4b03d_row8_col22, #T_4b03d_row8_col23, #T_4b03d_row8_col24, #T_4b03d_row8_col25, #T_4b03d_row8_col26, #T_4b03d_row8_col27, #T_4b03d_row8_col28, #T_4b03d_row8_col29, #T_4b03d_row8_col30, #T_4b03d_row8_col31, #T_4b03d_row8_col32, #T_4b03d_row8_col33, #T_4b03d_row8_col34, #T_4b03d_row8_col35, #T_4b03d_row8_col36, #T_4b03d_row8_col37, #T_4b03d_row8_col38, #T_4b03d_row8_col39, #T_4b03d_row8_col40, #T_4b03d_row8_col41, #T_4b03d_row8_col42, #T_4b03d_row8_col43, #T_4b03d_row8_col44, #T_4b03d_row8_col45, #T_4b03d_row8_col46, #T_4b03d_row8_col47, #T_4b03d_row8_col48, #T_4b03d_row8_col49, #T_4b03d_row8_col50, #T_4b03d_row8_col51, #T_4b03d_row8_col52, #T_4b03d_row8_col53, #T_4b03d_row8_col54, #T_4b03d_row8_col55, #T_4b03d_row8_col56, #T_4b03d_row8_col57, #T_4b03d_row8_col58, #T_4b03d_row8_col59, #T_4b03d_row8_col60, #T_4b03d_row8_col61, #T_4b03d_row8_col62, #T_4b03d_row8_col63, #T_4b03d_row8_col64, #T_4b03d_row8_col65, #T_4b03d_row8_col66, #T_4b03d_row8_col67, #T_4b03d_row8_col68, #T_4b03d_row8_col69, #T_4b03d_row8_col70, #T_4b03d_row8_col71, #T_4b03d_row8_col72, #T_4b03d_row8_col73, #T_4b03d_row8_col74, #T_4b03d_row8_col75, #T_4b03d_row8_col76, #T_4b03d_row8_col77, #T_4b03d_row8_col78, #T_4b03d_row8_col79, #T_4b03d_row8_col80, #T_4b03d_row8_col81, #T_4b03d_row8_col82, #T_4b03d_row8_col83, #T_4b03d_row8_col84, #T_4b03d_row8_col85, #T_4b03d_row8_col86, #T_4b03d_row8_col87, #T_4b03d_row8_col88, #T_4b03d_row8_col89, #T_4b03d_row8_col90, #T_4b03d_row8_col91, #T_4b03d_row8_col92, #T_4b03d_row8_col93, #T_4b03d_row8_col94, #T_4b03d_row8_col95, #T_4b03d_row8_col96, #T_4b03d_row8_col97, #T_4b03d_row8_col98, #T_4b03d_row8_col99, #T_4b03d_row8_col100, #T_4b03d_row8_col101, #T_4b03d_row9_col0, #T_4b03d_row9_col1, #T_4b03d_row9_col2, #T_4b03d_row9_col3, #T_4b03d_row9_col4, #T_4b03d_row9_col5, #T_4b03d_row9_col6, #T_4b03d_row9_col7, #T_4b03d_row9_col8, #T_4b03d_row9_col9, #T_4b03d_row9_col10, #T_4b03d_row9_col11, #T_4b03d_row9_col12, #T_4b03d_row9_col13, #T_4b03d_row9_col14, #T_4b03d_row9_col15, #T_4b03d_row9_col16, #T_4b03d_row9_col17, #T_4b03d_row9_col18, #T_4b03d_row9_col19, #T_4b03d_row9_col20, #T_4b03d_row9_col21, #T_4b03d_row9_col22, #T_4b03d_row9_col23, #T_4b03d_row9_col24, #T_4b03d_row9_col25, #T_4b03d_row9_col26, #T_4b03d_row9_col27, #T_4b03d_row9_col28, #T_4b03d_row9_col29, #T_4b03d_row9_col30, #T_4b03d_row9_col31, #T_4b03d_row9_col32, #T_4b03d_row9_col33, #T_4b03d_row9_col34, #T_4b03d_row9_col35, #T_4b03d_row9_col36, #T_4b03d_row9_col37, #T_4b03d_row9_col38, #T_4b03d_row9_col39, #T_4b03d_row9_col40, #T_4b03d_row9_col41, #T_4b03d_row9_col42, #T_4b03d_row9_col43, #T_4b03d_row9_col44, #T_4b03d_row9_col45, #T_4b03d_row9_col46, #T_4b03d_row9_col47, #T_4b03d_row9_col48, #T_4b03d_row9_col49, #T_4b03d_row9_col50, #T_4b03d_row9_col51, #T_4b03d_row9_col52, #T_4b03d_row9_col53, #T_4b03d_row9_col54, #T_4b03d_row9_col55, #T_4b03d_row9_col56, #T_4b03d_row9_col57, #T_4b03d_row9_col58, #T_4b03d_row9_col59, #T_4b03d_row9_col60, #T_4b03d_row9_col61, #T_4b03d_row9_col62, #T_4b03d_row9_col63, #T_4b03d_row9_col64, #T_4b03d_row9_col65, #T_4b03d_row9_col66, #T_4b03d_row9_col67, #T_4b03d_row9_col68, #T_4b03d_row9_col69, #T_4b03d_row9_col70, #T_4b03d_row9_col71, #T_4b03d_row9_col72, #T_4b03d_row9_col73, #T_4b03d_row9_col74, #T_4b03d_row9_col75, #T_4b03d_row9_col76, #T_4b03d_row9_col77, #T_4b03d_row9_col78, #T_4b03d_row9_col79, #T_4b03d_row9_col80, #T_4b03d_row9_col81, #T_4b03d_row9_col82, #T_4b03d_row9_col83, #T_4b03d_row9_col84, #T_4b03d_row9_col85, #T_4b03d_row9_col86, #T_4b03d_row9_col87, #T_4b03d_row9_col88, #T_4b03d_row9_col89, #T_4b03d_row9_col90, #T_4b03d_row9_col91, #T_4b03d_row9_col92, #T_4b03d_row9_col93, #T_4b03d_row9_col94, #T_4b03d_row9_col95, #T_4b03d_row9_col96, #T_4b03d_row9_col97, #T_4b03d_row9_col98, #T_4b03d_row9_col99, #T_4b03d_row9_col100, #T_4b03d_row9_col101, #T_4b03d_row10_col0, #T_4b03d_row10_col1, #T_4b03d_row10_col2, #T_4b03d_row10_col3, #T_4b03d_row10_col4, #T_4b03d_row10_col5, #T_4b03d_row10_col6, #T_4b03d_row10_col7, #T_4b03d_row10_col8, #T_4b03d_row10_col9, #T_4b03d_row10_col10, #T_4b03d_row10_col11, #T_4b03d_row10_col12, #T_4b03d_row10_col13, #T_4b03d_row10_col14, #T_4b03d_row10_col15, #T_4b03d_row10_col16, #T_4b03d_row10_col17, #T_4b03d_row10_col18, #T_4b03d_row10_col19, #T_4b03d_row10_col20, #T_4b03d_row10_col21, #T_4b03d_row10_col22, #T_4b03d_row10_col23, #T_4b03d_row10_col24, #T_4b03d_row10_col25, #T_4b03d_row10_col26, #T_4b03d_row10_col27, #T_4b03d_row10_col28, #T_4b03d_row10_col29, #T_4b03d_row10_col30, #T_4b03d_row10_col31, #T_4b03d_row10_col32, #T_4b03d_row10_col33, #T_4b03d_row10_col34, #T_4b03d_row10_col35, #T_4b03d_row10_col36, #T_4b03d_row10_col37, #T_4b03d_row10_col38, #T_4b03d_row10_col39, #T_4b03d_row10_col40, #T_4b03d_row10_col41, #T_4b03d_row10_col42, #T_4b03d_row10_col43, #T_4b03d_row10_col44, #T_4b03d_row10_col45, #T_4b03d_row10_col46, #T_4b03d_row10_col47, #T_4b03d_row10_col48, #T_4b03d_row10_col49, #T_4b03d_row10_col50, #T_4b03d_row10_col51, #T_4b03d_row10_col52, #T_4b03d_row10_col53, #T_4b03d_row10_col54, #T_4b03d_row10_col55, #T_4b03d_row10_col56, #T_4b03d_row10_col57, #T_4b03d_row10_col58, #T_4b03d_row10_col59, #T_4b03d_row10_col60, #T_4b03d_row10_col61, #T_4b03d_row10_col62, #T_4b03d_row10_col63, #T_4b03d_row10_col64, #T_4b03d_row10_col65, #T_4b03d_row10_col66, #T_4b03d_row10_col67, #T_4b03d_row10_col68, #T_4b03d_row10_col69, #T_4b03d_row10_col70, #T_4b03d_row10_col71, #T_4b03d_row10_col72, #T_4b03d_row10_col73, #T_4b03d_row10_col74, #T_4b03d_row10_col75, #T_4b03d_row10_col76, #T_4b03d_row10_col77, #T_4b03d_row10_col78, #T_4b03d_row10_col79, #T_4b03d_row10_col80, #T_4b03d_row10_col81, #T_4b03d_row10_col82, #T_4b03d_row10_col83, #T_4b03d_row10_col84, #T_4b03d_row10_col85, #T_4b03d_row10_col86, #T_4b03d_row10_col87, #T_4b03d_row10_col88, #T_4b03d_row10_col89, #T_4b03d_row10_col90, #T_4b03d_row10_col91, #T_4b03d_row10_col92, #T_4b03d_row10_col93, #T_4b03d_row10_col94, #T_4b03d_row10_col95, #T_4b03d_row10_col96, #T_4b03d_row10_col97, #T_4b03d_row10_col98, #T_4b03d_row10_col99, #T_4b03d_row10_col100, #T_4b03d_row10_col101, #T_4b03d_row11_col0, #T_4b03d_row11_col1, #T_4b03d_row11_col2, #T_4b03d_row11_col3, #T_4b03d_row11_col4, #T_4b03d_row11_col5, #T_4b03d_row11_col6, #T_4b03d_row11_col7, #T_4b03d_row11_col8, #T_4b03d_row11_col9, #T_4b03d_row11_col10, #T_4b03d_row11_col11, #T_4b03d_row11_col12, #T_4b03d_row11_col13, #T_4b03d_row11_col14, #T_4b03d_row11_col15, #T_4b03d_row11_col16, #T_4b03d_row11_col17, #T_4b03d_row11_col18, #T_4b03d_row11_col19, #T_4b03d_row11_col20, #T_4b03d_row11_col21, #T_4b03d_row11_col22, #T_4b03d_row11_col23, #T_4b03d_row11_col24, #T_4b03d_row11_col25, #T_4b03d_row11_col26, #T_4b03d_row11_col27, #T_4b03d_row11_col28, #T_4b03d_row11_col29, #T_4b03d_row11_col30, #T_4b03d_row11_col31, #T_4b03d_row11_col32, #T_4b03d_row11_col33, #T_4b03d_row11_col34, #T_4b03d_row11_col35, #T_4b03d_row11_col36, #T_4b03d_row11_col37, #T_4b03d_row11_col38, #T_4b03d_row11_col39, #T_4b03d_row11_col40, #T_4b03d_row11_col41, #T_4b03d_row11_col42, #T_4b03d_row11_col43, #T_4b03d_row11_col44, #T_4b03d_row11_col45, #T_4b03d_row11_col46, #T_4b03d_row11_col47, #T_4b03d_row11_col48, #T_4b03d_row11_col49, #T_4b03d_row11_col50, #T_4b03d_row11_col51, #T_4b03d_row11_col52, #T_4b03d_row11_col53, #T_4b03d_row11_col54, #T_4b03d_row11_col55, #T_4b03d_row11_col56, #T_4b03d_row11_col57, #T_4b03d_row11_col58, #T_4b03d_row11_col59, #T_4b03d_row11_col60, #T_4b03d_row11_col61, #T_4b03d_row11_col62, #T_4b03d_row11_col63, #T_4b03d_row11_col64, #T_4b03d_row11_col65, #T_4b03d_row11_col66, #T_4b03d_row11_col67, #T_4b03d_row11_col68, #T_4b03d_row11_col69, #T_4b03d_row11_col70, #T_4b03d_row11_col71, #T_4b03d_row11_col72, #T_4b03d_row11_col73, #T_4b03d_row11_col74, #T_4b03d_row11_col75, #T_4b03d_row11_col76, #T_4b03d_row11_col77, #T_4b03d_row11_col78, #T_4b03d_row11_col79, #T_4b03d_row11_col80, #T_4b03d_row11_col81, #T_4b03d_row11_col82, #T_4b03d_row11_col83, #T_4b03d_row11_col84, #T_4b03d_row11_col85, #T_4b03d_row11_col86, #T_4b03d_row11_col87, #T_4b03d_row11_col88, #T_4b03d_row11_col89, #T_4b03d_row11_col90, #T_4b03d_row11_col91, #T_4b03d_row11_col92, #T_4b03d_row11_col93, #T_4b03d_row11_col94, #T_4b03d_row11_col95, #T_4b03d_row11_col96, #T_4b03d_row11_col97, #T_4b03d_row11_col98, #T_4b03d_row11_col99, #T_4b03d_row11_col100, #T_4b03d_row11_col101, #T_4b03d_row12_col0, #T_4b03d_row12_col1, #T_4b03d_row12_col2, #T_4b03d_row12_col3, #T_4b03d_row12_col4, #T_4b03d_row12_col5, #T_4b03d_row12_col6, #T_4b03d_row12_col7, #T_4b03d_row12_col8, #T_4b03d_row12_col9, #T_4b03d_row12_col10, #T_4b03d_row12_col11, #T_4b03d_row12_col12, #T_4b03d_row12_col13, #T_4b03d_row12_col14, #T_4b03d_row12_col15, #T_4b03d_row12_col16, #T_4b03d_row12_col17, #T_4b03d_row12_col18, #T_4b03d_row12_col19, #T_4b03d_row12_col20, #T_4b03d_row12_col21, #T_4b03d_row12_col22, #T_4b03d_row12_col23, #T_4b03d_row12_col24, #T_4b03d_row12_col25, #T_4b03d_row12_col26, #T_4b03d_row12_col27, #T_4b03d_row12_col28, #T_4b03d_row12_col29, #T_4b03d_row12_col30, #T_4b03d_row12_col31, #T_4b03d_row12_col32, #T_4b03d_row12_col33, #T_4b03d_row12_col34, #T_4b03d_row12_col35, #T_4b03d_row12_col36, #T_4b03d_row12_col37, #T_4b03d_row12_col38, #T_4b03d_row12_col39, #T_4b03d_row12_col40, #T_4b03d_row12_col41, #T_4b03d_row12_col42, #T_4b03d_row12_col43, #T_4b03d_row12_col44, #T_4b03d_row12_col45, #T_4b03d_row12_col46, #T_4b03d_row12_col47, #T_4b03d_row12_col48, #T_4b03d_row12_col49, #T_4b03d_row12_col50, #T_4b03d_row12_col51, #T_4b03d_row12_col52, #T_4b03d_row12_col53, #T_4b03d_row12_col54, #T_4b03d_row12_col55, #T_4b03d_row12_col56, #T_4b03d_row12_col57, #T_4b03d_row12_col58, #T_4b03d_row12_col59, #T_4b03d_row12_col60, #T_4b03d_row12_col61, #T_4b03d_row12_col62, #T_4b03d_row12_col63, #T_4b03d_row12_col64, #T_4b03d_row12_col65, #T_4b03d_row12_col66, #T_4b03d_row12_col67, #T_4b03d_row12_col68, #T_4b03d_row12_col69, #T_4b03d_row12_col70, #T_4b03d_row12_col71, #T_4b03d_row12_col72, #T_4b03d_row12_col73, #T_4b03d_row12_col74, #T_4b03d_row12_col75, #T_4b03d_row12_col76, #T_4b03d_row12_col77, #T_4b03d_row12_col78, #T_4b03d_row12_col79, #T_4b03d_row12_col80, #T_4b03d_row12_col81, #T_4b03d_row12_col82, #T_4b03d_row12_col83, #T_4b03d_row12_col84, #T_4b03d_row12_col85, #T_4b03d_row12_col86, #T_4b03d_row12_col87, #T_4b03d_row12_col88, #T_4b03d_row12_col89, #T_4b03d_row12_col90, #T_4b03d_row12_col91, #T_4b03d_row12_col92, #T_4b03d_row12_col93, #T_4b03d_row12_col94, #T_4b03d_row12_col95, #T_4b03d_row12_col96, #T_4b03d_row12_col97, #T_4b03d_row12_col98, #T_4b03d_row12_col99, #T_4b03d_row12_col100, #T_4b03d_row12_col101, #T_4b03d_row13_col0, #T_4b03d_row13_col1, #T_4b03d_row13_col2, #T_4b03d_row13_col3, #T_4b03d_row13_col4, #T_4b03d_row13_col5, #T_4b03d_row13_col6, #T_4b03d_row13_col7, #T_4b03d_row13_col8, #T_4b03d_row13_col9, #T_4b03d_row13_col10, #T_4b03d_row13_col11, #T_4b03d_row13_col12, #T_4b03d_row13_col13, #T_4b03d_row13_col14, #T_4b03d_row13_col15, #T_4b03d_row13_col16, #T_4b03d_row13_col17, #T_4b03d_row13_col18, #T_4b03d_row13_col19, #T_4b03d_row13_col20, #T_4b03d_row13_col21, #T_4b03d_row13_col22, #T_4b03d_row13_col23, #T_4b03d_row13_col24, #T_4b03d_row13_col25, #T_4b03d_row13_col26, #T_4b03d_row13_col27, #T_4b03d_row13_col28, #T_4b03d_row13_col29, #T_4b03d_row13_col30, #T_4b03d_row13_col31, #T_4b03d_row13_col32, #T_4b03d_row13_col33, #T_4b03d_row13_col34, #T_4b03d_row13_col35, #T_4b03d_row13_col36, #T_4b03d_row13_col37, #T_4b03d_row13_col38, #T_4b03d_row13_col39, #T_4b03d_row13_col40, #T_4b03d_row13_col41, #T_4b03d_row13_col42, #T_4b03d_row13_col43, #T_4b03d_row13_col44, #T_4b03d_row13_col45, #T_4b03d_row13_col46, #T_4b03d_row13_col47, #T_4b03d_row13_col48, #T_4b03d_row13_col49, #T_4b03d_row13_col50, #T_4b03d_row13_col51, #T_4b03d_row13_col52, #T_4b03d_row13_col53, #T_4b03d_row13_col54, #T_4b03d_row13_col55, #T_4b03d_row13_col56, #T_4b03d_row13_col57, #T_4b03d_row13_col58, #T_4b03d_row13_col59, #T_4b03d_row13_col60, #T_4b03d_row13_col61, #T_4b03d_row13_col62, #T_4b03d_row13_col63, #T_4b03d_row13_col64, #T_4b03d_row13_col65, #T_4b03d_row13_col66, #T_4b03d_row13_col67, #T_4b03d_row13_col68, #T_4b03d_row13_col69, #T_4b03d_row13_col70, #T_4b03d_row13_col71, #T_4b03d_row13_col72, #T_4b03d_row13_col73, #T_4b03d_row13_col74, #T_4b03d_row13_col75, #T_4b03d_row13_col76, #T_4b03d_row13_col77, #T_4b03d_row13_col78, #T_4b03d_row13_col79, #T_4b03d_row13_col80, #T_4b03d_row13_col81, #T_4b03d_row13_col82, #T_4b03d_row13_col83, #T_4b03d_row13_col84, #T_4b03d_row13_col85, #T_4b03d_row13_col86, #T_4b03d_row13_col87, #T_4b03d_row13_col88, #T_4b03d_row13_col89, #T_4b03d_row13_col90, #T_4b03d_row13_col91, #T_4b03d_row13_col92, #T_4b03d_row13_col93, #T_4b03d_row13_col94, #T_4b03d_row13_col95, #T_4b03d_row13_col96, #T_4b03d_row13_col97, #T_4b03d_row13_col98, #T_4b03d_row13_col99, #T_4b03d_row13_col100, #T_4b03d_row13_col101, #T_4b03d_row14_col0, #T_4b03d_row14_col1, #T_4b03d_row14_col2, #T_4b03d_row14_col3, #T_4b03d_row14_col4, #T_4b03d_row14_col5, #T_4b03d_row14_col6, #T_4b03d_row14_col7, #T_4b03d_row14_col8, #T_4b03d_row14_col9, #T_4b03d_row14_col10, #T_4b03d_row14_col11, #T_4b03d_row14_col12, #T_4b03d_row14_col13, #T_4b03d_row14_col14, #T_4b03d_row14_col15, #T_4b03d_row14_col16, #T_4b03d_row14_col17, #T_4b03d_row14_col18, #T_4b03d_row14_col19, #T_4b03d_row14_col20, #T_4b03d_row14_col21, #T_4b03d_row14_col22, #T_4b03d_row14_col23, #T_4b03d_row14_col24, #T_4b03d_row14_col25, #T_4b03d_row14_col26, #T_4b03d_row14_col27, #T_4b03d_row14_col28, #T_4b03d_row14_col29, #T_4b03d_row14_col30, #T_4b03d_row14_col31, #T_4b03d_row14_col32, #T_4b03d_row14_col33, #T_4b03d_row14_col34, #T_4b03d_row14_col35, #T_4b03d_row14_col36, #T_4b03d_row14_col37, #T_4b03d_row14_col38, #T_4b03d_row14_col39, #T_4b03d_row14_col40, #T_4b03d_row14_col41, #T_4b03d_row14_col42, #T_4b03d_row14_col43, #T_4b03d_row14_col44, #T_4b03d_row14_col45, #T_4b03d_row14_col46, #T_4b03d_row14_col47, #T_4b03d_row14_col48, #T_4b03d_row14_col49, #T_4b03d_row14_col50, #T_4b03d_row14_col51, #T_4b03d_row14_col52, #T_4b03d_row14_col53, #T_4b03d_row14_col54, #T_4b03d_row14_col55, #T_4b03d_row14_col56, #T_4b03d_row14_col57, #T_4b03d_row14_col58, #T_4b03d_row14_col59, #T_4b03d_row14_col60, #T_4b03d_row14_col61, #T_4b03d_row14_col62, #T_4b03d_row14_col63, #T_4b03d_row14_col64, #T_4b03d_row14_col65, #T_4b03d_row14_col66, #T_4b03d_row14_col67, #T_4b03d_row14_col68, #T_4b03d_row14_col69, #T_4b03d_row14_col70, #T_4b03d_row14_col71, #T_4b03d_row14_col72, #T_4b03d_row14_col73, #T_4b03d_row14_col74, #T_4b03d_row14_col75, #T_4b03d_row14_col76, #T_4b03d_row14_col77, #T_4b03d_row14_col78, #T_4b03d_row14_col79, #T_4b03d_row14_col80, #T_4b03d_row14_col81, #T_4b03d_row14_col82, #T_4b03d_row14_col83, #T_4b03d_row14_col84, #T_4b03d_row14_col85, #T_4b03d_row14_col86, #T_4b03d_row14_col87, #T_4b03d_row14_col88, #T_4b03d_row14_col89, #T_4b03d_row14_col90, #T_4b03d_row14_col91, #T_4b03d_row14_col92, #T_4b03d_row14_col93, #T_4b03d_row14_col94, #T_4b03d_row14_col95, #T_4b03d_row14_col96, #T_4b03d_row14_col97, #T_4b03d_row14_col98, #T_4b03d_row14_col99, #T_4b03d_row14_col100, #T_4b03d_row14_col101, #T_4b03d_row15_col0, #T_4b03d_row15_col1, #T_4b03d_row15_col2, #T_4b03d_row15_col3, #T_4b03d_row15_col4, #T_4b03d_row15_col5, #T_4b03d_row15_col6, #T_4b03d_row15_col7, #T_4b03d_row15_col8, #T_4b03d_row15_col9, #T_4b03d_row15_col10, #T_4b03d_row15_col11, #T_4b03d_row15_col12, #T_4b03d_row15_col13, #T_4b03d_row15_col14, #T_4b03d_row15_col15, #T_4b03d_row15_col16, #T_4b03d_row15_col17, #T_4b03d_row15_col18, #T_4b03d_row15_col19, #T_4b03d_row15_col20, #T_4b03d_row15_col21, #T_4b03d_row15_col22, #T_4b03d_row15_col23, #T_4b03d_row15_col24, #T_4b03d_row15_col25, #T_4b03d_row15_col26, #T_4b03d_row15_col27, #T_4b03d_row15_col28, #T_4b03d_row15_col29, #T_4b03d_row15_col30, #T_4b03d_row15_col31, #T_4b03d_row15_col32, #T_4b03d_row15_col33, #T_4b03d_row15_col34, #T_4b03d_row15_col35, #T_4b03d_row15_col36, #T_4b03d_row15_col37, #T_4b03d_row15_col38, #T_4b03d_row15_col39, #T_4b03d_row15_col40, #T_4b03d_row15_col41, #T_4b03d_row15_col42, #T_4b03d_row15_col43, #T_4b03d_row15_col44, #T_4b03d_row15_col45, #T_4b03d_row15_col46, #T_4b03d_row15_col47, #T_4b03d_row15_col48, #T_4b03d_row15_col49, #T_4b03d_row15_col50, #T_4b03d_row15_col51, #T_4b03d_row15_col52, #T_4b03d_row15_col53, #T_4b03d_row15_col54, #T_4b03d_row15_col55, #T_4b03d_row15_col56, #T_4b03d_row15_col57, #T_4b03d_row15_col58, #T_4b03d_row15_col59, #T_4b03d_row15_col60, #T_4b03d_row15_col61, #T_4b03d_row15_col62, #T_4b03d_row15_col63, #T_4b03d_row15_col64, #T_4b03d_row15_col65, #T_4b03d_row15_col66, #T_4b03d_row15_col67, #T_4b03d_row15_col68, #T_4b03d_row15_col69, #T_4b03d_row15_col70, #T_4b03d_row15_col71, #T_4b03d_row15_col72, #T_4b03d_row15_col73, #T_4b03d_row15_col74, #T_4b03d_row15_col75, #T_4b03d_row15_col76, #T_4b03d_row15_col77, #T_4b03d_row15_col78, #T_4b03d_row15_col79, #T_4b03d_row15_col80, #T_4b03d_row15_col81, #T_4b03d_row15_col82, #T_4b03d_row15_col83, #T_4b03d_row15_col84, #T_4b03d_row15_col85, #T_4b03d_row15_col86, #T_4b03d_row15_col87, #T_4b03d_row15_col88, #T_4b03d_row15_col89, #T_4b03d_row15_col90, #T_4b03d_row15_col91, #T_4b03d_row15_col92, #T_4b03d_row15_col93, #T_4b03d_row15_col94, #T_4b03d_row15_col95, #T_4b03d_row15_col96, #T_4b03d_row15_col97, #T_4b03d_row15_col98, #T_4b03d_row15_col99, #T_4b03d_row15_col100, #T_4b03d_row15_col101, #T_4b03d_row16_col0, #T_4b03d_row16_col1, #T_4b03d_row16_col2, #T_4b03d_row16_col3, #T_4b03d_row16_col4, #T_4b03d_row16_col5, #T_4b03d_row16_col6, #T_4b03d_row16_col7, #T_4b03d_row16_col8, #T_4b03d_row16_col9, #T_4b03d_row16_col10, #T_4b03d_row16_col11, #T_4b03d_row16_col12, #T_4b03d_row16_col13, #T_4b03d_row16_col14, #T_4b03d_row16_col15, #T_4b03d_row16_col16, #T_4b03d_row16_col17, #T_4b03d_row16_col18, #T_4b03d_row16_col19, #T_4b03d_row16_col20, #T_4b03d_row16_col21, #T_4b03d_row16_col22, #T_4b03d_row16_col23, #T_4b03d_row16_col24, #T_4b03d_row16_col25, #T_4b03d_row16_col26, #T_4b03d_row16_col27, #T_4b03d_row16_col28, #T_4b03d_row16_col29, #T_4b03d_row16_col30, #T_4b03d_row16_col31, #T_4b03d_row16_col32, #T_4b03d_row16_col33, #T_4b03d_row16_col34, #T_4b03d_row16_col35, #T_4b03d_row16_col36, #T_4b03d_row16_col37, #T_4b03d_row16_col38, #T_4b03d_row16_col39, #T_4b03d_row16_col40, #T_4b03d_row16_col41, #T_4b03d_row16_col42, #T_4b03d_row16_col43, #T_4b03d_row16_col44, #T_4b03d_row16_col45, #T_4b03d_row16_col46, #T_4b03d_row16_col47, #T_4b03d_row16_col48, #T_4b03d_row16_col49, #T_4b03d_row16_col50, #T_4b03d_row16_col51, #T_4b03d_row16_col52, #T_4b03d_row16_col53, #T_4b03d_row16_col54, #T_4b03d_row16_col55, #T_4b03d_row16_col56, #T_4b03d_row16_col57, #T_4b03d_row16_col58, #T_4b03d_row16_col59, #T_4b03d_row16_col60, #T_4b03d_row16_col61, #T_4b03d_row16_col62, #T_4b03d_row16_col63, #T_4b03d_row16_col64, #T_4b03d_row16_col65, #T_4b03d_row16_col66, #T_4b03d_row16_col67, #T_4b03d_row16_col68, #T_4b03d_row16_col69, #T_4b03d_row16_col70, #T_4b03d_row16_col71, #T_4b03d_row16_col72, #T_4b03d_row16_col73, #T_4b03d_row16_col74, #T_4b03d_row16_col75, #T_4b03d_row16_col76, #T_4b03d_row16_col77, #T_4b03d_row16_col78, #T_4b03d_row16_col79, #T_4b03d_row16_col80, #T_4b03d_row16_col81, #T_4b03d_row16_col82, #T_4b03d_row16_col83, #T_4b03d_row16_col84, #T_4b03d_row16_col85, #T_4b03d_row16_col86, #T_4b03d_row16_col87, #T_4b03d_row16_col88, #T_4b03d_row16_col89, #T_4b03d_row16_col90, #T_4b03d_row16_col91, #T_4b03d_row16_col92, #T_4b03d_row16_col93, #T_4b03d_row16_col94, #T_4b03d_row16_col95, #T_4b03d_row16_col96, #T_4b03d_row16_col97, #T_4b03d_row16_col98, #T_4b03d_row16_col99, #T_4b03d_row16_col100, #T_4b03d_row16_col101, #T_4b03d_row17_col0, #T_4b03d_row17_col1, #T_4b03d_row17_col2, #T_4b03d_row17_col3, #T_4b03d_row17_col4, #T_4b03d_row17_col5, #T_4b03d_row17_col6, #T_4b03d_row17_col7, #T_4b03d_row17_col8, #T_4b03d_row17_col9, #T_4b03d_row17_col10, #T_4b03d_row17_col11, #T_4b03d_row17_col12, #T_4b03d_row17_col13, #T_4b03d_row17_col14, #T_4b03d_row17_col15, #T_4b03d_row17_col16, #T_4b03d_row17_col17, #T_4b03d_row17_col18, #T_4b03d_row17_col19, #T_4b03d_row17_col20, #T_4b03d_row17_col21, #T_4b03d_row17_col22, #T_4b03d_row17_col23, #T_4b03d_row17_col24, #T_4b03d_row17_col25, #T_4b03d_row17_col26, #T_4b03d_row17_col27, #T_4b03d_row17_col28, #T_4b03d_row17_col29, #T_4b03d_row17_col30, #T_4b03d_row17_col31, #T_4b03d_row17_col32, #T_4b03d_row17_col33, #T_4b03d_row17_col34, #T_4b03d_row17_col35, #T_4b03d_row17_col36, #T_4b03d_row17_col37, #T_4b03d_row17_col38, #T_4b03d_row17_col39, #T_4b03d_row17_col40, #T_4b03d_row17_col41, #T_4b03d_row17_col42, #T_4b03d_row17_col43, #T_4b03d_row17_col44, #T_4b03d_row17_col45, #T_4b03d_row17_col46, #T_4b03d_row17_col47, #T_4b03d_row17_col48, #T_4b03d_row17_col49, #T_4b03d_row17_col50, #T_4b03d_row17_col51, #T_4b03d_row17_col52, #T_4b03d_row17_col53, #T_4b03d_row17_col54, #T_4b03d_row17_col55, #T_4b03d_row17_col56, #T_4b03d_row17_col57, #T_4b03d_row17_col58, #T_4b03d_row17_col59, #T_4b03d_row17_col60, #T_4b03d_row17_col61, #T_4b03d_row17_col62, #T_4b03d_row17_col63, #T_4b03d_row17_col64, #T_4b03d_row17_col65, #T_4b03d_row17_col66, #T_4b03d_row17_col67, #T_4b03d_row17_col68, #T_4b03d_row17_col69, #T_4b03d_row17_col70, #T_4b03d_row17_col71, #T_4b03d_row17_col72, #T_4b03d_row17_col73, #T_4b03d_row17_col74, #T_4b03d_row17_col75, #T_4b03d_row17_col76, #T_4b03d_row17_col77, #T_4b03d_row17_col78, #T_4b03d_row17_col79, #T_4b03d_row17_col80, #T_4b03d_row17_col81, #T_4b03d_row17_col82, #T_4b03d_row17_col83, #T_4b03d_row17_col84, #T_4b03d_row17_col85, #T_4b03d_row17_col86, #T_4b03d_row17_col87, #T_4b03d_row17_col88, #T_4b03d_row17_col89, #T_4b03d_row17_col90, #T_4b03d_row17_col91, #T_4b03d_row17_col92, #T_4b03d_row17_col93, #T_4b03d_row17_col94, #T_4b03d_row17_col95, #T_4b03d_row17_col96, #T_4b03d_row17_col97, #T_4b03d_row17_col98, #T_4b03d_row17_col99, #T_4b03d_row17_col100, #T_4b03d_row17_col101, #T_4b03d_row18_col0, #T_4b03d_row18_col1, #T_4b03d_row18_col2, #T_4b03d_row18_col3, #T_4b03d_row18_col4, #T_4b03d_row18_col5, #T_4b03d_row18_col6, #T_4b03d_row18_col7, #T_4b03d_row18_col8, #T_4b03d_row18_col9, #T_4b03d_row18_col10, #T_4b03d_row18_col11, #T_4b03d_row18_col12, #T_4b03d_row18_col13, #T_4b03d_row18_col14, #T_4b03d_row18_col15, #T_4b03d_row18_col16, #T_4b03d_row18_col17, #T_4b03d_row18_col18, #T_4b03d_row18_col19, #T_4b03d_row18_col20, #T_4b03d_row18_col21, #T_4b03d_row18_col22, #T_4b03d_row18_col23, #T_4b03d_row18_col24, #T_4b03d_row18_col25, #T_4b03d_row18_col26, #T_4b03d_row18_col27, #T_4b03d_row18_col28, #T_4b03d_row18_col29, #T_4b03d_row18_col30, #T_4b03d_row18_col31, #T_4b03d_row18_col32, #T_4b03d_row18_col33, #T_4b03d_row18_col34, #T_4b03d_row18_col35, #T_4b03d_row18_col36, #T_4b03d_row18_col37, #T_4b03d_row18_col38, #T_4b03d_row18_col39, #T_4b03d_row18_col40, #T_4b03d_row18_col41, #T_4b03d_row18_col42, #T_4b03d_row18_col43, #T_4b03d_row18_col44, #T_4b03d_row18_col45, #T_4b03d_row18_col46, #T_4b03d_row18_col47, #T_4b03d_row18_col48, #T_4b03d_row18_col49, #T_4b03d_row18_col50, #T_4b03d_row18_col51, #T_4b03d_row18_col52, #T_4b03d_row18_col53, #T_4b03d_row18_col54, #T_4b03d_row18_col55, #T_4b03d_row18_col56, #T_4b03d_row18_col57, #T_4b03d_row18_col58, #T_4b03d_row18_col59, #T_4b03d_row18_col60, #T_4b03d_row18_col61, #T_4b03d_row18_col62, #T_4b03d_row18_col63, #T_4b03d_row18_col64, #T_4b03d_row18_col65, #T_4b03d_row18_col66, #T_4b03d_row18_col67, #T_4b03d_row18_col68, #T_4b03d_row18_col69, #T_4b03d_row18_col70, #T_4b03d_row18_col71, #T_4b03d_row18_col72, #T_4b03d_row18_col73, #T_4b03d_row18_col74, #T_4b03d_row18_col75, #T_4b03d_row18_col76, #T_4b03d_row18_col77, #T_4b03d_row18_col78, #T_4b03d_row18_col79, #T_4b03d_row18_col80, #T_4b03d_row18_col81, #T_4b03d_row18_col82, #T_4b03d_row18_col83, #T_4b03d_row18_col84, #T_4b03d_row18_col85, #T_4b03d_row18_col86, #T_4b03d_row18_col87, #T_4b03d_row18_col88, #T_4b03d_row18_col89, #T_4b03d_row18_col90, #T_4b03d_row18_col91, #T_4b03d_row18_col92, #T_4b03d_row18_col93, #T_4b03d_row18_col94, #T_4b03d_row18_col95, #T_4b03d_row18_col96, #T_4b03d_row18_col97, #T_4b03d_row18_col98, #T_4b03d_row18_col99, #T_4b03d_row18_col100, #T_4b03d_row18_col101, #T_4b03d_row19_col0, #T_4b03d_row19_col1, #T_4b03d_row19_col2, #T_4b03d_row19_col3, #T_4b03d_row19_col4, #T_4b03d_row19_col5, #T_4b03d_row19_col6, #T_4b03d_row19_col7, #T_4b03d_row19_col8, #T_4b03d_row19_col9, #T_4b03d_row19_col10, #T_4b03d_row19_col11, #T_4b03d_row19_col12, #T_4b03d_row19_col13, #T_4b03d_row19_col14, #T_4b03d_row19_col15, #T_4b03d_row19_col16, #T_4b03d_row19_col17, #T_4b03d_row19_col18, #T_4b03d_row19_col19, #T_4b03d_row19_col20, #T_4b03d_row19_col21, #T_4b03d_row19_col22, #T_4b03d_row19_col23, #T_4b03d_row19_col24, #T_4b03d_row19_col25, #T_4b03d_row19_col26, #T_4b03d_row19_col27, #T_4b03d_row19_col28, #T_4b03d_row19_col29, #T_4b03d_row19_col30, #T_4b03d_row19_col31, #T_4b03d_row19_col32, #T_4b03d_row19_col33, #T_4b03d_row19_col34, #T_4b03d_row19_col35, #T_4b03d_row19_col36, #T_4b03d_row19_col37, #T_4b03d_row19_col38, #T_4b03d_row19_col39, #T_4b03d_row19_col40, #T_4b03d_row19_col41, #T_4b03d_row19_col42, #T_4b03d_row19_col43, #T_4b03d_row19_col44, #T_4b03d_row19_col45, #T_4b03d_row19_col46, #T_4b03d_row19_col47, #T_4b03d_row19_col48, #T_4b03d_row19_col49, #T_4b03d_row19_col50, #T_4b03d_row19_col51, #T_4b03d_row19_col52, #T_4b03d_row19_col53, #T_4b03d_row19_col54, #T_4b03d_row19_col55, #T_4b03d_row19_col56, #T_4b03d_row19_col57, #T_4b03d_row19_col58, #T_4b03d_row19_col59, #T_4b03d_row19_col60, #T_4b03d_row19_col61, #T_4b03d_row19_col62, #T_4b03d_row19_col63, #T_4b03d_row19_col64, #T_4b03d_row19_col65, #T_4b03d_row19_col66, #T_4b03d_row19_col67, #T_4b03d_row19_col68, #T_4b03d_row19_col69, #T_4b03d_row19_col70, #T_4b03d_row19_col71, #T_4b03d_row19_col72, #T_4b03d_row19_col73, #T_4b03d_row19_col74, #T_4b03d_row19_col75, #T_4b03d_row19_col76, #T_4b03d_row19_col77, #T_4b03d_row19_col78, #T_4b03d_row19_col79, #T_4b03d_row19_col80, #T_4b03d_row19_col81, #T_4b03d_row19_col82, #T_4b03d_row19_col83, #T_4b03d_row19_col84, #T_4b03d_row19_col85, #T_4b03d_row19_col86, #T_4b03d_row19_col87, #T_4b03d_row19_col88, #T_4b03d_row19_col89, #T_4b03d_row19_col90, #T_4b03d_row19_col91, #T_4b03d_row19_col92, #T_4b03d_row19_col93, #T_4b03d_row19_col94, #T_4b03d_row19_col95, #T_4b03d_row19_col96, #T_4b03d_row19_col97, #T_4b03d_row19_col98, #T_4b03d_row19_col99, #T_4b03d_row19_col100, #T_4b03d_row19_col101, #T_4b03d_row20_col0, #T_4b03d_row20_col1, #T_4b03d_row20_col2, #T_4b03d_row20_col3, #T_4b03d_row20_col4, #T_4b03d_row20_col5, #T_4b03d_row20_col6, #T_4b03d_row20_col7, #T_4b03d_row20_col8, #T_4b03d_row20_col9, #T_4b03d_row20_col10, #T_4b03d_row20_col11, #T_4b03d_row20_col12, #T_4b03d_row20_col13, #T_4b03d_row20_col14, #T_4b03d_row20_col15, #T_4b03d_row20_col16, #T_4b03d_row20_col17, #T_4b03d_row20_col18, #T_4b03d_row20_col19, #T_4b03d_row20_col20, #T_4b03d_row20_col21, #T_4b03d_row20_col22, #T_4b03d_row20_col23, #T_4b03d_row20_col24, #T_4b03d_row20_col25, #T_4b03d_row20_col26, #T_4b03d_row20_col27, #T_4b03d_row20_col28, #T_4b03d_row20_col29, #T_4b03d_row20_col30, #T_4b03d_row20_col31, #T_4b03d_row20_col32, #T_4b03d_row20_col33, #T_4b03d_row20_col34, #T_4b03d_row20_col35, #T_4b03d_row20_col36, #T_4b03d_row20_col37, #T_4b03d_row20_col38, #T_4b03d_row20_col39, #T_4b03d_row20_col40, #T_4b03d_row20_col41, #T_4b03d_row20_col42, #T_4b03d_row20_col43, #T_4b03d_row20_col44, #T_4b03d_row20_col45, #T_4b03d_row20_col46, #T_4b03d_row20_col47, #T_4b03d_row20_col48, #T_4b03d_row20_col49, #T_4b03d_row20_col50, #T_4b03d_row20_col51, #T_4b03d_row20_col52, #T_4b03d_row20_col53, #T_4b03d_row20_col54, #T_4b03d_row20_col55, #T_4b03d_row20_col56, #T_4b03d_row20_col57, #T_4b03d_row20_col58, #T_4b03d_row20_col59, #T_4b03d_row20_col60, #T_4b03d_row20_col61, #T_4b03d_row20_col62, #T_4b03d_row20_col63, #T_4b03d_row20_col64, #T_4b03d_row20_col65, #T_4b03d_row20_col66, #T_4b03d_row20_col67, #T_4b03d_row20_col68, #T_4b03d_row20_col69, #T_4b03d_row20_col70, #T_4b03d_row20_col71, #T_4b03d_row20_col72, #T_4b03d_row20_col73, #T_4b03d_row20_col74, #T_4b03d_row20_col75, #T_4b03d_row20_col76, #T_4b03d_row20_col77, #T_4b03d_row20_col78, #T_4b03d_row20_col79, #T_4b03d_row20_col80, #T_4b03d_row20_col81, #T_4b03d_row20_col82, #T_4b03d_row20_col83, #T_4b03d_row20_col84, #T_4b03d_row20_col85, #T_4b03d_row20_col86, #T_4b03d_row20_col87, #T_4b03d_row20_col88, #T_4b03d_row20_col89, #T_4b03d_row20_col90, #T_4b03d_row20_col91, #T_4b03d_row20_col92, #T_4b03d_row20_col93, #T_4b03d_row20_col94, #T_4b03d_row20_col95, #T_4b03d_row20_col96, #T_4b03d_row20_col97, #T_4b03d_row20_col98, #T_4b03d_row20_col99, #T_4b03d_row20_col100, #T_4b03d_row20_col101, #T_4b03d_row21_col0, #T_4b03d_row21_col1, #T_4b03d_row21_col2, #T_4b03d_row21_col3, #T_4b03d_row21_col4, #T_4b03d_row21_col5, #T_4b03d_row21_col6, #T_4b03d_row21_col7, #T_4b03d_row21_col8, #T_4b03d_row21_col9, #T_4b03d_row21_col10, #T_4b03d_row21_col11, #T_4b03d_row21_col12, #T_4b03d_row21_col13, #T_4b03d_row21_col14, #T_4b03d_row21_col15, #T_4b03d_row21_col16, #T_4b03d_row21_col17, #T_4b03d_row21_col18, #T_4b03d_row21_col19, #T_4b03d_row21_col20, #T_4b03d_row21_col21, #T_4b03d_row21_col22, #T_4b03d_row21_col23, #T_4b03d_row21_col24, #T_4b03d_row21_col25, #T_4b03d_row21_col26, #T_4b03d_row21_col27, #T_4b03d_row21_col28, #T_4b03d_row21_col29, #T_4b03d_row21_col30, #T_4b03d_row21_col31, #T_4b03d_row21_col32, #T_4b03d_row21_col33, #T_4b03d_row21_col34, #T_4b03d_row21_col35, #T_4b03d_row21_col36, #T_4b03d_row21_col37, #T_4b03d_row21_col38, #T_4b03d_row21_col39, #T_4b03d_row21_col40, #T_4b03d_row21_col41, #T_4b03d_row21_col42, #T_4b03d_row21_col43, #T_4b03d_row21_col44, #T_4b03d_row21_col45, #T_4b03d_row21_col46, #T_4b03d_row21_col47, #T_4b03d_row21_col48, #T_4b03d_row21_col49, #T_4b03d_row21_col50, #T_4b03d_row21_col51, #T_4b03d_row21_col52, #T_4b03d_row21_col53, #T_4b03d_row21_col54, #T_4b03d_row21_col55, #T_4b03d_row21_col56, #T_4b03d_row21_col57, #T_4b03d_row21_col58, #T_4b03d_row21_col59, #T_4b03d_row21_col60, #T_4b03d_row21_col61, #T_4b03d_row21_col62, #T_4b03d_row21_col63, #T_4b03d_row21_col64, #T_4b03d_row21_col65, #T_4b03d_row21_col66, #T_4b03d_row21_col67, #T_4b03d_row21_col68, #T_4b03d_row21_col69, #T_4b03d_row21_col70, #T_4b03d_row21_col71, #T_4b03d_row21_col72, #T_4b03d_row21_col73, #T_4b03d_row21_col74, #T_4b03d_row21_col75, #T_4b03d_row21_col76, #T_4b03d_row21_col77, #T_4b03d_row21_col78, #T_4b03d_row21_col79, #T_4b03d_row21_col80, #T_4b03d_row21_col81, #T_4b03d_row21_col82, #T_4b03d_row21_col83, #T_4b03d_row21_col84, #T_4b03d_row21_col85, #T_4b03d_row21_col86, #T_4b03d_row21_col87, #T_4b03d_row21_col88, #T_4b03d_row21_col89, #T_4b03d_row21_col90, #T_4b03d_row21_col91, #T_4b03d_row21_col92, #T_4b03d_row21_col93, #T_4b03d_row21_col94, #T_4b03d_row21_col95, #T_4b03d_row21_col96, #T_4b03d_row21_col97, #T_4b03d_row21_col98, #T_4b03d_row21_col99, #T_4b03d_row21_col100, #T_4b03d_row21_col101, #T_4b03d_row22_col0, #T_4b03d_row22_col1, #T_4b03d_row22_col2, #T_4b03d_row22_col3, #T_4b03d_row22_col4, #T_4b03d_row22_col5, #T_4b03d_row22_col6, #T_4b03d_row22_col7, #T_4b03d_row22_col8, #T_4b03d_row22_col9, #T_4b03d_row22_col10, #T_4b03d_row22_col11, #T_4b03d_row22_col12, #T_4b03d_row22_col13, #T_4b03d_row22_col14, #T_4b03d_row22_col15, #T_4b03d_row22_col16, #T_4b03d_row22_col17, #T_4b03d_row22_col18, #T_4b03d_row22_col19, #T_4b03d_row22_col20, #T_4b03d_row22_col21, #T_4b03d_row22_col22, #T_4b03d_row22_col23, #T_4b03d_row22_col24, #T_4b03d_row22_col25, #T_4b03d_row22_col26, #T_4b03d_row22_col27, #T_4b03d_row22_col28, #T_4b03d_row22_col29, #T_4b03d_row22_col30, #T_4b03d_row22_col31, #T_4b03d_row22_col32, #T_4b03d_row22_col33, #T_4b03d_row22_col34, #T_4b03d_row22_col35, #T_4b03d_row22_col36, #T_4b03d_row22_col37, #T_4b03d_row22_col38, #T_4b03d_row22_col39, #T_4b03d_row22_col40, #T_4b03d_row22_col41, #T_4b03d_row22_col42, #T_4b03d_row22_col43, #T_4b03d_row22_col44, #T_4b03d_row22_col45, #T_4b03d_row22_col46, #T_4b03d_row22_col47, #T_4b03d_row22_col48, #T_4b03d_row22_col49, #T_4b03d_row22_col50, #T_4b03d_row22_col51, #T_4b03d_row22_col52, #T_4b03d_row22_col53, #T_4b03d_row22_col54, #T_4b03d_row22_col55, #T_4b03d_row22_col56, #T_4b03d_row22_col57, #T_4b03d_row22_col58, #T_4b03d_row22_col59, #T_4b03d_row22_col60, #T_4b03d_row22_col61, #T_4b03d_row22_col62, #T_4b03d_row22_col63, #T_4b03d_row22_col64, #T_4b03d_row22_col65, #T_4b03d_row22_col66, #T_4b03d_row22_col67, #T_4b03d_row22_col68, #T_4b03d_row22_col69, #T_4b03d_row22_col70, #T_4b03d_row22_col71, #T_4b03d_row22_col72, #T_4b03d_row22_col73, #T_4b03d_row22_col74, #T_4b03d_row22_col75, #T_4b03d_row22_col76, #T_4b03d_row22_col77, #T_4b03d_row22_col78, #T_4b03d_row22_col79, #T_4b03d_row22_col80, #T_4b03d_row22_col81, #T_4b03d_row22_col82, #T_4b03d_row22_col83, #T_4b03d_row22_col84, #T_4b03d_row22_col85, #T_4b03d_row22_col86, #T_4b03d_row22_col87, #T_4b03d_row22_col88, #T_4b03d_row22_col89, #T_4b03d_row22_col90, #T_4b03d_row22_col91, #T_4b03d_row22_col92, #T_4b03d_row22_col93, #T_4b03d_row22_col94, #T_4b03d_row22_col95, #T_4b03d_row22_col96, #T_4b03d_row22_col97, #T_4b03d_row22_col98, #T_4b03d_row22_col99, #T_4b03d_row22_col100, #T_4b03d_row22_col101, #T_4b03d_row23_col0, #T_4b03d_row23_col1, #T_4b03d_row23_col2, #T_4b03d_row23_col3, #T_4b03d_row23_col4, #T_4b03d_row23_col5, #T_4b03d_row23_col6, #T_4b03d_row23_col7, #T_4b03d_row23_col8, #T_4b03d_row23_col9, #T_4b03d_row23_col10, #T_4b03d_row23_col11, #T_4b03d_row23_col12, #T_4b03d_row23_col13, #T_4b03d_row23_col14, #T_4b03d_row23_col15, #T_4b03d_row23_col16, #T_4b03d_row23_col17, #T_4b03d_row23_col18, #T_4b03d_row23_col19, #T_4b03d_row23_col20, #T_4b03d_row23_col21, #T_4b03d_row23_col22, #T_4b03d_row23_col23, #T_4b03d_row23_col24, #T_4b03d_row23_col25, #T_4b03d_row23_col26, #T_4b03d_row23_col27, #T_4b03d_row23_col28, #T_4b03d_row23_col29, #T_4b03d_row23_col30, #T_4b03d_row23_col31, #T_4b03d_row23_col32, #T_4b03d_row23_col33, #T_4b03d_row23_col34, #T_4b03d_row23_col35, #T_4b03d_row23_col36, #T_4b03d_row23_col37, #T_4b03d_row23_col38, #T_4b03d_row23_col39, #T_4b03d_row23_col40, #T_4b03d_row23_col41, #T_4b03d_row23_col42, #T_4b03d_row23_col43, #T_4b03d_row23_col44, #T_4b03d_row23_col45, #T_4b03d_row23_col46, #T_4b03d_row23_col47, #T_4b03d_row23_col48, #T_4b03d_row23_col49, #T_4b03d_row23_col50, #T_4b03d_row23_col51, #T_4b03d_row23_col52, #T_4b03d_row23_col53, #T_4b03d_row23_col54, #T_4b03d_row23_col55, #T_4b03d_row23_col56, #T_4b03d_row23_col57, #T_4b03d_row23_col58, #T_4b03d_row23_col59, #T_4b03d_row23_col60, #T_4b03d_row23_col61, #T_4b03d_row23_col62, #T_4b03d_row23_col63, #T_4b03d_row23_col64, #T_4b03d_row23_col65, #T_4b03d_row23_col66, #T_4b03d_row23_col67, #T_4b03d_row23_col68, #T_4b03d_row23_col69, #T_4b03d_row23_col70, #T_4b03d_row23_col71, #T_4b03d_row23_col72, #T_4b03d_row23_col73, #T_4b03d_row23_col74, #T_4b03d_row23_col75, #T_4b03d_row23_col76, #T_4b03d_row23_col77, #T_4b03d_row23_col78, #T_4b03d_row23_col79, #T_4b03d_row23_col80, #T_4b03d_row23_col81, #T_4b03d_row23_col82, #T_4b03d_row23_col83, #T_4b03d_row23_col84, #T_4b03d_row23_col85, #T_4b03d_row23_col86, #T_4b03d_row23_col87, #T_4b03d_row23_col88, #T_4b03d_row23_col89, #T_4b03d_row23_col90, #T_4b03d_row23_col91, #T_4b03d_row23_col92, #T_4b03d_row23_col93, #T_4b03d_row23_col94, #T_4b03d_row23_col95, #T_4b03d_row23_col96, #T_4b03d_row23_col97, #T_4b03d_row23_col98, #T_4b03d_row23_col99, #T_4b03d_row23_col100, #T_4b03d_row23_col101, #T_4b03d_row24_col0, #T_4b03d_row24_col1, #T_4b03d_row24_col2, #T_4b03d_row24_col3, #T_4b03d_row24_col4, #T_4b03d_row24_col5, #T_4b03d_row24_col6, #T_4b03d_row24_col7, #T_4b03d_row24_col8, #T_4b03d_row24_col9, #T_4b03d_row24_col10, #T_4b03d_row24_col11, #T_4b03d_row24_col12, #T_4b03d_row24_col13, #T_4b03d_row24_col14, #T_4b03d_row24_col15, #T_4b03d_row24_col16, #T_4b03d_row24_col17, #T_4b03d_row24_col18, #T_4b03d_row24_col19, #T_4b03d_row24_col20, #T_4b03d_row24_col21, #T_4b03d_row24_col22, #T_4b03d_row24_col23, #T_4b03d_row24_col24, #T_4b03d_row24_col25, #T_4b03d_row24_col26, #T_4b03d_row24_col27, #T_4b03d_row24_col28, #T_4b03d_row24_col29, #T_4b03d_row24_col30, #T_4b03d_row24_col31, #T_4b03d_row24_col32, #T_4b03d_row24_col33, #T_4b03d_row24_col34, #T_4b03d_row24_col35, #T_4b03d_row24_col36, #T_4b03d_row24_col37, #T_4b03d_row24_col38, #T_4b03d_row24_col39, #T_4b03d_row24_col40, #T_4b03d_row24_col41, #T_4b03d_row24_col42, #T_4b03d_row24_col43, #T_4b03d_row24_col44, #T_4b03d_row24_col45, #T_4b03d_row24_col46, #T_4b03d_row24_col47, #T_4b03d_row24_col48, #T_4b03d_row24_col49, #T_4b03d_row24_col50, #T_4b03d_row24_col51, #T_4b03d_row24_col52, #T_4b03d_row24_col53, #T_4b03d_row24_col54, #T_4b03d_row24_col55, #T_4b03d_row24_col56, #T_4b03d_row24_col57, #T_4b03d_row24_col58, #T_4b03d_row24_col59, #T_4b03d_row24_col60, #T_4b03d_row24_col61, #T_4b03d_row24_col62, #T_4b03d_row24_col63, #T_4b03d_row24_col64, #T_4b03d_row24_col65, #T_4b03d_row24_col66, #T_4b03d_row24_col67, #T_4b03d_row24_col68, #T_4b03d_row24_col69, #T_4b03d_row24_col70, #T_4b03d_row24_col71, #T_4b03d_row24_col72, #T_4b03d_row24_col73, #T_4b03d_row24_col74, #T_4b03d_row24_col75, #T_4b03d_row24_col76, #T_4b03d_row24_col77, #T_4b03d_row24_col78, #T_4b03d_row24_col79, #T_4b03d_row24_col80, #T_4b03d_row24_col81, #T_4b03d_row24_col82, #T_4b03d_row24_col83, #T_4b03d_row24_col84, #T_4b03d_row24_col85, #T_4b03d_row24_col86, #T_4b03d_row24_col87, #T_4b03d_row24_col88, #T_4b03d_row24_col89, #T_4b03d_row24_col90, #T_4b03d_row24_col91, #T_4b03d_row24_col92, #T_4b03d_row24_col93, #T_4b03d_row24_col94, #T_4b03d_row24_col95, #T_4b03d_row24_col96, #T_4b03d_row24_col97, #T_4b03d_row24_col98, #T_4b03d_row24_col99, #T_4b03d_row24_col100, #T_4b03d_row24_col101, #T_4b03d_row25_col0, #T_4b03d_row25_col1, #T_4b03d_row25_col2, #T_4b03d_row25_col3, #T_4b03d_row25_col4, #T_4b03d_row25_col5, #T_4b03d_row25_col6, #T_4b03d_row25_col7, #T_4b03d_row25_col8, #T_4b03d_row25_col9, #T_4b03d_row25_col10, #T_4b03d_row25_col11, #T_4b03d_row25_col12, #T_4b03d_row25_col13, #T_4b03d_row25_col14, #T_4b03d_row25_col15, #T_4b03d_row25_col16, #T_4b03d_row25_col17, #T_4b03d_row25_col18, #T_4b03d_row25_col19, #T_4b03d_row25_col20, #T_4b03d_row25_col21, #T_4b03d_row25_col22, #T_4b03d_row25_col23, #T_4b03d_row25_col24, #T_4b03d_row25_col25, #T_4b03d_row25_col26, #T_4b03d_row25_col27, #T_4b03d_row25_col28, #T_4b03d_row25_col29, #T_4b03d_row25_col30, #T_4b03d_row25_col31, #T_4b03d_row25_col32, #T_4b03d_row25_col33, #T_4b03d_row25_col34, #T_4b03d_row25_col35, #T_4b03d_row25_col36, #T_4b03d_row25_col37, #T_4b03d_row25_col38, #T_4b03d_row25_col39, #T_4b03d_row25_col40, #T_4b03d_row25_col41, #T_4b03d_row25_col42, #T_4b03d_row25_col43, #T_4b03d_row25_col44, #T_4b03d_row25_col45, #T_4b03d_row25_col46, #T_4b03d_row25_col47, #T_4b03d_row25_col48, #T_4b03d_row25_col49, #T_4b03d_row25_col50, #T_4b03d_row25_col51, #T_4b03d_row25_col52, #T_4b03d_row25_col53, #T_4b03d_row25_col54, #T_4b03d_row25_col55, #T_4b03d_row25_col56, #T_4b03d_row25_col57, #T_4b03d_row25_col58, #T_4b03d_row25_col59, #T_4b03d_row25_col60, #T_4b03d_row25_col61, #T_4b03d_row25_col62, #T_4b03d_row25_col63, #T_4b03d_row25_col64, #T_4b03d_row25_col65, #T_4b03d_row25_col66, #T_4b03d_row25_col67, #T_4b03d_row25_col68, #T_4b03d_row25_col69, #T_4b03d_row25_col70, #T_4b03d_row25_col71, #T_4b03d_row25_col72, #T_4b03d_row25_col73, #T_4b03d_row25_col74, #T_4b03d_row25_col75, #T_4b03d_row25_col76, #T_4b03d_row25_col77, #T_4b03d_row25_col78, #T_4b03d_row25_col79, #T_4b03d_row25_col80, #T_4b03d_row25_col81, #T_4b03d_row25_col82, #T_4b03d_row25_col83, #T_4b03d_row25_col84, #T_4b03d_row25_col85, #T_4b03d_row25_col86, #T_4b03d_row25_col87, #T_4b03d_row25_col88, #T_4b03d_row25_col89, #T_4b03d_row25_col90, #T_4b03d_row25_col91, #T_4b03d_row25_col92, #T_4b03d_row25_col93, #T_4b03d_row25_col94, #T_4b03d_row25_col95, #T_4b03d_row25_col96, #T_4b03d_row25_col97, #T_4b03d_row25_col98, #T_4b03d_row25_col99, #T_4b03d_row25_col100, #T_4b03d_row25_col101, #T_4b03d_row26_col0, #T_4b03d_row26_col1, #T_4b03d_row26_col2, #T_4b03d_row26_col3, #T_4b03d_row26_col4, #T_4b03d_row26_col5, #T_4b03d_row26_col6, #T_4b03d_row26_col7, #T_4b03d_row26_col8, #T_4b03d_row26_col9, #T_4b03d_row26_col10, #T_4b03d_row26_col11, #T_4b03d_row26_col12, #T_4b03d_row26_col13, #T_4b03d_row26_col14, #T_4b03d_row26_col15, #T_4b03d_row26_col16, #T_4b03d_row26_col17, #T_4b03d_row26_col18, #T_4b03d_row26_col19, #T_4b03d_row26_col20, #T_4b03d_row26_col21, #T_4b03d_row26_col22, #T_4b03d_row26_col23, #T_4b03d_row26_col24, #T_4b03d_row26_col25, #T_4b03d_row26_col26, #T_4b03d_row26_col27, #T_4b03d_row26_col28, #T_4b03d_row26_col29, #T_4b03d_row26_col30, #T_4b03d_row26_col31, #T_4b03d_row26_col32, #T_4b03d_row26_col33, #T_4b03d_row26_col34, #T_4b03d_row26_col35, #T_4b03d_row26_col36, #T_4b03d_row26_col37, #T_4b03d_row26_col38, #T_4b03d_row26_col39, #T_4b03d_row26_col40, #T_4b03d_row26_col41, #T_4b03d_row26_col42, #T_4b03d_row26_col43, #T_4b03d_row26_col44, #T_4b03d_row26_col45, #T_4b03d_row26_col46, #T_4b03d_row26_col47, #T_4b03d_row26_col48, #T_4b03d_row26_col49, #T_4b03d_row26_col50, #T_4b03d_row26_col51, #T_4b03d_row26_col52, #T_4b03d_row26_col53, #T_4b03d_row26_col54, #T_4b03d_row26_col55, #T_4b03d_row26_col56, #T_4b03d_row26_col57, #T_4b03d_row26_col58, #T_4b03d_row26_col59, #T_4b03d_row26_col60, #T_4b03d_row26_col61, #T_4b03d_row26_col62, #T_4b03d_row26_col63, #T_4b03d_row26_col64, #T_4b03d_row26_col65, #T_4b03d_row26_col66, #T_4b03d_row26_col67, #T_4b03d_row26_col68, #T_4b03d_row26_col69, #T_4b03d_row26_col70, #T_4b03d_row26_col71, #T_4b03d_row26_col72, #T_4b03d_row26_col73, #T_4b03d_row26_col74, #T_4b03d_row26_col75, #T_4b03d_row26_col76, #T_4b03d_row26_col77, #T_4b03d_row26_col78, #T_4b03d_row26_col79, #T_4b03d_row26_col80, #T_4b03d_row26_col81, #T_4b03d_row26_col82, #T_4b03d_row26_col83, #T_4b03d_row26_col84, #T_4b03d_row26_col85, #T_4b03d_row26_col86, #T_4b03d_row26_col87, #T_4b03d_row26_col88, #T_4b03d_row26_col89, #T_4b03d_row26_col90, #T_4b03d_row26_col91, #T_4b03d_row26_col92, #T_4b03d_row26_col93, #T_4b03d_row26_col94, #T_4b03d_row26_col95, #T_4b03d_row26_col96, #T_4b03d_row26_col97, #T_4b03d_row26_col98, #T_4b03d_row26_col99, #T_4b03d_row26_col100, #T_4b03d_row26_col101, #T_4b03d_row27_col0, #T_4b03d_row27_col1, #T_4b03d_row27_col2, #T_4b03d_row27_col3, #T_4b03d_row27_col4, #T_4b03d_row27_col5, #T_4b03d_row27_col6, #T_4b03d_row27_col7, #T_4b03d_row27_col8, #T_4b03d_row27_col9, #T_4b03d_row27_col10, #T_4b03d_row27_col11, #T_4b03d_row27_col12, #T_4b03d_row27_col13, #T_4b03d_row27_col14, #T_4b03d_row27_col15, #T_4b03d_row27_col16, #T_4b03d_row27_col17, #T_4b03d_row27_col18, #T_4b03d_row27_col19, #T_4b03d_row27_col20, #T_4b03d_row27_col21, #T_4b03d_row27_col22, #T_4b03d_row27_col23, #T_4b03d_row27_col24, #T_4b03d_row27_col25, #T_4b03d_row27_col26, #T_4b03d_row27_col27, #T_4b03d_row27_col28, #T_4b03d_row27_col29, #T_4b03d_row27_col30, #T_4b03d_row27_col31, #T_4b03d_row27_col32, #T_4b03d_row27_col33, #T_4b03d_row27_col34, #T_4b03d_row27_col35, #T_4b03d_row27_col36, #T_4b03d_row27_col37, #T_4b03d_row27_col38, #T_4b03d_row27_col39, #T_4b03d_row27_col40, #T_4b03d_row27_col41, #T_4b03d_row27_col42, #T_4b03d_row27_col43, #T_4b03d_row27_col44, #T_4b03d_row27_col45, #T_4b03d_row27_col46, #T_4b03d_row27_col47, #T_4b03d_row27_col48, #T_4b03d_row27_col49, #T_4b03d_row27_col50, #T_4b03d_row27_col51, #T_4b03d_row27_col52, #T_4b03d_row27_col53, #T_4b03d_row27_col54, #T_4b03d_row27_col55, #T_4b03d_row27_col56, #T_4b03d_row27_col57, #T_4b03d_row27_col58, #T_4b03d_row27_col59, #T_4b03d_row27_col60, #T_4b03d_row27_col61, #T_4b03d_row27_col62, #T_4b03d_row27_col63, #T_4b03d_row27_col64, #T_4b03d_row27_col65, #T_4b03d_row27_col66, #T_4b03d_row27_col67, #T_4b03d_row27_col68, #T_4b03d_row27_col69, #T_4b03d_row27_col70, #T_4b03d_row27_col71, #T_4b03d_row27_col72, #T_4b03d_row27_col73, #T_4b03d_row27_col74, #T_4b03d_row27_col75, #T_4b03d_row27_col76, #T_4b03d_row27_col77, #T_4b03d_row27_col78, #T_4b03d_row27_col79, #T_4b03d_row27_col80, #T_4b03d_row27_col81, #T_4b03d_row27_col82, #T_4b03d_row27_col83, #T_4b03d_row27_col84, #T_4b03d_row27_col85, #T_4b03d_row27_col86, #T_4b03d_row27_col87, #T_4b03d_row27_col88, #T_4b03d_row27_col89, #T_4b03d_row27_col90, #T_4b03d_row27_col91, #T_4b03d_row27_col92, #T_4b03d_row27_col93, #T_4b03d_row27_col94, #T_4b03d_row27_col95, #T_4b03d_row27_col96, #T_4b03d_row27_col97, #T_4b03d_row27_col98, #T_4b03d_row27_col99, #T_4b03d_row27_col100, #T_4b03d_row27_col101, #T_4b03d_row28_col0, #T_4b03d_row28_col1, #T_4b03d_row28_col2, #T_4b03d_row28_col3, #T_4b03d_row28_col4, #T_4b03d_row28_col5, #T_4b03d_row28_col6, #T_4b03d_row28_col7, #T_4b03d_row28_col8, #T_4b03d_row28_col9, #T_4b03d_row28_col10, #T_4b03d_row28_col11, #T_4b03d_row28_col12, #T_4b03d_row28_col13, #T_4b03d_row28_col14, #T_4b03d_row28_col15, #T_4b03d_row28_col16, #T_4b03d_row28_col17, #T_4b03d_row28_col18, #T_4b03d_row28_col19, #T_4b03d_row28_col20, #T_4b03d_row28_col21, #T_4b03d_row28_col22, #T_4b03d_row28_col23, #T_4b03d_row28_col24, #T_4b03d_row28_col25, #T_4b03d_row28_col26, #T_4b03d_row28_col27, #T_4b03d_row28_col28, #T_4b03d_row28_col29, #T_4b03d_row28_col30, #T_4b03d_row28_col31, #T_4b03d_row28_col32, #T_4b03d_row28_col33, #T_4b03d_row28_col34, #T_4b03d_row28_col35, #T_4b03d_row28_col36, #T_4b03d_row28_col37, #T_4b03d_row28_col38, #T_4b03d_row28_col39, #T_4b03d_row28_col40, #T_4b03d_row28_col41, #T_4b03d_row28_col42, #T_4b03d_row28_col43, #T_4b03d_row28_col44, #T_4b03d_row28_col45, #T_4b03d_row28_col46, #T_4b03d_row28_col47, #T_4b03d_row28_col48, #T_4b03d_row28_col49, #T_4b03d_row28_col50, #T_4b03d_row28_col51, #T_4b03d_row28_col52, #T_4b03d_row28_col53, #T_4b03d_row28_col54, #T_4b03d_row28_col55, #T_4b03d_row28_col56, #T_4b03d_row28_col57, #T_4b03d_row28_col58, #T_4b03d_row28_col59, #T_4b03d_row28_col60, #T_4b03d_row28_col61, #T_4b03d_row28_col62, #T_4b03d_row28_col63, #T_4b03d_row28_col64, #T_4b03d_row28_col65, #T_4b03d_row28_col66, #T_4b03d_row28_col67, #T_4b03d_row28_col68, #T_4b03d_row28_col69, #T_4b03d_row28_col70, #T_4b03d_row28_col71, #T_4b03d_row28_col72, #T_4b03d_row28_col73, #T_4b03d_row28_col74, #T_4b03d_row28_col75, #T_4b03d_row28_col76, #T_4b03d_row28_col77, #T_4b03d_row28_col78, #T_4b03d_row28_col79, #T_4b03d_row28_col80, #T_4b03d_row28_col81, #T_4b03d_row28_col82, #T_4b03d_row28_col83, #T_4b03d_row28_col84, #T_4b03d_row28_col85, #T_4b03d_row28_col86, #T_4b03d_row28_col87, #T_4b03d_row28_col88, #T_4b03d_row28_col89, #T_4b03d_row28_col90, #T_4b03d_row28_col91, #T_4b03d_row28_col92, #T_4b03d_row28_col93, #T_4b03d_row28_col94, #T_4b03d_row28_col95, #T_4b03d_row28_col96, #T_4b03d_row28_col97, #T_4b03d_row28_col98, #T_4b03d_row28_col99, #T_4b03d_row28_col100, #T_4b03d_row28_col101, #T_4b03d_row29_col0, #T_4b03d_row29_col1, #T_4b03d_row29_col2, #T_4b03d_row29_col3, #T_4b03d_row29_col4, #T_4b03d_row29_col5, #T_4b03d_row29_col6, #T_4b03d_row29_col7, #T_4b03d_row29_col8, #T_4b03d_row29_col9, #T_4b03d_row29_col10, #T_4b03d_row29_col11, #T_4b03d_row29_col12, #T_4b03d_row29_col13, #T_4b03d_row29_col14, #T_4b03d_row29_col15, #T_4b03d_row29_col16, #T_4b03d_row29_col17, #T_4b03d_row29_col18, #T_4b03d_row29_col19, #T_4b03d_row29_col20, #T_4b03d_row29_col21, #T_4b03d_row29_col22, #T_4b03d_row29_col23, #T_4b03d_row29_col24, #T_4b03d_row29_col25, #T_4b03d_row29_col26, #T_4b03d_row29_col27, #T_4b03d_row29_col28, #T_4b03d_row29_col29, #T_4b03d_row29_col30, #T_4b03d_row29_col31, #T_4b03d_row29_col32, #T_4b03d_row29_col33, #T_4b03d_row29_col34, #T_4b03d_row29_col35, #T_4b03d_row29_col36, #T_4b03d_row29_col37, #T_4b03d_row29_col38, #T_4b03d_row29_col39, #T_4b03d_row29_col40, #T_4b03d_row29_col41, #T_4b03d_row29_col42, #T_4b03d_row29_col43, #T_4b03d_row29_col44, #T_4b03d_row29_col45, #T_4b03d_row29_col46, #T_4b03d_row29_col47, #T_4b03d_row29_col48, #T_4b03d_row29_col49, #T_4b03d_row29_col50, #T_4b03d_row29_col51, #T_4b03d_row29_col52, #T_4b03d_row29_col53, #T_4b03d_row29_col54, #T_4b03d_row29_col55, #T_4b03d_row29_col56, #T_4b03d_row29_col57, #T_4b03d_row29_col58, #T_4b03d_row29_col59, #T_4b03d_row29_col60, #T_4b03d_row29_col61, #T_4b03d_row29_col62, #T_4b03d_row29_col63, #T_4b03d_row29_col64, #T_4b03d_row29_col65, #T_4b03d_row29_col66, #T_4b03d_row29_col67, #T_4b03d_row29_col68, #T_4b03d_row29_col69, #T_4b03d_row29_col70, #T_4b03d_row29_col71, #T_4b03d_row29_col72, #T_4b03d_row29_col73, #T_4b03d_row29_col74, #T_4b03d_row29_col75, #T_4b03d_row29_col76, #T_4b03d_row29_col77, #T_4b03d_row29_col78, #T_4b03d_row29_col79, #T_4b03d_row29_col80, #T_4b03d_row29_col81, #T_4b03d_row29_col82, #T_4b03d_row29_col83, #T_4b03d_row29_col84, #T_4b03d_row29_col85, #T_4b03d_row29_col86, #T_4b03d_row29_col87, #T_4b03d_row29_col88, #T_4b03d_row29_col89, #T_4b03d_row29_col90, #T_4b03d_row29_col91, #T_4b03d_row29_col92, #T_4b03d_row29_col93, #T_4b03d_row29_col94, #T_4b03d_row29_col95, #T_4b03d_row29_col96, #T_4b03d_row29_col97, #T_4b03d_row29_col98, #T_4b03d_row29_col99, #T_4b03d_row29_col100, #T_4b03d_row29_col101, #T_4b03d_row30_col0, #T_4b03d_row30_col1, #T_4b03d_row30_col2, #T_4b03d_row30_col3, #T_4b03d_row30_col4, #T_4b03d_row30_col5, #T_4b03d_row30_col6, #T_4b03d_row30_col7, #T_4b03d_row30_col8, #T_4b03d_row30_col9, #T_4b03d_row30_col10, #T_4b03d_row30_col11, #T_4b03d_row30_col12, #T_4b03d_row30_col13, #T_4b03d_row30_col14, #T_4b03d_row30_col15, #T_4b03d_row30_col16, #T_4b03d_row30_col17, #T_4b03d_row30_col18, #T_4b03d_row30_col19, #T_4b03d_row30_col20, #T_4b03d_row30_col21, #T_4b03d_row30_col22, #T_4b03d_row30_col23, #T_4b03d_row30_col24, #T_4b03d_row30_col25, #T_4b03d_row30_col26, #T_4b03d_row30_col27, #T_4b03d_row30_col28, #T_4b03d_row30_col29, #T_4b03d_row30_col30, #T_4b03d_row30_col31, #T_4b03d_row30_col32, #T_4b03d_row30_col33, #T_4b03d_row30_col34, #T_4b03d_row30_col35, #T_4b03d_row30_col36, #T_4b03d_row30_col37, #T_4b03d_row30_col38, #T_4b03d_row30_col39, #T_4b03d_row30_col40, #T_4b03d_row30_col41, #T_4b03d_row30_col42, #T_4b03d_row30_col43, #T_4b03d_row30_col44, #T_4b03d_row30_col45, #T_4b03d_row30_col46, #T_4b03d_row30_col47, #T_4b03d_row30_col48, #T_4b03d_row30_col49, #T_4b03d_row30_col50, #T_4b03d_row30_col51, #T_4b03d_row30_col52, #T_4b03d_row30_col53, #T_4b03d_row30_col54, #T_4b03d_row30_col55, #T_4b03d_row30_col56, #T_4b03d_row30_col57, #T_4b03d_row30_col58, #T_4b03d_row30_col59, #T_4b03d_row30_col60, #T_4b03d_row30_col61, #T_4b03d_row30_col62, #T_4b03d_row30_col63, #T_4b03d_row30_col64, #T_4b03d_row30_col65, #T_4b03d_row30_col66, #T_4b03d_row30_col67, #T_4b03d_row30_col68, #T_4b03d_row30_col69, #T_4b03d_row30_col70, #T_4b03d_row30_col71, #T_4b03d_row30_col72, #T_4b03d_row30_col73, #T_4b03d_row30_col74, #T_4b03d_row30_col75, #T_4b03d_row30_col76, #T_4b03d_row30_col77, #T_4b03d_row30_col78, #T_4b03d_row30_col79, #T_4b03d_row30_col80, #T_4b03d_row30_col81, #T_4b03d_row30_col82, #T_4b03d_row30_col83, #T_4b03d_row30_col84, #T_4b03d_row30_col85, #T_4b03d_row30_col86, #T_4b03d_row30_col87, #T_4b03d_row30_col88, #T_4b03d_row30_col89, #T_4b03d_row30_col90, #T_4b03d_row30_col91, #T_4b03d_row30_col92, #T_4b03d_row30_col93, #T_4b03d_row30_col94, #T_4b03d_row30_col95, #T_4b03d_row30_col96, #T_4b03d_row30_col97, #T_4b03d_row30_col98, #T_4b03d_row30_col99, #T_4b03d_row30_col100, #T_4b03d_row30_col101, #T_4b03d_row31_col0, #T_4b03d_row31_col1, #T_4b03d_row31_col2, #T_4b03d_row31_col3, #T_4b03d_row31_col4, #T_4b03d_row31_col5, #T_4b03d_row31_col6, #T_4b03d_row31_col7, #T_4b03d_row31_col8, #T_4b03d_row31_col9, #T_4b03d_row31_col10, #T_4b03d_row31_col11, #T_4b03d_row31_col12, #T_4b03d_row31_col13, #T_4b03d_row31_col14, #T_4b03d_row31_col15, #T_4b03d_row31_col16, #T_4b03d_row31_col17, #T_4b03d_row31_col18, #T_4b03d_row31_col19, #T_4b03d_row31_col20, #T_4b03d_row31_col21, #T_4b03d_row31_col22, #T_4b03d_row31_col23, #T_4b03d_row31_col24, #T_4b03d_row31_col25, #T_4b03d_row31_col26, #T_4b03d_row31_col27, #T_4b03d_row31_col28, #T_4b03d_row31_col29, #T_4b03d_row31_col30, #T_4b03d_row31_col31, #T_4b03d_row31_col32, #T_4b03d_row31_col33, #T_4b03d_row31_col34, #T_4b03d_row31_col35, #T_4b03d_row31_col36, #T_4b03d_row31_col37, #T_4b03d_row31_col38, #T_4b03d_row31_col39, #T_4b03d_row31_col40, #T_4b03d_row31_col41, #T_4b03d_row31_col42, #T_4b03d_row31_col43, #T_4b03d_row31_col44, #T_4b03d_row31_col45, #T_4b03d_row31_col46, #T_4b03d_row31_col47, #T_4b03d_row31_col48, #T_4b03d_row31_col49, #T_4b03d_row31_col50, #T_4b03d_row31_col51, #T_4b03d_row31_col52, #T_4b03d_row31_col53, #T_4b03d_row31_col54, #T_4b03d_row31_col55, #T_4b03d_row31_col56, #T_4b03d_row31_col57, #T_4b03d_row31_col58, #T_4b03d_row31_col59, #T_4b03d_row31_col60, #T_4b03d_row31_col61, #T_4b03d_row31_col62, #T_4b03d_row31_col63, #T_4b03d_row31_col64, #T_4b03d_row31_col65, #T_4b03d_row31_col66, #T_4b03d_row31_col67, #T_4b03d_row31_col68, #T_4b03d_row31_col69, #T_4b03d_row31_col70, #T_4b03d_row31_col71, #T_4b03d_row31_col72, #T_4b03d_row31_col73, #T_4b03d_row31_col74, #T_4b03d_row31_col75, #T_4b03d_row31_col76, #T_4b03d_row31_col77, #T_4b03d_row31_col78, #T_4b03d_row31_col79, #T_4b03d_row31_col80, #T_4b03d_row31_col81, #T_4b03d_row31_col82, #T_4b03d_row31_col83, #T_4b03d_row31_col84, #T_4b03d_row31_col85, #T_4b03d_row31_col86, #T_4b03d_row31_col87, #T_4b03d_row31_col88, #T_4b03d_row31_col89, #T_4b03d_row31_col90, #T_4b03d_row31_col91, #T_4b03d_row31_col92, #T_4b03d_row31_col93, #T_4b03d_row31_col94, #T_4b03d_row31_col95, #T_4b03d_row31_col96, #T_4b03d_row31_col97, #T_4b03d_row31_col98, #T_4b03d_row31_col99, #T_4b03d_row31_col100, #T_4b03d_row31_col101, #T_4b03d_row32_col0, #T_4b03d_row32_col1, #T_4b03d_row32_col2, #T_4b03d_row32_col3, #T_4b03d_row32_col4, #T_4b03d_row32_col5, #T_4b03d_row32_col6, #T_4b03d_row32_col7, #T_4b03d_row32_col8, #T_4b03d_row32_col9, #T_4b03d_row32_col10, #T_4b03d_row32_col11, #T_4b03d_row32_col12, #T_4b03d_row32_col13, #T_4b03d_row32_col14, #T_4b03d_row32_col15, #T_4b03d_row32_col16, #T_4b03d_row32_col17, #T_4b03d_row32_col18, #T_4b03d_row32_col19, #T_4b03d_row32_col20, #T_4b03d_row32_col21, #T_4b03d_row32_col22, #T_4b03d_row32_col23, #T_4b03d_row32_col24, #T_4b03d_row32_col25, #T_4b03d_row32_col26, #T_4b03d_row32_col27, #T_4b03d_row32_col28, #T_4b03d_row32_col29, #T_4b03d_row32_col30, #T_4b03d_row32_col31, #T_4b03d_row32_col32, #T_4b03d_row32_col33, #T_4b03d_row32_col34, #T_4b03d_row32_col35, #T_4b03d_row32_col36, #T_4b03d_row32_col37, #T_4b03d_row32_col38, #T_4b03d_row32_col39, #T_4b03d_row32_col40, #T_4b03d_row32_col41, #T_4b03d_row32_col42, #T_4b03d_row32_col43, #T_4b03d_row32_col44, #T_4b03d_row32_col45, #T_4b03d_row32_col46, #T_4b03d_row32_col47, #T_4b03d_row32_col48, #T_4b03d_row32_col49, #T_4b03d_row32_col50, #T_4b03d_row32_col51, #T_4b03d_row32_col52, #T_4b03d_row32_col53, #T_4b03d_row32_col54, #T_4b03d_row32_col55, #T_4b03d_row32_col56, #T_4b03d_row32_col57, #T_4b03d_row32_col58, #T_4b03d_row32_col59, #T_4b03d_row32_col60, #T_4b03d_row32_col61, #T_4b03d_row32_col62, #T_4b03d_row32_col63, #T_4b03d_row32_col64, #T_4b03d_row32_col65, #T_4b03d_row32_col66, #T_4b03d_row32_col67, #T_4b03d_row32_col68, #T_4b03d_row32_col69, #T_4b03d_row32_col70, #T_4b03d_row32_col71, #T_4b03d_row32_col72, #T_4b03d_row32_col73, #T_4b03d_row32_col74, #T_4b03d_row32_col75, #T_4b03d_row32_col76, #T_4b03d_row32_col77, #T_4b03d_row32_col78, #T_4b03d_row32_col79, #T_4b03d_row32_col80, #T_4b03d_row32_col81, #T_4b03d_row32_col82, #T_4b03d_row32_col83, #T_4b03d_row32_col84, #T_4b03d_row32_col85, #T_4b03d_row32_col86, #T_4b03d_row32_col87, #T_4b03d_row32_col88, #T_4b03d_row32_col89, #T_4b03d_row32_col90, #T_4b03d_row32_col91, #T_4b03d_row32_col92, #T_4b03d_row32_col93, #T_4b03d_row32_col94, #T_4b03d_row32_col95, #T_4b03d_row32_col96, #T_4b03d_row32_col97, #T_4b03d_row32_col98, #T_4b03d_row32_col99, #T_4b03d_row32_col100, #T_4b03d_row32_col101, #T_4b03d_row33_col0, #T_4b03d_row33_col1, #T_4b03d_row33_col2, #T_4b03d_row33_col3, #T_4b03d_row33_col4, #T_4b03d_row33_col5, #T_4b03d_row33_col6, #T_4b03d_row33_col7, #T_4b03d_row33_col8, #T_4b03d_row33_col9, #T_4b03d_row33_col10, #T_4b03d_row33_col11, #T_4b03d_row33_col12, #T_4b03d_row33_col13, #T_4b03d_row33_col14, #T_4b03d_row33_col15, #T_4b03d_row33_col16, #T_4b03d_row33_col17, #T_4b03d_row33_col18, #T_4b03d_row33_col19, #T_4b03d_row33_col20, #T_4b03d_row33_col21, #T_4b03d_row33_col22, #T_4b03d_row33_col23, #T_4b03d_row33_col24, #T_4b03d_row33_col25, #T_4b03d_row33_col26, #T_4b03d_row33_col27, #T_4b03d_row33_col28, #T_4b03d_row33_col29, #T_4b03d_row33_col30, #T_4b03d_row33_col31, #T_4b03d_row33_col32, #T_4b03d_row33_col33, #T_4b03d_row33_col34, #T_4b03d_row33_col35, #T_4b03d_row33_col36, #T_4b03d_row33_col37, #T_4b03d_row33_col38, #T_4b03d_row33_col39, #T_4b03d_row33_col40, #T_4b03d_row33_col41, #T_4b03d_row33_col42, #T_4b03d_row33_col43, #T_4b03d_row33_col44, #T_4b03d_row33_col45, #T_4b03d_row33_col46, #T_4b03d_row33_col47, #T_4b03d_row33_col48, #T_4b03d_row33_col49, #T_4b03d_row33_col50, #T_4b03d_row33_col51, #T_4b03d_row33_col52, #T_4b03d_row33_col53, #T_4b03d_row33_col54, #T_4b03d_row33_col55, #T_4b03d_row33_col56, #T_4b03d_row33_col57, #T_4b03d_row33_col58, #T_4b03d_row33_col59, #T_4b03d_row33_col60, #T_4b03d_row33_col61, #T_4b03d_row33_col62, #T_4b03d_row33_col63, #T_4b03d_row33_col64, #T_4b03d_row33_col65, #T_4b03d_row33_col66, #T_4b03d_row33_col67, #T_4b03d_row33_col68, #T_4b03d_row33_col69, #T_4b03d_row33_col70, #T_4b03d_row33_col71, #T_4b03d_row33_col72, #T_4b03d_row33_col73, #T_4b03d_row33_col74, #T_4b03d_row33_col75, #T_4b03d_row33_col76, #T_4b03d_row33_col77, #T_4b03d_row33_col78, #T_4b03d_row33_col79, #T_4b03d_row33_col80, #T_4b03d_row33_col81, #T_4b03d_row33_col82, #T_4b03d_row33_col83, #T_4b03d_row33_col84, #T_4b03d_row33_col85, #T_4b03d_row33_col86, #T_4b03d_row33_col87, #T_4b03d_row33_col88, #T_4b03d_row33_col89, #T_4b03d_row33_col90, #T_4b03d_row33_col91, #T_4b03d_row33_col92, #T_4b03d_row33_col93, #T_4b03d_row33_col94, #T_4b03d_row33_col95, #T_4b03d_row33_col96, #T_4b03d_row33_col97, #T_4b03d_row33_col98, #T_4b03d_row33_col99, #T_4b03d_row33_col100, #T_4b03d_row33_col101, #T_4b03d_row34_col0, #T_4b03d_row34_col1, #T_4b03d_row34_col2, #T_4b03d_row34_col3, #T_4b03d_row34_col4, #T_4b03d_row34_col5, #T_4b03d_row34_col6, #T_4b03d_row34_col7, #T_4b03d_row34_col8, #T_4b03d_row34_col9, #T_4b03d_row34_col10, #T_4b03d_row34_col11, #T_4b03d_row34_col12, #T_4b03d_row34_col13, #T_4b03d_row34_col14, #T_4b03d_row34_col15, #T_4b03d_row34_col16, #T_4b03d_row34_col17, #T_4b03d_row34_col18, #T_4b03d_row34_col19, #T_4b03d_row34_col20, #T_4b03d_row34_col21, #T_4b03d_row34_col22, #T_4b03d_row34_col23, #T_4b03d_row34_col24, #T_4b03d_row34_col25, #T_4b03d_row34_col26, #T_4b03d_row34_col27, #T_4b03d_row34_col28, #T_4b03d_row34_col29, #T_4b03d_row34_col30, #T_4b03d_row34_col31, #T_4b03d_row34_col32, #T_4b03d_row34_col33, #T_4b03d_row34_col34, #T_4b03d_row34_col35, #T_4b03d_row34_col36, #T_4b03d_row34_col37, #T_4b03d_row34_col38, #T_4b03d_row34_col39, #T_4b03d_row34_col40, #T_4b03d_row34_col41, #T_4b03d_row34_col42, #T_4b03d_row34_col43, #T_4b03d_row34_col44, #T_4b03d_row34_col45, #T_4b03d_row34_col46, #T_4b03d_row34_col47, #T_4b03d_row34_col48, #T_4b03d_row34_col49, #T_4b03d_row34_col50, #T_4b03d_row34_col51, #T_4b03d_row34_col52, #T_4b03d_row34_col53, #T_4b03d_row34_col54, #T_4b03d_row34_col55, #T_4b03d_row34_col56, #T_4b03d_row34_col57, #T_4b03d_row34_col58, #T_4b03d_row34_col59, #T_4b03d_row34_col60, #T_4b03d_row34_col61, #T_4b03d_row34_col62, #T_4b03d_row34_col63, #T_4b03d_row34_col64, #T_4b03d_row34_col65, #T_4b03d_row34_col66, #T_4b03d_row34_col67, #T_4b03d_row34_col68, #T_4b03d_row34_col69, #T_4b03d_row34_col70, #T_4b03d_row34_col71, #T_4b03d_row34_col72, #T_4b03d_row34_col73, #T_4b03d_row34_col74, #T_4b03d_row34_col75, #T_4b03d_row34_col76, #T_4b03d_row34_col77, #T_4b03d_row34_col78, #T_4b03d_row34_col79, #T_4b03d_row34_col80, #T_4b03d_row34_col81, #T_4b03d_row34_col82, #T_4b03d_row34_col83, #T_4b03d_row34_col84, #T_4b03d_row34_col85, #T_4b03d_row34_col86, #T_4b03d_row34_col87, #T_4b03d_row34_col88, #T_4b03d_row34_col89, #T_4b03d_row34_col90, #T_4b03d_row34_col91, #T_4b03d_row34_col92, #T_4b03d_row34_col93, #T_4b03d_row34_col94, #T_4b03d_row34_col95, #T_4b03d_row34_col96, #T_4b03d_row34_col97, #T_4b03d_row34_col98, #T_4b03d_row34_col99, #T_4b03d_row34_col100, #T_4b03d_row34_col101, #T_4b03d_row35_col0, #T_4b03d_row35_col1, #T_4b03d_row35_col2, #T_4b03d_row35_col3, #T_4b03d_row35_col4, #T_4b03d_row35_col5, #T_4b03d_row35_col6, #T_4b03d_row35_col7, #T_4b03d_row35_col8, #T_4b03d_row35_col9, #T_4b03d_row35_col10, #T_4b03d_row35_col11, #T_4b03d_row35_col12, #T_4b03d_row35_col13, #T_4b03d_row35_col14, #T_4b03d_row35_col15, #T_4b03d_row35_col16, #T_4b03d_row35_col17, #T_4b03d_row35_col18, #T_4b03d_row35_col19, #T_4b03d_row35_col20, #T_4b03d_row35_col21, #T_4b03d_row35_col22, #T_4b03d_row35_col23, #T_4b03d_row35_col24, #T_4b03d_row35_col25, #T_4b03d_row35_col26, #T_4b03d_row35_col27, #T_4b03d_row35_col28, #T_4b03d_row35_col29, #T_4b03d_row35_col30, #T_4b03d_row35_col31, #T_4b03d_row35_col32, #T_4b03d_row35_col33, #T_4b03d_row35_col34, #T_4b03d_row35_col35, #T_4b03d_row35_col36, #T_4b03d_row35_col37, #T_4b03d_row35_col38, #T_4b03d_row35_col39, #T_4b03d_row35_col40, #T_4b03d_row35_col41, #T_4b03d_row35_col42, #T_4b03d_row35_col43, #T_4b03d_row35_col44, #T_4b03d_row35_col45, #T_4b03d_row35_col46, #T_4b03d_row35_col47, #T_4b03d_row35_col48, #T_4b03d_row35_col49, #T_4b03d_row35_col50, #T_4b03d_row35_col51, #T_4b03d_row35_col52, #T_4b03d_row35_col53, #T_4b03d_row35_col54, #T_4b03d_row35_col55, #T_4b03d_row35_col56, #T_4b03d_row35_col57, #T_4b03d_row35_col58, #T_4b03d_row35_col59, #T_4b03d_row35_col60, #T_4b03d_row35_col61, #T_4b03d_row35_col62, #T_4b03d_row35_col63, #T_4b03d_row35_col64, #T_4b03d_row35_col65, #T_4b03d_row35_col66, #T_4b03d_row35_col67, #T_4b03d_row35_col68, #T_4b03d_row35_col69, #T_4b03d_row35_col70, #T_4b03d_row35_col71, #T_4b03d_row35_col72, #T_4b03d_row35_col73, #T_4b03d_row35_col74, #T_4b03d_row35_col75, #T_4b03d_row35_col76, #T_4b03d_row35_col77, #T_4b03d_row35_col78, #T_4b03d_row35_col79, #T_4b03d_row35_col80, #T_4b03d_row35_col81, #T_4b03d_row35_col82, #T_4b03d_row35_col83, #T_4b03d_row35_col84, #T_4b03d_row35_col85, #T_4b03d_row35_col86, #T_4b03d_row35_col87, #T_4b03d_row35_col88, #T_4b03d_row35_col89, #T_4b03d_row35_col90, #T_4b03d_row35_col91, #T_4b03d_row35_col92, #T_4b03d_row35_col93, #T_4b03d_row35_col94, #T_4b03d_row35_col95, #T_4b03d_row35_col96, #T_4b03d_row35_col97, #T_4b03d_row35_col98, #T_4b03d_row35_col99, #T_4b03d_row35_col100, #T_4b03d_row35_col101, #T_4b03d_row36_col0, #T_4b03d_row36_col1, #T_4b03d_row36_col2, #T_4b03d_row36_col3, #T_4b03d_row36_col4, #T_4b03d_row36_col5, #T_4b03d_row36_col6, #T_4b03d_row36_col7, #T_4b03d_row36_col8, #T_4b03d_row36_col9, #T_4b03d_row36_col10, #T_4b03d_row36_col11, #T_4b03d_row36_col12, #T_4b03d_row36_col13, #T_4b03d_row36_col14, #T_4b03d_row36_col15, #T_4b03d_row36_col16, #T_4b03d_row36_col17, #T_4b03d_row36_col18, #T_4b03d_row36_col19, #T_4b03d_row36_col20, #T_4b03d_row36_col21, #T_4b03d_row36_col22, #T_4b03d_row36_col23, #T_4b03d_row36_col24, #T_4b03d_row36_col25, #T_4b03d_row36_col26, #T_4b03d_row36_col27, #T_4b03d_row36_col28, #T_4b03d_row36_col29, #T_4b03d_row36_col30, #T_4b03d_row36_col31, #T_4b03d_row36_col32, #T_4b03d_row36_col33, #T_4b03d_row36_col34, #T_4b03d_row36_col35, #T_4b03d_row36_col36, #T_4b03d_row36_col37, #T_4b03d_row36_col38, #T_4b03d_row36_col39, #T_4b03d_row36_col40, #T_4b03d_row36_col41, #T_4b03d_row36_col42, #T_4b03d_row36_col43, #T_4b03d_row36_col44, #T_4b03d_row36_col45, #T_4b03d_row36_col46, #T_4b03d_row36_col47, #T_4b03d_row36_col48, #T_4b03d_row36_col49, #T_4b03d_row36_col50, #T_4b03d_row36_col51, #T_4b03d_row36_col52, #T_4b03d_row36_col53, #T_4b03d_row36_col54, #T_4b03d_row36_col55, #T_4b03d_row36_col56, #T_4b03d_row36_col57, #T_4b03d_row36_col58, #T_4b03d_row36_col59, #T_4b03d_row36_col60, #T_4b03d_row36_col61, #T_4b03d_row36_col62, #T_4b03d_row36_col63, #T_4b03d_row36_col64, #T_4b03d_row36_col65, #T_4b03d_row36_col66, #T_4b03d_row36_col67, #T_4b03d_row36_col68, #T_4b03d_row36_col69, #T_4b03d_row36_col70, #T_4b03d_row36_col71, #T_4b03d_row36_col72, #T_4b03d_row36_col73, #T_4b03d_row36_col74, #T_4b03d_row36_col75, #T_4b03d_row36_col76, #T_4b03d_row36_col77, #T_4b03d_row36_col78, #T_4b03d_row36_col79, #T_4b03d_row36_col80, #T_4b03d_row36_col81, #T_4b03d_row36_col82, #T_4b03d_row36_col83, #T_4b03d_row36_col84, #T_4b03d_row36_col85, #T_4b03d_row36_col86, #T_4b03d_row36_col87, #T_4b03d_row36_col88, #T_4b03d_row36_col89, #T_4b03d_row36_col90, #T_4b03d_row36_col91, #T_4b03d_row36_col92, #T_4b03d_row36_col93, #T_4b03d_row36_col94, #T_4b03d_row36_col95, #T_4b03d_row36_col96, #T_4b03d_row36_col97, #T_4b03d_row36_col98, #T_4b03d_row36_col99, #T_4b03d_row36_col100, #T_4b03d_row36_col101, #T_4b03d_row37_col0, #T_4b03d_row37_col1, #T_4b03d_row37_col2, #T_4b03d_row37_col3, #T_4b03d_row37_col4, #T_4b03d_row37_col5, #T_4b03d_row37_col6, #T_4b03d_row37_col7, #T_4b03d_row37_col8, #T_4b03d_row37_col9, #T_4b03d_row37_col10, #T_4b03d_row37_col11, #T_4b03d_row37_col12, #T_4b03d_row37_col13, #T_4b03d_row37_col14, #T_4b03d_row37_col15, #T_4b03d_row37_col16, #T_4b03d_row37_col17, #T_4b03d_row37_col18, #T_4b03d_row37_col19, #T_4b03d_row37_col20, #T_4b03d_row37_col21, #T_4b03d_row37_col22, #T_4b03d_row37_col23, #T_4b03d_row37_col24, #T_4b03d_row37_col25, #T_4b03d_row37_col26, #T_4b03d_row37_col27, #T_4b03d_row37_col28, #T_4b03d_row37_col29, #T_4b03d_row37_col30, #T_4b03d_row37_col31, #T_4b03d_row37_col32, #T_4b03d_row37_col33, #T_4b03d_row37_col34, #T_4b03d_row37_col35, #T_4b03d_row37_col36, #T_4b03d_row37_col37, #T_4b03d_row37_col38, #T_4b03d_row37_col39, #T_4b03d_row37_col40, #T_4b03d_row37_col41, #T_4b03d_row37_col42, #T_4b03d_row37_col43, #T_4b03d_row37_col44, #T_4b03d_row37_col45, #T_4b03d_row37_col46, #T_4b03d_row37_col47, #T_4b03d_row37_col48, #T_4b03d_row37_col49, #T_4b03d_row37_col50, #T_4b03d_row37_col51, #T_4b03d_row37_col52, #T_4b03d_row37_col53, #T_4b03d_row37_col54, #T_4b03d_row37_col55, #T_4b03d_row37_col56, #T_4b03d_row37_col57, #T_4b03d_row37_col58, #T_4b03d_row37_col59, #T_4b03d_row37_col60, #T_4b03d_row37_col61, #T_4b03d_row37_col62, #T_4b03d_row37_col63, #T_4b03d_row37_col64, #T_4b03d_row37_col65, #T_4b03d_row37_col66, #T_4b03d_row37_col67, #T_4b03d_row37_col68, #T_4b03d_row37_col69, #T_4b03d_row37_col70, #T_4b03d_row37_col71, #T_4b03d_row37_col72, #T_4b03d_row37_col73, #T_4b03d_row37_col74, #T_4b03d_row37_col75, #T_4b03d_row37_col76, #T_4b03d_row37_col77, #T_4b03d_row37_col78, #T_4b03d_row37_col79, #T_4b03d_row37_col80, #T_4b03d_row37_col81, #T_4b03d_row37_col82, #T_4b03d_row37_col83, #T_4b03d_row37_col84, #T_4b03d_row37_col85, #T_4b03d_row37_col86, #T_4b03d_row37_col87, #T_4b03d_row37_col88, #T_4b03d_row37_col89, #T_4b03d_row37_col90, #T_4b03d_row37_col91, #T_4b03d_row37_col92, #T_4b03d_row37_col93, #T_4b03d_row37_col94, #T_4b03d_row37_col95, #T_4b03d_row37_col96, #T_4b03d_row37_col97, #T_4b03d_row37_col98, #T_4b03d_row37_col99, #T_4b03d_row37_col100, #T_4b03d_row37_col101, #T_4b03d_row38_col0, #T_4b03d_row38_col1, #T_4b03d_row38_col2, #T_4b03d_row38_col3, #T_4b03d_row38_col4, #T_4b03d_row38_col5, #T_4b03d_row38_col6, #T_4b03d_row38_col7, #T_4b03d_row38_col8, #T_4b03d_row38_col9, #T_4b03d_row38_col10, #T_4b03d_row38_col11, #T_4b03d_row38_col12, #T_4b03d_row38_col13, #T_4b03d_row38_col14, #T_4b03d_row38_col15, #T_4b03d_row38_col16, #T_4b03d_row38_col17, #T_4b03d_row38_col18, #T_4b03d_row38_col19, #T_4b03d_row38_col20, #T_4b03d_row38_col21, #T_4b03d_row38_col22, #T_4b03d_row38_col23, #T_4b03d_row38_col24, #T_4b03d_row38_col25, #T_4b03d_row38_col26, #T_4b03d_row38_col27, #T_4b03d_row38_col28, #T_4b03d_row38_col29, #T_4b03d_row38_col30, #T_4b03d_row38_col31, #T_4b03d_row38_col32, #T_4b03d_row38_col33, #T_4b03d_row38_col34, #T_4b03d_row38_col35, #T_4b03d_row38_col36, #T_4b03d_row38_col37, #T_4b03d_row38_col38, #T_4b03d_row38_col39, #T_4b03d_row38_col40, #T_4b03d_row38_col41, #T_4b03d_row38_col42, #T_4b03d_row38_col43, #T_4b03d_row38_col44, #T_4b03d_row38_col45, #T_4b03d_row38_col46, #T_4b03d_row38_col47, #T_4b03d_row38_col48, #T_4b03d_row38_col49, #T_4b03d_row38_col50, #T_4b03d_row38_col51, #T_4b03d_row38_col52, #T_4b03d_row38_col53, #T_4b03d_row38_col54, #T_4b03d_row38_col55, #T_4b03d_row38_col56, #T_4b03d_row38_col57, #T_4b03d_row38_col58, #T_4b03d_row38_col59, #T_4b03d_row38_col60, #T_4b03d_row38_col61, #T_4b03d_row38_col62, #T_4b03d_row38_col63, #T_4b03d_row38_col64, #T_4b03d_row38_col65, #T_4b03d_row38_col66, #T_4b03d_row38_col67, #T_4b03d_row38_col68, #T_4b03d_row38_col69, #T_4b03d_row38_col70, #T_4b03d_row38_col71, #T_4b03d_row38_col72, #T_4b03d_row38_col73, #T_4b03d_row38_col74, #T_4b03d_row38_col75, #T_4b03d_row38_col76, #T_4b03d_row38_col77, #T_4b03d_row38_col78, #T_4b03d_row38_col79, #T_4b03d_row38_col80, #T_4b03d_row38_col81, #T_4b03d_row38_col82, #T_4b03d_row38_col83, #T_4b03d_row38_col84, #T_4b03d_row38_col85, #T_4b03d_row38_col86, #T_4b03d_row38_col87, #T_4b03d_row38_col88, #T_4b03d_row38_col89, #T_4b03d_row38_col90, #T_4b03d_row38_col91, #T_4b03d_row38_col92, #T_4b03d_row38_col93, #T_4b03d_row38_col94, #T_4b03d_row38_col95, #T_4b03d_row38_col96, #T_4b03d_row38_col97, #T_4b03d_row38_col98, #T_4b03d_row38_col99, #T_4b03d_row38_col100, #T_4b03d_row38_col101, #T_4b03d_row39_col0, #T_4b03d_row39_col1, #T_4b03d_row39_col2, #T_4b03d_row39_col3, #T_4b03d_row39_col4, #T_4b03d_row39_col5, #T_4b03d_row39_col6, #T_4b03d_row39_col7, #T_4b03d_row39_col8, #T_4b03d_row39_col9, #T_4b03d_row39_col10, #T_4b03d_row39_col11, #T_4b03d_row39_col12, #T_4b03d_row39_col13, #T_4b03d_row39_col14, #T_4b03d_row39_col15, #T_4b03d_row39_col16, #T_4b03d_row39_col17, #T_4b03d_row39_col18, #T_4b03d_row39_col19, #T_4b03d_row39_col20, #T_4b03d_row39_col21, #T_4b03d_row39_col22, #T_4b03d_row39_col23, #T_4b03d_row39_col24, #T_4b03d_row39_col25, #T_4b03d_row39_col26, #T_4b03d_row39_col27, #T_4b03d_row39_col28, #T_4b03d_row39_col29, #T_4b03d_row39_col30, #T_4b03d_row39_col31, #T_4b03d_row39_col32, #T_4b03d_row39_col33, #T_4b03d_row39_col34, #T_4b03d_row39_col35, #T_4b03d_row39_col36, #T_4b03d_row39_col37, #T_4b03d_row39_col38, #T_4b03d_row39_col39, #T_4b03d_row39_col40, #T_4b03d_row39_col41, #T_4b03d_row39_col42, #T_4b03d_row39_col43, #T_4b03d_row39_col44, #T_4b03d_row39_col45, #T_4b03d_row39_col46, #T_4b03d_row39_col47, #T_4b03d_row39_col48, #T_4b03d_row39_col49, #T_4b03d_row39_col50, #T_4b03d_row39_col51, #T_4b03d_row39_col52, #T_4b03d_row39_col53, #T_4b03d_row39_col54, #T_4b03d_row39_col55, #T_4b03d_row39_col56, #T_4b03d_row39_col57, #T_4b03d_row39_col58, #T_4b03d_row39_col59, #T_4b03d_row39_col60, #T_4b03d_row39_col61, #T_4b03d_row39_col62, #T_4b03d_row39_col63, #T_4b03d_row39_col64, #T_4b03d_row39_col65, #T_4b03d_row39_col66, #T_4b03d_row39_col67, #T_4b03d_row39_col68, #T_4b03d_row39_col69, #T_4b03d_row39_col70, #T_4b03d_row39_col71, #T_4b03d_row39_col72, #T_4b03d_row39_col73, #T_4b03d_row39_col74, #T_4b03d_row39_col75, #T_4b03d_row39_col76, #T_4b03d_row39_col77, #T_4b03d_row39_col78, #T_4b03d_row39_col79, #T_4b03d_row39_col80, #T_4b03d_row39_col81, #T_4b03d_row39_col82, #T_4b03d_row39_col83, #T_4b03d_row39_col84, #T_4b03d_row39_col85, #T_4b03d_row39_col86, #T_4b03d_row39_col87, #T_4b03d_row39_col88, #T_4b03d_row39_col89, #T_4b03d_row39_col90, #T_4b03d_row39_col91, #T_4b03d_row39_col92, #T_4b03d_row39_col93, #T_4b03d_row39_col94, #T_4b03d_row39_col95, #T_4b03d_row39_col96, #T_4b03d_row39_col97, #T_4b03d_row39_col98, #T_4b03d_row39_col99, #T_4b03d_row39_col100, #T_4b03d_row39_col101, #T_4b03d_row40_col0, #T_4b03d_row40_col1, #T_4b03d_row40_col2, #T_4b03d_row40_col3, #T_4b03d_row40_col4, #T_4b03d_row40_col5, #T_4b03d_row40_col6, #T_4b03d_row40_col7, #T_4b03d_row40_col8, #T_4b03d_row40_col9, #T_4b03d_row40_col10, #T_4b03d_row40_col11, #T_4b03d_row40_col12, #T_4b03d_row40_col13, #T_4b03d_row40_col14, #T_4b03d_row40_col15, #T_4b03d_row40_col16, #T_4b03d_row40_col17, #T_4b03d_row40_col18, #T_4b03d_row40_col19, #T_4b03d_row40_col20, #T_4b03d_row40_col21, #T_4b03d_row40_col22, #T_4b03d_row40_col23, #T_4b03d_row40_col24, #T_4b03d_row40_col25, #T_4b03d_row40_col26, #T_4b03d_row40_col27, #T_4b03d_row40_col28, #T_4b03d_row40_col29, #T_4b03d_row40_col30, #T_4b03d_row40_col31, #T_4b03d_row40_col32, #T_4b03d_row40_col33, #T_4b03d_row40_col34, #T_4b03d_row40_col35, #T_4b03d_row40_col36, #T_4b03d_row40_col37, #T_4b03d_row40_col38, #T_4b03d_row40_col39, #T_4b03d_row40_col40, #T_4b03d_row40_col41, #T_4b03d_row40_col42, #T_4b03d_row40_col43, #T_4b03d_row40_col44, #T_4b03d_row40_col45, #T_4b03d_row40_col46, #T_4b03d_row40_col47, #T_4b03d_row40_col48, #T_4b03d_row40_col49, #T_4b03d_row40_col50, #T_4b03d_row40_col51, #T_4b03d_row40_col52, #T_4b03d_row40_col53, #T_4b03d_row40_col54, #T_4b03d_row40_col55, #T_4b03d_row40_col56, #T_4b03d_row40_col57, #T_4b03d_row40_col58, #T_4b03d_row40_col59, #T_4b03d_row40_col60, #T_4b03d_row40_col61, #T_4b03d_row40_col62, #T_4b03d_row40_col63, #T_4b03d_row40_col64, #T_4b03d_row40_col65, #T_4b03d_row40_col66, #T_4b03d_row40_col67, #T_4b03d_row40_col68, #T_4b03d_row40_col69, #T_4b03d_row40_col70, #T_4b03d_row40_col71, #T_4b03d_row40_col72, #T_4b03d_row40_col73, #T_4b03d_row40_col74, #T_4b03d_row40_col75, #T_4b03d_row40_col76, #T_4b03d_row40_col77, #T_4b03d_row40_col78, #T_4b03d_row40_col79, #T_4b03d_row40_col80, #T_4b03d_row40_col81, #T_4b03d_row40_col82, #T_4b03d_row40_col83, #T_4b03d_row40_col84, #T_4b03d_row40_col85, #T_4b03d_row40_col86, #T_4b03d_row40_col87, #T_4b03d_row40_col88, #T_4b03d_row40_col89, #T_4b03d_row40_col90, #T_4b03d_row40_col91, #T_4b03d_row40_col92, #T_4b03d_row40_col93, #T_4b03d_row40_col94, #T_4b03d_row40_col95, #T_4b03d_row40_col96, #T_4b03d_row40_col97, #T_4b03d_row40_col98, #T_4b03d_row40_col99, #T_4b03d_row40_col100, #T_4b03d_row40_col101, #T_4b03d_row41_col0, #T_4b03d_row41_col1, #T_4b03d_row41_col2, #T_4b03d_row41_col3, #T_4b03d_row41_col4, #T_4b03d_row41_col5, #T_4b03d_row41_col6, #T_4b03d_row41_col7, #T_4b03d_row41_col8, #T_4b03d_row41_col9, #T_4b03d_row41_col10, #T_4b03d_row41_col11, #T_4b03d_row41_col12, #T_4b03d_row41_col13, #T_4b03d_row41_col14, #T_4b03d_row41_col15, #T_4b03d_row41_col16, #T_4b03d_row41_col17, #T_4b03d_row41_col18, #T_4b03d_row41_col19, #T_4b03d_row41_col20, #T_4b03d_row41_col21, #T_4b03d_row41_col22, #T_4b03d_row41_col23, #T_4b03d_row41_col24, #T_4b03d_row41_col25, #T_4b03d_row41_col26, #T_4b03d_row41_col27, #T_4b03d_row41_col28, #T_4b03d_row41_col29, #T_4b03d_row41_col30, #T_4b03d_row41_col31, #T_4b03d_row41_col32, #T_4b03d_row41_col33, #T_4b03d_row41_col34, #T_4b03d_row41_col35, #T_4b03d_row41_col36, #T_4b03d_row41_col37, #T_4b03d_row41_col38, #T_4b03d_row41_col39, #T_4b03d_row41_col40, #T_4b03d_row41_col41, #T_4b03d_row41_col42, #T_4b03d_row41_col43, #T_4b03d_row41_col44, #T_4b03d_row41_col45, #T_4b03d_row41_col46, #T_4b03d_row41_col47, #T_4b03d_row41_col48, #T_4b03d_row41_col49, #T_4b03d_row41_col50, #T_4b03d_row41_col51, #T_4b03d_row41_col52, #T_4b03d_row41_col53, #T_4b03d_row41_col54, #T_4b03d_row41_col55, #T_4b03d_row41_col56, #T_4b03d_row41_col57, #T_4b03d_row41_col58, #T_4b03d_row41_col59, #T_4b03d_row41_col60, #T_4b03d_row41_col61, #T_4b03d_row41_col62, #T_4b03d_row41_col63, #T_4b03d_row41_col64, #T_4b03d_row41_col65, #T_4b03d_row41_col66, #T_4b03d_row41_col67, #T_4b03d_row41_col68, #T_4b03d_row41_col69, #T_4b03d_row41_col70, #T_4b03d_row41_col71, #T_4b03d_row41_col72, #T_4b03d_row41_col73, #T_4b03d_row41_col74, #T_4b03d_row41_col75, #T_4b03d_row41_col76, #T_4b03d_row41_col77, #T_4b03d_row41_col78, #T_4b03d_row41_col79, #T_4b03d_row41_col80, #T_4b03d_row41_col81, #T_4b03d_row41_col82, #T_4b03d_row41_col83, #T_4b03d_row41_col84, #T_4b03d_row41_col85, #T_4b03d_row41_col86, #T_4b03d_row41_col87, #T_4b03d_row41_col88, #T_4b03d_row41_col89, #T_4b03d_row41_col90, #T_4b03d_row41_col91, #T_4b03d_row41_col92, #T_4b03d_row41_col93, #T_4b03d_row41_col94, #T_4b03d_row41_col95, #T_4b03d_row41_col96, #T_4b03d_row41_col97, #T_4b03d_row41_col98, #T_4b03d_row41_col99, #T_4b03d_row41_col100, #T_4b03d_row41_col101, #T_4b03d_row42_col0, #T_4b03d_row42_col1, #T_4b03d_row42_col2, #T_4b03d_row42_col3, #T_4b03d_row42_col4, #T_4b03d_row42_col5, #T_4b03d_row42_col6, #T_4b03d_row42_col7, #T_4b03d_row42_col8, #T_4b03d_row42_col9, #T_4b03d_row42_col10, #T_4b03d_row42_col11, #T_4b03d_row42_col12, #T_4b03d_row42_col13, #T_4b03d_row42_col14, #T_4b03d_row42_col15, #T_4b03d_row42_col16, #T_4b03d_row42_col17, #T_4b03d_row42_col18, #T_4b03d_row42_col19, #T_4b03d_row42_col20, #T_4b03d_row42_col21, #T_4b03d_row42_col22, #T_4b03d_row42_col23, #T_4b03d_row42_col24, #T_4b03d_row42_col25, #T_4b03d_row42_col26, #T_4b03d_row42_col27, #T_4b03d_row42_col28, #T_4b03d_row42_col29, #T_4b03d_row42_col30, #T_4b03d_row42_col31, #T_4b03d_row42_col32, #T_4b03d_row42_col33, #T_4b03d_row42_col34, #T_4b03d_row42_col35, #T_4b03d_row42_col36, #T_4b03d_row42_col37, #T_4b03d_row42_col38, #T_4b03d_row42_col39, #T_4b03d_row42_col40, #T_4b03d_row42_col41, #T_4b03d_row42_col42, #T_4b03d_row42_col43, #T_4b03d_row42_col44, #T_4b03d_row42_col45, #T_4b03d_row42_col46, #T_4b03d_row42_col47, #T_4b03d_row42_col48, #T_4b03d_row42_col49, #T_4b03d_row42_col50, #T_4b03d_row42_col51, #T_4b03d_row42_col52, #T_4b03d_row42_col53, #T_4b03d_row42_col54, #T_4b03d_row42_col55, #T_4b03d_row42_col56, #T_4b03d_row42_col57, #T_4b03d_row42_col58, #T_4b03d_row42_col59, #T_4b03d_row42_col60, #T_4b03d_row42_col61, #T_4b03d_row42_col62, #T_4b03d_row42_col63, #T_4b03d_row42_col64, #T_4b03d_row42_col65, #T_4b03d_row42_col66, #T_4b03d_row42_col67, #T_4b03d_row42_col68, #T_4b03d_row42_col69, #T_4b03d_row42_col70, #T_4b03d_row42_col71, #T_4b03d_row42_col72, #T_4b03d_row42_col73, #T_4b03d_row42_col74, #T_4b03d_row42_col75, #T_4b03d_row42_col76, #T_4b03d_row42_col77, #T_4b03d_row42_col78, #T_4b03d_row42_col79, #T_4b03d_row42_col80, #T_4b03d_row42_col81, #T_4b03d_row42_col82, #T_4b03d_row42_col83, #T_4b03d_row42_col84, #T_4b03d_row42_col85, #T_4b03d_row42_col86, #T_4b03d_row42_col87, #T_4b03d_row42_col88, #T_4b03d_row42_col89, #T_4b03d_row42_col90, #T_4b03d_row42_col91, #T_4b03d_row42_col92, #T_4b03d_row42_col93, #T_4b03d_row42_col94, #T_4b03d_row42_col95, #T_4b03d_row42_col96, #T_4b03d_row42_col97, #T_4b03d_row42_col98, #T_4b03d_row42_col99, #T_4b03d_row42_col100, #T_4b03d_row42_col101, #T_4b03d_row43_col0, #T_4b03d_row43_col1, #T_4b03d_row43_col2, #T_4b03d_row43_col3, #T_4b03d_row43_col4, #T_4b03d_row43_col5, #T_4b03d_row43_col6, #T_4b03d_row43_col7, #T_4b03d_row43_col8, #T_4b03d_row43_col9, #T_4b03d_row43_col10, #T_4b03d_row43_col11, #T_4b03d_row43_col12, #T_4b03d_row43_col13, #T_4b03d_row43_col14, #T_4b03d_row43_col15, #T_4b03d_row43_col16, #T_4b03d_row43_col17, #T_4b03d_row43_col18, #T_4b03d_row43_col19, #T_4b03d_row43_col20, #T_4b03d_row43_col21, #T_4b03d_row43_col22, #T_4b03d_row43_col23, #T_4b03d_row43_col24, #T_4b03d_row43_col25, #T_4b03d_row43_col26, #T_4b03d_row43_col27, #T_4b03d_row43_col28, #T_4b03d_row43_col29, #T_4b03d_row43_col30, #T_4b03d_row43_col31, #T_4b03d_row43_col32, #T_4b03d_row43_col33, #T_4b03d_row43_col34, #T_4b03d_row43_col35, #T_4b03d_row43_col36, #T_4b03d_row43_col37, #T_4b03d_row43_col38, #T_4b03d_row43_col39, #T_4b03d_row43_col40, #T_4b03d_row43_col41, #T_4b03d_row43_col42, #T_4b03d_row43_col43, #T_4b03d_row43_col44, #T_4b03d_row43_col45, #T_4b03d_row43_col46, #T_4b03d_row43_col47, #T_4b03d_row43_col48, #T_4b03d_row43_col49, #T_4b03d_row43_col50, #T_4b03d_row43_col51, #T_4b03d_row43_col52, #T_4b03d_row43_col53, #T_4b03d_row43_col54, #T_4b03d_row43_col55, #T_4b03d_row43_col56, #T_4b03d_row43_col57, #T_4b03d_row43_col58, #T_4b03d_row43_col59, #T_4b03d_row43_col60, #T_4b03d_row43_col61, #T_4b03d_row43_col62, #T_4b03d_row43_col63, #T_4b03d_row43_col64, #T_4b03d_row43_col65, #T_4b03d_row43_col66, #T_4b03d_row43_col67, #T_4b03d_row43_col68, #T_4b03d_row43_col69, #T_4b03d_row43_col70, #T_4b03d_row43_col71, #T_4b03d_row43_col72, #T_4b03d_row43_col73, #T_4b03d_row43_col74, #T_4b03d_row43_col75, #T_4b03d_row43_col76, #T_4b03d_row43_col77, #T_4b03d_row43_col78, #T_4b03d_row43_col79, #T_4b03d_row43_col80, #T_4b03d_row43_col81, #T_4b03d_row43_col82, #T_4b03d_row43_col83, #T_4b03d_row43_col84, #T_4b03d_row43_col85, #T_4b03d_row43_col86, #T_4b03d_row43_col87, #T_4b03d_row43_col88, #T_4b03d_row43_col89, #T_4b03d_row43_col90, #T_4b03d_row43_col91, #T_4b03d_row43_col92, #T_4b03d_row43_col93, #T_4b03d_row43_col94, #T_4b03d_row43_col95, #T_4b03d_row43_col96, #T_4b03d_row43_col97, #T_4b03d_row43_col98, #T_4b03d_row43_col99, #T_4b03d_row43_col100, #T_4b03d_row43_col101, #T_4b03d_row44_col0, #T_4b03d_row44_col1, #T_4b03d_row44_col2, #T_4b03d_row44_col3, #T_4b03d_row44_col4, #T_4b03d_row44_col5, #T_4b03d_row44_col6, #T_4b03d_row44_col7, #T_4b03d_row44_col8, #T_4b03d_row44_col9, #T_4b03d_row44_col10, #T_4b03d_row44_col11, #T_4b03d_row44_col12, #T_4b03d_row44_col13, #T_4b03d_row44_col14, #T_4b03d_row44_col15, #T_4b03d_row44_col16, #T_4b03d_row44_col17, #T_4b03d_row44_col18, #T_4b03d_row44_col19, #T_4b03d_row44_col20, #T_4b03d_row44_col21, #T_4b03d_row44_col22, #T_4b03d_row44_col23, #T_4b03d_row44_col24, #T_4b03d_row44_col25, #T_4b03d_row44_col26, #T_4b03d_row44_col27, #T_4b03d_row44_col28, #T_4b03d_row44_col29, #T_4b03d_row44_col30, #T_4b03d_row44_col31, #T_4b03d_row44_col32, #T_4b03d_row44_col33, #T_4b03d_row44_col34, #T_4b03d_row44_col35, #T_4b03d_row44_col36, #T_4b03d_row44_col37, #T_4b03d_row44_col38, #T_4b03d_row44_col39, #T_4b03d_row44_col40, #T_4b03d_row44_col41, #T_4b03d_row44_col42, #T_4b03d_row44_col43, #T_4b03d_row44_col44, #T_4b03d_row44_col45, #T_4b03d_row44_col46, #T_4b03d_row44_col47, #T_4b03d_row44_col48, #T_4b03d_row44_col49, #T_4b03d_row44_col50, #T_4b03d_row44_col51, #T_4b03d_row44_col52, #T_4b03d_row44_col53, #T_4b03d_row44_col54, #T_4b03d_row44_col55, #T_4b03d_row44_col56, #T_4b03d_row44_col57, #T_4b03d_row44_col58, #T_4b03d_row44_col59, #T_4b03d_row44_col60, #T_4b03d_row44_col61, #T_4b03d_row44_col62, #T_4b03d_row44_col63, #T_4b03d_row44_col64, #T_4b03d_row44_col65, #T_4b03d_row44_col66, #T_4b03d_row44_col67, #T_4b03d_row44_col68, #T_4b03d_row44_col69, #T_4b03d_row44_col70, #T_4b03d_row44_col71, #T_4b03d_row44_col72, #T_4b03d_row44_col73, #T_4b03d_row44_col74, #T_4b03d_row44_col75, #T_4b03d_row44_col76, #T_4b03d_row44_col77, #T_4b03d_row44_col78, #T_4b03d_row44_col79, #T_4b03d_row44_col80, #T_4b03d_row44_col81, #T_4b03d_row44_col82, #T_4b03d_row44_col83, #T_4b03d_row44_col84, #T_4b03d_row44_col85, #T_4b03d_row44_col86, #T_4b03d_row44_col87, #T_4b03d_row44_col88, #T_4b03d_row44_col89, #T_4b03d_row44_col90, #T_4b03d_row44_col91, #T_4b03d_row44_col92, #T_4b03d_row44_col93, #T_4b03d_row44_col94, #T_4b03d_row44_col95, #T_4b03d_row44_col96, #T_4b03d_row44_col97, #T_4b03d_row44_col98, #T_4b03d_row44_col99, #T_4b03d_row44_col100, #T_4b03d_row44_col101, #T_4b03d_row45_col0, #T_4b03d_row45_col1, #T_4b03d_row45_col2, #T_4b03d_row45_col3, #T_4b03d_row45_col4, #T_4b03d_row45_col5, #T_4b03d_row45_col6, #T_4b03d_row45_col7, #T_4b03d_row45_col8, #T_4b03d_row45_col9, #T_4b03d_row45_col10, #T_4b03d_row45_col11, #T_4b03d_row45_col12, #T_4b03d_row45_col13, #T_4b03d_row45_col14, #T_4b03d_row45_col15, #T_4b03d_row45_col16, #T_4b03d_row45_col17, #T_4b03d_row45_col18, #T_4b03d_row45_col19, #T_4b03d_row45_col20, #T_4b03d_row45_col21, #T_4b03d_row45_col22, #T_4b03d_row45_col23, #T_4b03d_row45_col24, #T_4b03d_row45_col25, #T_4b03d_row45_col26, #T_4b03d_row45_col27, #T_4b03d_row45_col28, #T_4b03d_row45_col29, #T_4b03d_row45_col30, #T_4b03d_row45_col31, #T_4b03d_row45_col32, #T_4b03d_row45_col33, #T_4b03d_row45_col34, #T_4b03d_row45_col35, #T_4b03d_row45_col36, #T_4b03d_row45_col37, #T_4b03d_row45_col38, #T_4b03d_row45_col39, #T_4b03d_row45_col40, #T_4b03d_row45_col41, #T_4b03d_row45_col42, #T_4b03d_row45_col43, #T_4b03d_row45_col44, #T_4b03d_row45_col45, #T_4b03d_row45_col46, #T_4b03d_row45_col47, #T_4b03d_row45_col48, #T_4b03d_row45_col49, #T_4b03d_row45_col50, #T_4b03d_row45_col51, #T_4b03d_row45_col52, #T_4b03d_row45_col53, #T_4b03d_row45_col54, #T_4b03d_row45_col55, #T_4b03d_row45_col56, #T_4b03d_row45_col57, #T_4b03d_row45_col58, #T_4b03d_row45_col59, #T_4b03d_row45_col60, #T_4b03d_row45_col61, #T_4b03d_row45_col62, #T_4b03d_row45_col63, #T_4b03d_row45_col64, #T_4b03d_row45_col65, #T_4b03d_row45_col66, #T_4b03d_row45_col67, #T_4b03d_row45_col68, #T_4b03d_row45_col69, #T_4b03d_row45_col70, #T_4b03d_row45_col71, #T_4b03d_row45_col72, #T_4b03d_row45_col73, #T_4b03d_row45_col74, #T_4b03d_row45_col75, #T_4b03d_row45_col76, #T_4b03d_row45_col77, #T_4b03d_row45_col78, #T_4b03d_row45_col79, #T_4b03d_row45_col80, #T_4b03d_row45_col81, #T_4b03d_row45_col82, #T_4b03d_row45_col83, #T_4b03d_row45_col84, #T_4b03d_row45_col85, #T_4b03d_row45_col86, #T_4b03d_row45_col87, #T_4b03d_row45_col88, #T_4b03d_row45_col89, #T_4b03d_row45_col90, #T_4b03d_row45_col91, #T_4b03d_row45_col92, #T_4b03d_row45_col93, #T_4b03d_row45_col94, #T_4b03d_row45_col95, #T_4b03d_row45_col96, #T_4b03d_row45_col97, #T_4b03d_row45_col98, #T_4b03d_row45_col99, #T_4b03d_row45_col100, #T_4b03d_row45_col101, #T_4b03d_row46_col0, #T_4b03d_row46_col1, #T_4b03d_row46_col2, #T_4b03d_row46_col3, #T_4b03d_row46_col4, #T_4b03d_row46_col5, #T_4b03d_row46_col6, #T_4b03d_row46_col7, #T_4b03d_row46_col8, #T_4b03d_row46_col9, #T_4b03d_row46_col10, #T_4b03d_row46_col11, #T_4b03d_row46_col12, #T_4b03d_row46_col13, #T_4b03d_row46_col14, #T_4b03d_row46_col15, #T_4b03d_row46_col16, #T_4b03d_row46_col17, #T_4b03d_row46_col18, #T_4b03d_row46_col19, #T_4b03d_row46_col20, #T_4b03d_row46_col21, #T_4b03d_row46_col22, #T_4b03d_row46_col23, #T_4b03d_row46_col24, #T_4b03d_row46_col25, #T_4b03d_row46_col26, #T_4b03d_row46_col27, #T_4b03d_row46_col28, #T_4b03d_row46_col29, #T_4b03d_row46_col30, #T_4b03d_row46_col31, #T_4b03d_row46_col32, #T_4b03d_row46_col33, #T_4b03d_row46_col34, #T_4b03d_row46_col35, #T_4b03d_row46_col36, #T_4b03d_row46_col37, #T_4b03d_row46_col38, #T_4b03d_row46_col39, #T_4b03d_row46_col40, #T_4b03d_row46_col41, #T_4b03d_row46_col42, #T_4b03d_row46_col43, #T_4b03d_row46_col44, #T_4b03d_row46_col45, #T_4b03d_row46_col46, #T_4b03d_row46_col47, #T_4b03d_row46_col48, #T_4b03d_row46_col49, #T_4b03d_row46_col50, #T_4b03d_row46_col51, #T_4b03d_row46_col52, #T_4b03d_row46_col53, #T_4b03d_row46_col54, #T_4b03d_row46_col55, #T_4b03d_row46_col56, #T_4b03d_row46_col57, #T_4b03d_row46_col58, #T_4b03d_row46_col59, #T_4b03d_row46_col60, #T_4b03d_row46_col61, #T_4b03d_row46_col62, #T_4b03d_row46_col63, #T_4b03d_row46_col64, #T_4b03d_row46_col65, #T_4b03d_row46_col66, #T_4b03d_row46_col67, #T_4b03d_row46_col68, #T_4b03d_row46_col69, #T_4b03d_row46_col70, #T_4b03d_row46_col71, #T_4b03d_row46_col72, #T_4b03d_row46_col73, #T_4b03d_row46_col74, #T_4b03d_row46_col75, #T_4b03d_row46_col76, #T_4b03d_row46_col77, #T_4b03d_row46_col78, #T_4b03d_row46_col79, #T_4b03d_row46_col80, #T_4b03d_row46_col81, #T_4b03d_row46_col82, #T_4b03d_row46_col83, #T_4b03d_row46_col84, #T_4b03d_row46_col85, #T_4b03d_row46_col86, #T_4b03d_row46_col87, #T_4b03d_row46_col88, #T_4b03d_row46_col89, #T_4b03d_row46_col90, #T_4b03d_row46_col91, #T_4b03d_row46_col92, #T_4b03d_row46_col93, #T_4b03d_row46_col94, #T_4b03d_row46_col95, #T_4b03d_row46_col96, #T_4b03d_row46_col97, #T_4b03d_row46_col98, #T_4b03d_row46_col99, #T_4b03d_row46_col100, #T_4b03d_row46_col101, #T_4b03d_row47_col0, #T_4b03d_row47_col1, #T_4b03d_row47_col2, #T_4b03d_row47_col3, #T_4b03d_row47_col4, #T_4b03d_row47_col5, #T_4b03d_row47_col6, #T_4b03d_row47_col7, #T_4b03d_row47_col8, #T_4b03d_row47_col9, #T_4b03d_row47_col10, #T_4b03d_row47_col11, #T_4b03d_row47_col12, #T_4b03d_row47_col13, #T_4b03d_row47_col14, #T_4b03d_row47_col15, #T_4b03d_row47_col16, #T_4b03d_row47_col17, #T_4b03d_row47_col18, #T_4b03d_row47_col19, #T_4b03d_row47_col20, #T_4b03d_row47_col21, #T_4b03d_row47_col22, #T_4b03d_row47_col23, #T_4b03d_row47_col24, #T_4b03d_row47_col25, #T_4b03d_row47_col26, #T_4b03d_row47_col27, #T_4b03d_row47_col28, #T_4b03d_row47_col29, #T_4b03d_row47_col30, #T_4b03d_row47_col31, #T_4b03d_row47_col32, #T_4b03d_row47_col33, #T_4b03d_row47_col34, #T_4b03d_row47_col35, #T_4b03d_row47_col36, #T_4b03d_row47_col37, #T_4b03d_row47_col38, #T_4b03d_row47_col39, #T_4b03d_row47_col40, #T_4b03d_row47_col41, #T_4b03d_row47_col42, #T_4b03d_row47_col43, #T_4b03d_row47_col44, #T_4b03d_row47_col45, #T_4b03d_row47_col46, #T_4b03d_row47_col47, #T_4b03d_row47_col48, #T_4b03d_row47_col49, #T_4b03d_row47_col50, #T_4b03d_row47_col51, #T_4b03d_row47_col52, #T_4b03d_row47_col53, #T_4b03d_row47_col54, #T_4b03d_row47_col55, #T_4b03d_row47_col56, #T_4b03d_row47_col57, #T_4b03d_row47_col58, #T_4b03d_row47_col59, #T_4b03d_row47_col60, #T_4b03d_row47_col61, #T_4b03d_row47_col62, #T_4b03d_row47_col63, #T_4b03d_row47_col64, #T_4b03d_row47_col65, #T_4b03d_row47_col66, #T_4b03d_row47_col67, #T_4b03d_row47_col68, #T_4b03d_row47_col69, #T_4b03d_row47_col70, #T_4b03d_row47_col71, #T_4b03d_row47_col72, #T_4b03d_row47_col73, #T_4b03d_row47_col74, #T_4b03d_row47_col75, #T_4b03d_row47_col76, #T_4b03d_row47_col77, #T_4b03d_row47_col78, #T_4b03d_row47_col79, #T_4b03d_row47_col80, #T_4b03d_row47_col81, #T_4b03d_row47_col82, #T_4b03d_row47_col83, #T_4b03d_row47_col84, #T_4b03d_row47_col85, #T_4b03d_row47_col86, #T_4b03d_row47_col87, #T_4b03d_row47_col88, #T_4b03d_row47_col89, #T_4b03d_row47_col90, #T_4b03d_row47_col91, #T_4b03d_row47_col92, #T_4b03d_row47_col93, #T_4b03d_row47_col94, #T_4b03d_row47_col95, #T_4b03d_row47_col96, #T_4b03d_row47_col97, #T_4b03d_row47_col98, #T_4b03d_row47_col99, #T_4b03d_row47_col100, #T_4b03d_row47_col101, #T_4b03d_row48_col0, #T_4b03d_row48_col1, #T_4b03d_row48_col2, #T_4b03d_row48_col3, #T_4b03d_row48_col4, #T_4b03d_row48_col5, #T_4b03d_row48_col6, #T_4b03d_row48_col7, #T_4b03d_row48_col8, #T_4b03d_row48_col9, #T_4b03d_row48_col10, #T_4b03d_row48_col11, #T_4b03d_row48_col12, #T_4b03d_row48_col13, #T_4b03d_row48_col14, #T_4b03d_row48_col15, #T_4b03d_row48_col16, #T_4b03d_row48_col17, #T_4b03d_row48_col18, #T_4b03d_row48_col19, #T_4b03d_row48_col20, #T_4b03d_row48_col21, #T_4b03d_row48_col22, #T_4b03d_row48_col23, #T_4b03d_row48_col24, #T_4b03d_row48_col25, #T_4b03d_row48_col26, #T_4b03d_row48_col27, #T_4b03d_row48_col28, #T_4b03d_row48_col29, #T_4b03d_row48_col30, #T_4b03d_row48_col31, #T_4b03d_row48_col32, #T_4b03d_row48_col33, #T_4b03d_row48_col34, #T_4b03d_row48_col35, #T_4b03d_row48_col36, #T_4b03d_row48_col37, #T_4b03d_row48_col38, #T_4b03d_row48_col39, #T_4b03d_row48_col40, #T_4b03d_row48_col41, #T_4b03d_row48_col42, #T_4b03d_row48_col43, #T_4b03d_row48_col44, #T_4b03d_row48_col45, #T_4b03d_row48_col46, #T_4b03d_row48_col47, #T_4b03d_row48_col48, #T_4b03d_row48_col49, #T_4b03d_row48_col50, #T_4b03d_row48_col51, #T_4b03d_row48_col52, #T_4b03d_row48_col53, #T_4b03d_row48_col54, #T_4b03d_row48_col55, #T_4b03d_row48_col56, #T_4b03d_row48_col57, #T_4b03d_row48_col58, #T_4b03d_row48_col59, #T_4b03d_row48_col60, #T_4b03d_row48_col61, #T_4b03d_row48_col62, #T_4b03d_row48_col63, #T_4b03d_row48_col64, #T_4b03d_row48_col65, #T_4b03d_row48_col66, #T_4b03d_row48_col67, #T_4b03d_row48_col68, #T_4b03d_row48_col69, #T_4b03d_row48_col70, #T_4b03d_row48_col71, #T_4b03d_row48_col72, #T_4b03d_row48_col73, #T_4b03d_row48_col74, #T_4b03d_row48_col75, #T_4b03d_row48_col76, #T_4b03d_row48_col77, #T_4b03d_row48_col78, #T_4b03d_row48_col79, #T_4b03d_row48_col80, #T_4b03d_row48_col81, #T_4b03d_row48_col82, #T_4b03d_row48_col83, #T_4b03d_row48_col84, #T_4b03d_row48_col85, #T_4b03d_row48_col86, #T_4b03d_row48_col87, #T_4b03d_row48_col88, #T_4b03d_row48_col89, #T_4b03d_row48_col90, #T_4b03d_row48_col91, #T_4b03d_row48_col92, #T_4b03d_row48_col93, #T_4b03d_row48_col94, #T_4b03d_row48_col95, #T_4b03d_row48_col96, #T_4b03d_row48_col97, #T_4b03d_row48_col98, #T_4b03d_row48_col99, #T_4b03d_row48_col100, #T_4b03d_row48_col101, #T_4b03d_row49_col0, #T_4b03d_row49_col1, #T_4b03d_row49_col2, #T_4b03d_row49_col3, #T_4b03d_row49_col4, #T_4b03d_row49_col5, #T_4b03d_row49_col6, #T_4b03d_row49_col7, #T_4b03d_row49_col8, #T_4b03d_row49_col9, #T_4b03d_row49_col10, #T_4b03d_row49_col11, #T_4b03d_row49_col12, #T_4b03d_row49_col13, #T_4b03d_row49_col14, #T_4b03d_row49_col15, #T_4b03d_row49_col16, #T_4b03d_row49_col17, #T_4b03d_row49_col18, #T_4b03d_row49_col19, #T_4b03d_row49_col20, #T_4b03d_row49_col21, #T_4b03d_row49_col22, #T_4b03d_row49_col23, #T_4b03d_row49_col24, #T_4b03d_row49_col25, #T_4b03d_row49_col26, #T_4b03d_row49_col27, #T_4b03d_row49_col28, #T_4b03d_row49_col29, #T_4b03d_row49_col30, #T_4b03d_row49_col31, #T_4b03d_row49_col32, #T_4b03d_row49_col33, #T_4b03d_row49_col34, #T_4b03d_row49_col35, #T_4b03d_row49_col36, #T_4b03d_row49_col37, #T_4b03d_row49_col38, #T_4b03d_row49_col39, #T_4b03d_row49_col40, #T_4b03d_row49_col41, #T_4b03d_row49_col42, #T_4b03d_row49_col43, #T_4b03d_row49_col44, #T_4b03d_row49_col45, #T_4b03d_row49_col46, #T_4b03d_row49_col47, #T_4b03d_row49_col48, #T_4b03d_row49_col49, #T_4b03d_row49_col50, #T_4b03d_row49_col51, #T_4b03d_row49_col52, #T_4b03d_row49_col53, #T_4b03d_row49_col54, #T_4b03d_row49_col55, #T_4b03d_row49_col56, #T_4b03d_row49_col57, #T_4b03d_row49_col58, #T_4b03d_row49_col59, #T_4b03d_row49_col60, #T_4b03d_row49_col61, #T_4b03d_row49_col62, #T_4b03d_row49_col63, #T_4b03d_row49_col64, #T_4b03d_row49_col65, #T_4b03d_row49_col66, #T_4b03d_row49_col67, #T_4b03d_row49_col68, #T_4b03d_row49_col69, #T_4b03d_row49_col70, #T_4b03d_row49_col71, #T_4b03d_row49_col72, #T_4b03d_row49_col73, #T_4b03d_row49_col74, #T_4b03d_row49_col75, #T_4b03d_row49_col76, #T_4b03d_row49_col77, #T_4b03d_row49_col78, #T_4b03d_row49_col79, #T_4b03d_row49_col80, #T_4b03d_row49_col81, #T_4b03d_row49_col82, #T_4b03d_row49_col83, #T_4b03d_row49_col84, #T_4b03d_row49_col85, #T_4b03d_row49_col86, #T_4b03d_row49_col87, #T_4b03d_row49_col88, #T_4b03d_row49_col89, #T_4b03d_row49_col90, #T_4b03d_row49_col91, #T_4b03d_row49_col92, #T_4b03d_row49_col93, #T_4b03d_row49_col94, #T_4b03d_row49_col95, #T_4b03d_row49_col96, #T_4b03d_row49_col97, #T_4b03d_row49_col98, #T_4b03d_row49_col99, #T_4b03d_row49_col100, #T_4b03d_row49_col101, #T_4b03d_row50_col0, #T_4b03d_row50_col1, #T_4b03d_row50_col2, #T_4b03d_row50_col3, #T_4b03d_row50_col4, #T_4b03d_row50_col5, #T_4b03d_row50_col6, #T_4b03d_row50_col7, #T_4b03d_row50_col8, #T_4b03d_row50_col9, #T_4b03d_row50_col10, #T_4b03d_row50_col11, #T_4b03d_row50_col12, #T_4b03d_row50_col13, #T_4b03d_row50_col14, #T_4b03d_row50_col15, #T_4b03d_row50_col16, #T_4b03d_row50_col17, #T_4b03d_row50_col18, #T_4b03d_row50_col19, #T_4b03d_row50_col20, #T_4b03d_row50_col21, #T_4b03d_row50_col22, #T_4b03d_row50_col23, #T_4b03d_row50_col24, #T_4b03d_row50_col25, #T_4b03d_row50_col26, #T_4b03d_row50_col27, #T_4b03d_row50_col28, #T_4b03d_row50_col29, #T_4b03d_row50_col30, #T_4b03d_row50_col31, #T_4b03d_row50_col32, #T_4b03d_row50_col33, #T_4b03d_row50_col34, #T_4b03d_row50_col35, #T_4b03d_row50_col36, #T_4b03d_row50_col37, #T_4b03d_row50_col38, #T_4b03d_row50_col39, #T_4b03d_row50_col40, #T_4b03d_row50_col41, #T_4b03d_row50_col42, #T_4b03d_row50_col43, #T_4b03d_row50_col44, #T_4b03d_row50_col45, #T_4b03d_row50_col46, #T_4b03d_row50_col47, #T_4b03d_row50_col48, #T_4b03d_row50_col49, #T_4b03d_row50_col50, #T_4b03d_row50_col51, #T_4b03d_row50_col52, #T_4b03d_row50_col53, #T_4b03d_row50_col54, #T_4b03d_row50_col55, #T_4b03d_row50_col56, #T_4b03d_row50_col57, #T_4b03d_row50_col58, #T_4b03d_row50_col59, #T_4b03d_row50_col60, #T_4b03d_row50_col61, #T_4b03d_row50_col62, #T_4b03d_row50_col63, #T_4b03d_row50_col64, #T_4b03d_row50_col65, #T_4b03d_row50_col66, #T_4b03d_row50_col67, #T_4b03d_row50_col68, #T_4b03d_row50_col69, #T_4b03d_row50_col70, #T_4b03d_row50_col71, #T_4b03d_row50_col72, #T_4b03d_row50_col73, #T_4b03d_row50_col74, #T_4b03d_row50_col75, #T_4b03d_row50_col76, #T_4b03d_row50_col77, #T_4b03d_row50_col78, #T_4b03d_row50_col79, #T_4b03d_row50_col80, #T_4b03d_row50_col81, #T_4b03d_row50_col82, #T_4b03d_row50_col83, #T_4b03d_row50_col84, #T_4b03d_row50_col85, #T_4b03d_row50_col86, #T_4b03d_row50_col87, #T_4b03d_row50_col88, #T_4b03d_row50_col89, #T_4b03d_row50_col90, #T_4b03d_row50_col91, #T_4b03d_row50_col92, #T_4b03d_row50_col93, #T_4b03d_row50_col94, #T_4b03d_row50_col95, #T_4b03d_row50_col96, #T_4b03d_row50_col97, #T_4b03d_row50_col98, #T_4b03d_row50_col99, #T_4b03d_row50_col100, #T_4b03d_row50_col101, #T_4b03d_row51_col0, #T_4b03d_row51_col1, #T_4b03d_row51_col2, #T_4b03d_row51_col3, #T_4b03d_row51_col4, #T_4b03d_row51_col5, #T_4b03d_row51_col6, #T_4b03d_row51_col7, #T_4b03d_row51_col8, #T_4b03d_row51_col9, #T_4b03d_row51_col10, #T_4b03d_row51_col11, #T_4b03d_row51_col12, #T_4b03d_row51_col13, #T_4b03d_row51_col14, #T_4b03d_row51_col15, #T_4b03d_row51_col16, #T_4b03d_row51_col17, #T_4b03d_row51_col18, #T_4b03d_row51_col19, #T_4b03d_row51_col20, #T_4b03d_row51_col21, #T_4b03d_row51_col22, #T_4b03d_row51_col23, #T_4b03d_row51_col24, #T_4b03d_row51_col25, #T_4b03d_row51_col26, #T_4b03d_row51_col27, #T_4b03d_row51_col28, #T_4b03d_row51_col29, #T_4b03d_row51_col30, #T_4b03d_row51_col31, #T_4b03d_row51_col32, #T_4b03d_row51_col33, #T_4b03d_row51_col34, #T_4b03d_row51_col35, #T_4b03d_row51_col36, #T_4b03d_row51_col37, #T_4b03d_row51_col38, #T_4b03d_row51_col39, #T_4b03d_row51_col40, #T_4b03d_row51_col41, #T_4b03d_row51_col42, #T_4b03d_row51_col43, #T_4b03d_row51_col44, #T_4b03d_row51_col45, #T_4b03d_row51_col46, #T_4b03d_row51_col47, #T_4b03d_row51_col48, #T_4b03d_row51_col49, #T_4b03d_row51_col50, #T_4b03d_row51_col51, #T_4b03d_row51_col52, #T_4b03d_row51_col53, #T_4b03d_row51_col54, #T_4b03d_row51_col55, #T_4b03d_row51_col56, #T_4b03d_row51_col57, #T_4b03d_row51_col58, #T_4b03d_row51_col59, #T_4b03d_row51_col60, #T_4b03d_row51_col61, #T_4b03d_row51_col62, #T_4b03d_row51_col63, #T_4b03d_row51_col64, #T_4b03d_row51_col65, #T_4b03d_row51_col66, #T_4b03d_row51_col67, #T_4b03d_row51_col68, #T_4b03d_row51_col69, #T_4b03d_row51_col70, #T_4b03d_row51_col71, #T_4b03d_row51_col72, #T_4b03d_row51_col73, #T_4b03d_row51_col74, #T_4b03d_row51_col75, #T_4b03d_row51_col76, #T_4b03d_row51_col77, #T_4b03d_row51_col78, #T_4b03d_row51_col79, #T_4b03d_row51_col80, #T_4b03d_row51_col81, #T_4b03d_row51_col82, #T_4b03d_row51_col83, #T_4b03d_row51_col84, #T_4b03d_row51_col85, #T_4b03d_row51_col86, #T_4b03d_row51_col87, #T_4b03d_row51_col88, #T_4b03d_row51_col89, #T_4b03d_row51_col90, #T_4b03d_row51_col91, #T_4b03d_row51_col92, #T_4b03d_row51_col93, #T_4b03d_row51_col94, #T_4b03d_row51_col95, #T_4b03d_row51_col96, #T_4b03d_row51_col97, #T_4b03d_row51_col98, #T_4b03d_row51_col99, #T_4b03d_row51_col100, #T_4b03d_row51_col101, #T_4b03d_row52_col0, #T_4b03d_row52_col1, #T_4b03d_row52_col2, #T_4b03d_row52_col3, #T_4b03d_row52_col4, #T_4b03d_row52_col5, #T_4b03d_row52_col6, #T_4b03d_row52_col7, #T_4b03d_row52_col8, #T_4b03d_row52_col9, #T_4b03d_row52_col10, #T_4b03d_row52_col11, #T_4b03d_row52_col12, #T_4b03d_row52_col13, #T_4b03d_row52_col14, #T_4b03d_row52_col15, #T_4b03d_row52_col16, #T_4b03d_row52_col17, #T_4b03d_row52_col18, #T_4b03d_row52_col19, #T_4b03d_row52_col20, #T_4b03d_row52_col21, #T_4b03d_row52_col22, #T_4b03d_row52_col23, #T_4b03d_row52_col24, #T_4b03d_row52_col25, #T_4b03d_row52_col26, #T_4b03d_row52_col27, #T_4b03d_row52_col28, #T_4b03d_row52_col29, #T_4b03d_row52_col30, #T_4b03d_row52_col31, #T_4b03d_row52_col32, #T_4b03d_row52_col33, #T_4b03d_row52_col34, #T_4b03d_row52_col35, #T_4b03d_row52_col36, #T_4b03d_row52_col37, #T_4b03d_row52_col38, #T_4b03d_row52_col39, #T_4b03d_row52_col40, #T_4b03d_row52_col41, #T_4b03d_row52_col42, #T_4b03d_row52_col43, #T_4b03d_row52_col44, #T_4b03d_row52_col45, #T_4b03d_row52_col46, #T_4b03d_row52_col47, #T_4b03d_row52_col48, #T_4b03d_row52_col49, #T_4b03d_row52_col50, #T_4b03d_row52_col51, #T_4b03d_row52_col52, #T_4b03d_row52_col53, #T_4b03d_row52_col54, #T_4b03d_row52_col55, #T_4b03d_row52_col56, #T_4b03d_row52_col57, #T_4b03d_row52_col58, #T_4b03d_row52_col59, #T_4b03d_row52_col60, #T_4b03d_row52_col61, #T_4b03d_row52_col62, #T_4b03d_row52_col63, #T_4b03d_row52_col64, #T_4b03d_row52_col65, #T_4b03d_row52_col66, #T_4b03d_row52_col67, #T_4b03d_row52_col68, #T_4b03d_row52_col69, #T_4b03d_row52_col70, #T_4b03d_row52_col71, #T_4b03d_row52_col72, #T_4b03d_row52_col73, #T_4b03d_row52_col74, #T_4b03d_row52_col75, #T_4b03d_row52_col76, #T_4b03d_row52_col77, #T_4b03d_row52_col78, #T_4b03d_row52_col79, #T_4b03d_row52_col80, #T_4b03d_row52_col81, #T_4b03d_row52_col82, #T_4b03d_row52_col83, #T_4b03d_row52_col84, #T_4b03d_row52_col85, #T_4b03d_row52_col86, #T_4b03d_row52_col87, #T_4b03d_row52_col88, #T_4b03d_row52_col89, #T_4b03d_row52_col90, #T_4b03d_row52_col91, #T_4b03d_row52_col92, #T_4b03d_row52_col93, #T_4b03d_row52_col94, #T_4b03d_row52_col95, #T_4b03d_row52_col96, #T_4b03d_row52_col97, #T_4b03d_row52_col98, #T_4b03d_row52_col99, #T_4b03d_row52_col100, #T_4b03d_row52_col101 {\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4b03d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4b03d_level0_col0\" class=\"col_heading level0 col0\" >Mutation Operator</th>\n",
              "      <th id=\"T_4b03d_level0_col1\" class=\"col_heading level0 col1\" >Mutated SMILES</th>\n",
              "      <th id=\"T_4b03d_level0_col2\" class=\"col_heading level0 col2\" >Predicted Lipophilicity</th>\n",
              "      <th id=\"T_4b03d_level0_col3\" class=\"col_heading level0 col3\" >SMILES</th>\n",
              "      <th id=\"T_4b03d_level0_col4\" class=\"col_heading level0 col4\" >molecular_weight</th>\n",
              "      <th id=\"T_4b03d_level0_col5\" class=\"col_heading level0 col5\" >logP</th>\n",
              "      <th id=\"T_4b03d_level0_col6\" class=\"col_heading level0 col6\" >hydrogen_bond_acceptors</th>\n",
              "      <th id=\"T_4b03d_level0_col7\" class=\"col_heading level0 col7\" >hydrogen_bond_donors</th>\n",
              "      <th id=\"T_4b03d_level0_col8\" class=\"col_heading level0 col8\" >Lipinski</th>\n",
              "      <th id=\"T_4b03d_level0_col9\" class=\"col_heading level0 col9\" >QED</th>\n",
              "      <th id=\"T_4b03d_level0_col10\" class=\"col_heading level0 col10\" >stereo_centers</th>\n",
              "      <th id=\"T_4b03d_level0_col11\" class=\"col_heading level0 col11\" >tpsa</th>\n",
              "      <th id=\"T_4b03d_level0_col12\" class=\"col_heading level0 col12\" >AMES</th>\n",
              "      <th id=\"T_4b03d_level0_col13\" class=\"col_heading level0 col13\" >BBB_Martins</th>\n",
              "      <th id=\"T_4b03d_level0_col14\" class=\"col_heading level0 col14\" >Bioavailability_Ma</th>\n",
              "      <th id=\"T_4b03d_level0_col15\" class=\"col_heading level0 col15\" >CYP1A2_Veith</th>\n",
              "      <th id=\"T_4b03d_level0_col16\" class=\"col_heading level0 col16\" >CYP2C19_Veith</th>\n",
              "      <th id=\"T_4b03d_level0_col17\" class=\"col_heading level0 col17\" >CYP2C9_Substrate_CarbonMangels</th>\n",
              "      <th id=\"T_4b03d_level0_col18\" class=\"col_heading level0 col18\" >CYP2C9_Veith</th>\n",
              "      <th id=\"T_4b03d_level0_col19\" class=\"col_heading level0 col19\" >CYP2D6_Substrate_CarbonMangels</th>\n",
              "      <th id=\"T_4b03d_level0_col20\" class=\"col_heading level0 col20\" >CYP2D6_Veith</th>\n",
              "      <th id=\"T_4b03d_level0_col21\" class=\"col_heading level0 col21\" >CYP3A4_Substrate_CarbonMangels</th>\n",
              "      <th id=\"T_4b03d_level0_col22\" class=\"col_heading level0 col22\" >CYP3A4_Veith</th>\n",
              "      <th id=\"T_4b03d_level0_col23\" class=\"col_heading level0 col23\" >Carcinogens_Lagunin</th>\n",
              "      <th id=\"T_4b03d_level0_col24\" class=\"col_heading level0 col24\" >ClinTox</th>\n",
              "      <th id=\"T_4b03d_level0_col25\" class=\"col_heading level0 col25\" >DILI</th>\n",
              "      <th id=\"T_4b03d_level0_col26\" class=\"col_heading level0 col26\" >HIA_Hou</th>\n",
              "      <th id=\"T_4b03d_level0_col27\" class=\"col_heading level0 col27\" >NR-AR-LBD</th>\n",
              "      <th id=\"T_4b03d_level0_col28\" class=\"col_heading level0 col28\" >NR-AR</th>\n",
              "      <th id=\"T_4b03d_level0_col29\" class=\"col_heading level0 col29\" >NR-AhR</th>\n",
              "      <th id=\"T_4b03d_level0_col30\" class=\"col_heading level0 col30\" >NR-Aromatase</th>\n",
              "      <th id=\"T_4b03d_level0_col31\" class=\"col_heading level0 col31\" >NR-ER-LBD</th>\n",
              "      <th id=\"T_4b03d_level0_col32\" class=\"col_heading level0 col32\" >NR-ER</th>\n",
              "      <th id=\"T_4b03d_level0_col33\" class=\"col_heading level0 col33\" >NR-PPAR-gamma</th>\n",
              "      <th id=\"T_4b03d_level0_col34\" class=\"col_heading level0 col34\" >PAMPA_NCATS</th>\n",
              "      <th id=\"T_4b03d_level0_col35\" class=\"col_heading level0 col35\" >Pgp_Broccatelli</th>\n",
              "      <th id=\"T_4b03d_level0_col36\" class=\"col_heading level0 col36\" >SR-ARE</th>\n",
              "      <th id=\"T_4b03d_level0_col37\" class=\"col_heading level0 col37\" >SR-ATAD5</th>\n",
              "      <th id=\"T_4b03d_level0_col38\" class=\"col_heading level0 col38\" >SR-HSE</th>\n",
              "      <th id=\"T_4b03d_level0_col39\" class=\"col_heading level0 col39\" >SR-MMP</th>\n",
              "      <th id=\"T_4b03d_level0_col40\" class=\"col_heading level0 col40\" >SR-p53</th>\n",
              "      <th id=\"T_4b03d_level0_col41\" class=\"col_heading level0 col41\" >Skin_Reaction</th>\n",
              "      <th id=\"T_4b03d_level0_col42\" class=\"col_heading level0 col42\" >hERG</th>\n",
              "      <th id=\"T_4b03d_level0_col43\" class=\"col_heading level0 col43\" >Caco2_Wang</th>\n",
              "      <th id=\"T_4b03d_level0_col44\" class=\"col_heading level0 col44\" >Clearance_Hepatocyte_AZ</th>\n",
              "      <th id=\"T_4b03d_level0_col45\" class=\"col_heading level0 col45\" >Clearance_Microsome_AZ</th>\n",
              "      <th id=\"T_4b03d_level0_col46\" class=\"col_heading level0 col46\" >Half_Life_Obach</th>\n",
              "      <th id=\"T_4b03d_level0_col47\" class=\"col_heading level0 col47\" >HydrationFreeEnergy_FreeSolv</th>\n",
              "      <th id=\"T_4b03d_level0_col48\" class=\"col_heading level0 col48\" >LD50_Zhu</th>\n",
              "      <th id=\"T_4b03d_level0_col49\" class=\"col_heading level0 col49\" >Lipophilicity_AstraZeneca</th>\n",
              "      <th id=\"T_4b03d_level0_col50\" class=\"col_heading level0 col50\" >PPBR_AZ</th>\n",
              "      <th id=\"T_4b03d_level0_col51\" class=\"col_heading level0 col51\" >Solubility_AqSolDB</th>\n",
              "      <th id=\"T_4b03d_level0_col52\" class=\"col_heading level0 col52\" >VDss_Lombardo</th>\n",
              "      <th id=\"T_4b03d_level0_col53\" class=\"col_heading level0 col53\" >molecular_weight_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col54\" class=\"col_heading level0 col54\" >logP_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col55\" class=\"col_heading level0 col55\" >hydrogen_bond_acceptors_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col56\" class=\"col_heading level0 col56\" >hydrogen_bond_donors_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col57\" class=\"col_heading level0 col57\" >Lipinski_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col58\" class=\"col_heading level0 col58\" >QED_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col59\" class=\"col_heading level0 col59\" >stereo_centers_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col60\" class=\"col_heading level0 col60\" >tpsa_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col61\" class=\"col_heading level0 col61\" >AMES_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col62\" class=\"col_heading level0 col62\" >BBB_Martins_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col63\" class=\"col_heading level0 col63\" >Bioavailability_Ma_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col64\" class=\"col_heading level0 col64\" >CYP1A2_Veith_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col65\" class=\"col_heading level0 col65\" >CYP2C19_Veith_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col66\" class=\"col_heading level0 col66\" >CYP2C9_Substrate_CarbonMangels_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col67\" class=\"col_heading level0 col67\" >CYP2C9_Veith_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col68\" class=\"col_heading level0 col68\" >CYP2D6_Substrate_CarbonMangels_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col69\" class=\"col_heading level0 col69\" >CYP2D6_Veith_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col70\" class=\"col_heading level0 col70\" >CYP3A4_Substrate_CarbonMangels_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col71\" class=\"col_heading level0 col71\" >CYP3A4_Veith_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col72\" class=\"col_heading level0 col72\" >Carcinogens_Lagunin_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col73\" class=\"col_heading level0 col73\" >ClinTox_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col74\" class=\"col_heading level0 col74\" >DILI_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col75\" class=\"col_heading level0 col75\" >HIA_Hou_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col76\" class=\"col_heading level0 col76\" >NR-AR-LBD_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col77\" class=\"col_heading level0 col77\" >NR-AR_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col78\" class=\"col_heading level0 col78\" >NR-AhR_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col79\" class=\"col_heading level0 col79\" >NR-Aromatase_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col80\" class=\"col_heading level0 col80\" >NR-ER-LBD_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col81\" class=\"col_heading level0 col81\" >NR-ER_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col82\" class=\"col_heading level0 col82\" >NR-PPAR-gamma_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col83\" class=\"col_heading level0 col83\" >PAMPA_NCATS_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col84\" class=\"col_heading level0 col84\" >Pgp_Broccatelli_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col85\" class=\"col_heading level0 col85\" >SR-ARE_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col86\" class=\"col_heading level0 col86\" >SR-ATAD5_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col87\" class=\"col_heading level0 col87\" >SR-HSE_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col88\" class=\"col_heading level0 col88\" >SR-MMP_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col89\" class=\"col_heading level0 col89\" >SR-p53_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col90\" class=\"col_heading level0 col90\" >Skin_Reaction_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col91\" class=\"col_heading level0 col91\" >hERG_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col92\" class=\"col_heading level0 col92\" >Caco2_Wang_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col93\" class=\"col_heading level0 col93\" >Clearance_Hepatocyte_AZ_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col94\" class=\"col_heading level0 col94\" >Clearance_Microsome_AZ_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col95\" class=\"col_heading level0 col95\" >Half_Life_Obach_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col96\" class=\"col_heading level0 col96\" >HydrationFreeEnergy_FreeSolv_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col97\" class=\"col_heading level0 col97\" >LD50_Zhu_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col98\" class=\"col_heading level0 col98\" >Lipophilicity_AstraZeneca_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col99\" class=\"col_heading level0 col99\" >PPBR_AZ_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col100\" class=\"col_heading level0 col100\" >Solubility_AqSolDB_drugbank_approved_percentile</th>\n",
              "      <th id=\"T_4b03d_level0_col101\" class=\"col_heading level0 col101\" >VDss_Lombardo_drugbank_approved_percentile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_4b03d_row0_col0\" class=\"data row0 col0\" >Stereochemistry Alteration</td>\n",
              "      <td id=\"T_4b03d_row0_col1\" class=\"data row0 col1\" >CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row0_col2\" class=\"data row0 col2\" >2.565190</td>\n",
              "      <td id=\"T_4b03d_row0_col3\" class=\"data row0 col3\" >CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row0_col4\" class=\"data row0 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row0_col5\" class=\"data row0 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row0_col6\" class=\"data row0 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row0_col7\" class=\"data row0 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row0_col8\" class=\"data row0 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row0_col9\" class=\"data row0 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row0_col10\" class=\"data row0 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row0_col11\" class=\"data row0 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row0_col12\" class=\"data row0 col12\" >0.365814</td>\n",
              "      <td id=\"T_4b03d_row0_col13\" class=\"data row0 col13\" >0.311598</td>\n",
              "      <td id=\"T_4b03d_row0_col14\" class=\"data row0 col14\" >0.660250</td>\n",
              "      <td id=\"T_4b03d_row0_col15\" class=\"data row0 col15\" >0.008532</td>\n",
              "      <td id=\"T_4b03d_row0_col16\" class=\"data row0 col16\" >0.057158</td>\n",
              "      <td id=\"T_4b03d_row0_col17\" class=\"data row0 col17\" >0.038424</td>\n",
              "      <td id=\"T_4b03d_row0_col18\" class=\"data row0 col18\" >0.119140</td>\n",
              "      <td id=\"T_4b03d_row0_col19\" class=\"data row0 col19\" >0.024473</td>\n",
              "      <td id=\"T_4b03d_row0_col20\" class=\"data row0 col20\" >0.067545</td>\n",
              "      <td id=\"T_4b03d_row0_col21\" class=\"data row0 col21\" >0.637120</td>\n",
              "      <td id=\"T_4b03d_row0_col22\" class=\"data row0 col22\" >0.609762</td>\n",
              "      <td id=\"T_4b03d_row0_col23\" class=\"data row0 col23\" >0.077062</td>\n",
              "      <td id=\"T_4b03d_row0_col24\" class=\"data row0 col24\" >0.329605</td>\n",
              "      <td id=\"T_4b03d_row0_col25\" class=\"data row0 col25\" >0.774216</td>\n",
              "      <td id=\"T_4b03d_row0_col26\" class=\"data row0 col26\" >0.977409</td>\n",
              "      <td id=\"T_4b03d_row0_col27\" class=\"data row0 col27\" >0.243521</td>\n",
              "      <td id=\"T_4b03d_row0_col28\" class=\"data row0 col28\" >0.466249</td>\n",
              "      <td id=\"T_4b03d_row0_col29\" class=\"data row0 col29\" >0.046050</td>\n",
              "      <td id=\"T_4b03d_row0_col30\" class=\"data row0 col30\" >0.278337</td>\n",
              "      <td id=\"T_4b03d_row0_col31\" class=\"data row0 col31\" >0.031847</td>\n",
              "      <td id=\"T_4b03d_row0_col32\" class=\"data row0 col32\" >0.215310</td>\n",
              "      <td id=\"T_4b03d_row0_col33\" class=\"data row0 col33\" >0.086647</td>\n",
              "      <td id=\"T_4b03d_row0_col34\" class=\"data row0 col34\" >0.496751</td>\n",
              "      <td id=\"T_4b03d_row0_col35\" class=\"data row0 col35\" >0.729301</td>\n",
              "      <td id=\"T_4b03d_row0_col36\" class=\"data row0 col36\" >0.309806</td>\n",
              "      <td id=\"T_4b03d_row0_col37\" class=\"data row0 col37\" >0.283111</td>\n",
              "      <td id=\"T_4b03d_row0_col38\" class=\"data row0 col38\" >0.038665</td>\n",
              "      <td id=\"T_4b03d_row0_col39\" class=\"data row0 col39\" >0.899910</td>\n",
              "      <td id=\"T_4b03d_row0_col40\" class=\"data row0 col40\" >0.329493</td>\n",
              "      <td id=\"T_4b03d_row0_col41\" class=\"data row0 col41\" >0.085074</td>\n",
              "      <td id=\"T_4b03d_row0_col42\" class=\"data row0 col42\" >0.607149</td>\n",
              "      <td id=\"T_4b03d_row0_col43\" class=\"data row0 col43\" >-5.653007</td>\n",
              "      <td id=\"T_4b03d_row0_col44\" class=\"data row0 col44\" >126.158450</td>\n",
              "      <td id=\"T_4b03d_row0_col45\" class=\"data row0 col45\" >96.605691</td>\n",
              "      <td id=\"T_4b03d_row0_col46\" class=\"data row0 col46\" >46.264950</td>\n",
              "      <td id=\"T_4b03d_row0_col47\" class=\"data row0 col47\" >-11.153003</td>\n",
              "      <td id=\"T_4b03d_row0_col48\" class=\"data row0 col48\" >4.357207</td>\n",
              "      <td id=\"T_4b03d_row0_col49\" class=\"data row0 col49\" >2.565190</td>\n",
              "      <td id=\"T_4b03d_row0_col50\" class=\"data row0 col50\" >99.619293</td>\n",
              "      <td id=\"T_4b03d_row0_col51\" class=\"data row0 col51\" >-5.935346</td>\n",
              "      <td id=\"T_4b03d_row0_col52\" class=\"data row0 col52\" >3.278859</td>\n",
              "      <td id=\"T_4b03d_row0_col53\" class=\"data row0 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row0_col54\" class=\"data row0 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row0_col55\" class=\"data row0 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row0_col56\" class=\"data row0 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row0_col57\" class=\"data row0 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row0_col58\" class=\"data row0 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row0_col59\" class=\"data row0 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row0_col60\" class=\"data row0 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row0_col61\" class=\"data row0 col61\" >73.090345</td>\n",
              "      <td id=\"T_4b03d_row0_col62\" class=\"data row0 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row0_col63\" class=\"data row0 col63\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row0_col64\" class=\"data row0 col64\" >31.756495</td>\n",
              "      <td id=\"T_4b03d_row0_col65\" class=\"data row0 col65\" >40.480807</td>\n",
              "      <td id=\"T_4b03d_row0_col66\" class=\"data row0 col66\" >13.997674</td>\n",
              "      <td id=\"T_4b03d_row0_col67\" class=\"data row0 col67\" >68.321055</td>\n",
              "      <td id=\"T_4b03d_row0_col68\" class=\"data row0 col68\" >17.991470</td>\n",
              "      <td id=\"T_4b03d_row0_col69\" class=\"data row0 col69\" >55.253974</td>\n",
              "      <td id=\"T_4b03d_row0_col70\" class=\"data row0 col70\" >65.219077</td>\n",
              "      <td id=\"T_4b03d_row0_col71\" class=\"data row0 col71\" >83.947266</td>\n",
              "      <td id=\"T_4b03d_row0_col72\" class=\"data row0 col72\" >29.934083</td>\n",
              "      <td id=\"T_4b03d_row0_col73\" class=\"data row0 col73\" >78.324932</td>\n",
              "      <td id=\"T_4b03d_row0_col74\" class=\"data row0 col74\" >69.600620</td>\n",
              "      <td id=\"T_4b03d_row0_col75\" class=\"data row0 col75\" >35.323769</td>\n",
              "      <td id=\"T_4b03d_row0_col76\" class=\"data row0 col76\" >93.485847</td>\n",
              "      <td id=\"T_4b03d_row0_col77\" class=\"data row0 col77\" >94.610314</td>\n",
              "      <td id=\"T_4b03d_row0_col78\" class=\"data row0 col78\" >59.751842</td>\n",
              "      <td id=\"T_4b03d_row0_col79\" class=\"data row0 col79\" >89.879798</td>\n",
              "      <td id=\"T_4b03d_row0_col80\" class=\"data row0 col80\" >66.149670</td>\n",
              "      <td id=\"T_4b03d_row0_col81\" class=\"data row0 col81\" >81.620783</td>\n",
              "      <td id=\"T_4b03d_row0_col82\" class=\"data row0 col82\" >89.841024</td>\n",
              "      <td id=\"T_4b03d_row0_col83\" class=\"data row0 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row0_col84\" class=\"data row0 col84\" >84.916634</td>\n",
              "      <td id=\"T_4b03d_row0_col85\" class=\"data row0 col85\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row0_col86\" class=\"data row0 col86\" >96.432726</td>\n",
              "      <td id=\"T_4b03d_row0_col87\" class=\"data row0 col87\" >70.298565</td>\n",
              "      <td id=\"T_4b03d_row0_col88\" class=\"data row0 col88\" >97.789841</td>\n",
              "      <td id=\"T_4b03d_row0_col89\" class=\"data row0 col89\" >91.624661</td>\n",
              "      <td id=\"T_4b03d_row0_col90\" class=\"data row0 col90\" >3.683598</td>\n",
              "      <td id=\"T_4b03d_row0_col91\" class=\"data row0 col91\" >68.553703</td>\n",
              "      <td id=\"T_4b03d_row0_col92\" class=\"data row0 col92\" >19.464909</td>\n",
              "      <td id=\"T_4b03d_row0_col93\" class=\"data row0 col93\" >96.238852</td>\n",
              "      <td id=\"T_4b03d_row0_col94\" class=\"data row0 col94\" >94.920512</td>\n",
              "      <td id=\"T_4b03d_row0_col95\" class=\"data row0 col95\" >85.886002</td>\n",
              "      <td id=\"T_4b03d_row0_col96\" class=\"data row0 col96\" >34.819698</td>\n",
              "      <td id=\"T_4b03d_row0_col97\" class=\"data row0 col97\" >99.224506</td>\n",
              "      <td id=\"T_4b03d_row0_col98\" class=\"data row0 col98\" >71.655680</td>\n",
              "      <td id=\"T_4b03d_row0_col99\" class=\"data row0 col99\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row0_col100\" class=\"data row0 col100\" >5.777433</td>\n",
              "      <td id=\"T_4b03d_row0_col101\" class=\"data row0 col101\" >62.853819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_4b03d_row1_col0\" class=\"data row1 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row1_col1\" class=\"data row1 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row1_col2\" class=\"data row1 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row1_col3\" class=\"data row1 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row1_col4\" class=\"data row1 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row1_col5\" class=\"data row1 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row1_col6\" class=\"data row1 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row1_col7\" class=\"data row1 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row1_col8\" class=\"data row1 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row1_col9\" class=\"data row1 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row1_col10\" class=\"data row1 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row1_col11\" class=\"data row1 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row1_col12\" class=\"data row1 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row1_col13\" class=\"data row1 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row1_col14\" class=\"data row1 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row1_col15\" class=\"data row1 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row1_col16\" class=\"data row1 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row1_col17\" class=\"data row1 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row1_col18\" class=\"data row1 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row1_col19\" class=\"data row1 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row1_col20\" class=\"data row1 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row1_col21\" class=\"data row1 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row1_col22\" class=\"data row1 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row1_col23\" class=\"data row1 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row1_col24\" class=\"data row1 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row1_col25\" class=\"data row1 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row1_col26\" class=\"data row1 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row1_col27\" class=\"data row1 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row1_col28\" class=\"data row1 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row1_col29\" class=\"data row1 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row1_col30\" class=\"data row1 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row1_col31\" class=\"data row1 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row1_col32\" class=\"data row1 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row1_col33\" class=\"data row1 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row1_col34\" class=\"data row1 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row1_col35\" class=\"data row1 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row1_col36\" class=\"data row1 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row1_col37\" class=\"data row1 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row1_col38\" class=\"data row1 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row1_col39\" class=\"data row1 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row1_col40\" class=\"data row1 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row1_col41\" class=\"data row1 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row1_col42\" class=\"data row1 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row1_col43\" class=\"data row1 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row1_col44\" class=\"data row1 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row1_col45\" class=\"data row1 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row1_col46\" class=\"data row1 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row1_col47\" class=\"data row1 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row1_col48\" class=\"data row1 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row1_col49\" class=\"data row1 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row1_col50\" class=\"data row1 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row1_col51\" class=\"data row1 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row1_col52\" class=\"data row1 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row1_col53\" class=\"data row1 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row1_col54\" class=\"data row1 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row1_col55\" class=\"data row1 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row1_col56\" class=\"data row1 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row1_col57\" class=\"data row1 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row1_col58\" class=\"data row1 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row1_col59\" class=\"data row1 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row1_col60\" class=\"data row1 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row1_col61\" class=\"data row1 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row1_col62\" class=\"data row1 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row1_col63\" class=\"data row1 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row1_col64\" class=\"data row1 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row1_col65\" class=\"data row1 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row1_col66\" class=\"data row1 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row1_col67\" class=\"data row1 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row1_col68\" class=\"data row1 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row1_col69\" class=\"data row1 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row1_col70\" class=\"data row1 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row1_col71\" class=\"data row1 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row1_col72\" class=\"data row1 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row1_col73\" class=\"data row1 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row1_col74\" class=\"data row1 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row1_col75\" class=\"data row1 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row1_col76\" class=\"data row1 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row1_col77\" class=\"data row1 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row1_col78\" class=\"data row1 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row1_col79\" class=\"data row1 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row1_col80\" class=\"data row1 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row1_col81\" class=\"data row1 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row1_col82\" class=\"data row1 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row1_col83\" class=\"data row1 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row1_col84\" class=\"data row1 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row1_col85\" class=\"data row1 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row1_col86\" class=\"data row1 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row1_col87\" class=\"data row1 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row1_col88\" class=\"data row1 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row1_col89\" class=\"data row1 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row1_col90\" class=\"data row1 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row1_col91\" class=\"data row1 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row1_col92\" class=\"data row1 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row1_col93\" class=\"data row1 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row1_col94\" class=\"data row1 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row1_col95\" class=\"data row1 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row1_col96\" class=\"data row1 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row1_col97\" class=\"data row1 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row1_col98\" class=\"data row1 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row1_col99\" class=\"data row1 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row1_col100\" class=\"data row1 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row1_col101\" class=\"data row1 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_4b03d_row2_col0\" class=\"data row2 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row2_col1\" class=\"data row2 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row2_col2\" class=\"data row2 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row2_col3\" class=\"data row2 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row2_col4\" class=\"data row2 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row2_col5\" class=\"data row2 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row2_col6\" class=\"data row2 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row2_col7\" class=\"data row2 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row2_col8\" class=\"data row2 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row2_col9\" class=\"data row2 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row2_col10\" class=\"data row2 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row2_col11\" class=\"data row2 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row2_col12\" class=\"data row2 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row2_col13\" class=\"data row2 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row2_col14\" class=\"data row2 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row2_col15\" class=\"data row2 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row2_col16\" class=\"data row2 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row2_col17\" class=\"data row2 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row2_col18\" class=\"data row2 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row2_col19\" class=\"data row2 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row2_col20\" class=\"data row2 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row2_col21\" class=\"data row2 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row2_col22\" class=\"data row2 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row2_col23\" class=\"data row2 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row2_col24\" class=\"data row2 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row2_col25\" class=\"data row2 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row2_col26\" class=\"data row2 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row2_col27\" class=\"data row2 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row2_col28\" class=\"data row2 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row2_col29\" class=\"data row2 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row2_col30\" class=\"data row2 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row2_col31\" class=\"data row2 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row2_col32\" class=\"data row2 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row2_col33\" class=\"data row2 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row2_col34\" class=\"data row2 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row2_col35\" class=\"data row2 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row2_col36\" class=\"data row2 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row2_col37\" class=\"data row2 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row2_col38\" class=\"data row2 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row2_col39\" class=\"data row2 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row2_col40\" class=\"data row2 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row2_col41\" class=\"data row2 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row2_col42\" class=\"data row2 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row2_col43\" class=\"data row2 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row2_col44\" class=\"data row2 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row2_col45\" class=\"data row2 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row2_col46\" class=\"data row2 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row2_col47\" class=\"data row2 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row2_col48\" class=\"data row2 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row2_col49\" class=\"data row2 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row2_col50\" class=\"data row2 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row2_col51\" class=\"data row2 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row2_col52\" class=\"data row2 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row2_col53\" class=\"data row2 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row2_col54\" class=\"data row2 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row2_col55\" class=\"data row2 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row2_col56\" class=\"data row2 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row2_col57\" class=\"data row2 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row2_col58\" class=\"data row2 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row2_col59\" class=\"data row2 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row2_col60\" class=\"data row2 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row2_col61\" class=\"data row2 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row2_col62\" class=\"data row2 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row2_col63\" class=\"data row2 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row2_col64\" class=\"data row2 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row2_col65\" class=\"data row2 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row2_col66\" class=\"data row2 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row2_col67\" class=\"data row2 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row2_col68\" class=\"data row2 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row2_col69\" class=\"data row2 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row2_col70\" class=\"data row2 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row2_col71\" class=\"data row2 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row2_col72\" class=\"data row2 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row2_col73\" class=\"data row2 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row2_col74\" class=\"data row2 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row2_col75\" class=\"data row2 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row2_col76\" class=\"data row2 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row2_col77\" class=\"data row2 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row2_col78\" class=\"data row2 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row2_col79\" class=\"data row2 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row2_col80\" class=\"data row2 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row2_col81\" class=\"data row2 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row2_col82\" class=\"data row2 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row2_col83\" class=\"data row2 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row2_col84\" class=\"data row2 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row2_col85\" class=\"data row2 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row2_col86\" class=\"data row2 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row2_col87\" class=\"data row2 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row2_col88\" class=\"data row2 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row2_col89\" class=\"data row2 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row2_col90\" class=\"data row2 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row2_col91\" class=\"data row2 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row2_col92\" class=\"data row2 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row2_col93\" class=\"data row2 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row2_col94\" class=\"data row2 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row2_col95\" class=\"data row2 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row2_col96\" class=\"data row2 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row2_col97\" class=\"data row2 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row2_col98\" class=\"data row2 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row2_col99\" class=\"data row2 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row2_col100\" class=\"data row2 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row2_col101\" class=\"data row2 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_4b03d_row3_col0\" class=\"data row3 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row3_col1\" class=\"data row3 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row3_col2\" class=\"data row3 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row3_col3\" class=\"data row3 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row3_col4\" class=\"data row3 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row3_col5\" class=\"data row3 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row3_col6\" class=\"data row3 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row3_col7\" class=\"data row3 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row3_col8\" class=\"data row3 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row3_col9\" class=\"data row3 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row3_col10\" class=\"data row3 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row3_col11\" class=\"data row3 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row3_col12\" class=\"data row3 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row3_col13\" class=\"data row3 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row3_col14\" class=\"data row3 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row3_col15\" class=\"data row3 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row3_col16\" class=\"data row3 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row3_col17\" class=\"data row3 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row3_col18\" class=\"data row3 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row3_col19\" class=\"data row3 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row3_col20\" class=\"data row3 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row3_col21\" class=\"data row3 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row3_col22\" class=\"data row3 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row3_col23\" class=\"data row3 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row3_col24\" class=\"data row3 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row3_col25\" class=\"data row3 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row3_col26\" class=\"data row3 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row3_col27\" class=\"data row3 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row3_col28\" class=\"data row3 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row3_col29\" class=\"data row3 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row3_col30\" class=\"data row3 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row3_col31\" class=\"data row3 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row3_col32\" class=\"data row3 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row3_col33\" class=\"data row3 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row3_col34\" class=\"data row3 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row3_col35\" class=\"data row3 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row3_col36\" class=\"data row3 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row3_col37\" class=\"data row3 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row3_col38\" class=\"data row3 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row3_col39\" class=\"data row3 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row3_col40\" class=\"data row3 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row3_col41\" class=\"data row3 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row3_col42\" class=\"data row3 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row3_col43\" class=\"data row3 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row3_col44\" class=\"data row3 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row3_col45\" class=\"data row3 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row3_col46\" class=\"data row3 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row3_col47\" class=\"data row3 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row3_col48\" class=\"data row3 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row3_col49\" class=\"data row3 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row3_col50\" class=\"data row3 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row3_col51\" class=\"data row3 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row3_col52\" class=\"data row3 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row3_col53\" class=\"data row3 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row3_col54\" class=\"data row3 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row3_col55\" class=\"data row3 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row3_col56\" class=\"data row3 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row3_col57\" class=\"data row3 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row3_col58\" class=\"data row3 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row3_col59\" class=\"data row3 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row3_col60\" class=\"data row3 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row3_col61\" class=\"data row3 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row3_col62\" class=\"data row3 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row3_col63\" class=\"data row3 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row3_col64\" class=\"data row3 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row3_col65\" class=\"data row3 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row3_col66\" class=\"data row3 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row3_col67\" class=\"data row3 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row3_col68\" class=\"data row3 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row3_col69\" class=\"data row3 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row3_col70\" class=\"data row3 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row3_col71\" class=\"data row3 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row3_col72\" class=\"data row3 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row3_col73\" class=\"data row3 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row3_col74\" class=\"data row3 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row3_col75\" class=\"data row3 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row3_col76\" class=\"data row3 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row3_col77\" class=\"data row3 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row3_col78\" class=\"data row3 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row3_col79\" class=\"data row3 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row3_col80\" class=\"data row3 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row3_col81\" class=\"data row3 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row3_col82\" class=\"data row3 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row3_col83\" class=\"data row3 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row3_col84\" class=\"data row3 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row3_col85\" class=\"data row3 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row3_col86\" class=\"data row3 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row3_col87\" class=\"data row3 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row3_col88\" class=\"data row3 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row3_col89\" class=\"data row3 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row3_col90\" class=\"data row3 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row3_col91\" class=\"data row3 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row3_col92\" class=\"data row3 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row3_col93\" class=\"data row3 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row3_col94\" class=\"data row3 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row3_col95\" class=\"data row3 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row3_col96\" class=\"data row3 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row3_col97\" class=\"data row3 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row3_col98\" class=\"data row3 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row3_col99\" class=\"data row3 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row3_col100\" class=\"data row3 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row3_col101\" class=\"data row3 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_4b03d_row4_col0\" class=\"data row4 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row4_col1\" class=\"data row4 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row4_col2\" class=\"data row4 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row4_col3\" class=\"data row4 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row4_col4\" class=\"data row4 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row4_col5\" class=\"data row4 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row4_col6\" class=\"data row4 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row4_col7\" class=\"data row4 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row4_col8\" class=\"data row4 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row4_col9\" class=\"data row4 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row4_col10\" class=\"data row4 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row4_col11\" class=\"data row4 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row4_col12\" class=\"data row4 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row4_col13\" class=\"data row4 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row4_col14\" class=\"data row4 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row4_col15\" class=\"data row4 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row4_col16\" class=\"data row4 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row4_col17\" class=\"data row4 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row4_col18\" class=\"data row4 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row4_col19\" class=\"data row4 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row4_col20\" class=\"data row4 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row4_col21\" class=\"data row4 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row4_col22\" class=\"data row4 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row4_col23\" class=\"data row4 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row4_col24\" class=\"data row4 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row4_col25\" class=\"data row4 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row4_col26\" class=\"data row4 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row4_col27\" class=\"data row4 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row4_col28\" class=\"data row4 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row4_col29\" class=\"data row4 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row4_col30\" class=\"data row4 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row4_col31\" class=\"data row4 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row4_col32\" class=\"data row4 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row4_col33\" class=\"data row4 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row4_col34\" class=\"data row4 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row4_col35\" class=\"data row4 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row4_col36\" class=\"data row4 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row4_col37\" class=\"data row4 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row4_col38\" class=\"data row4 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row4_col39\" class=\"data row4 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row4_col40\" class=\"data row4 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row4_col41\" class=\"data row4 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row4_col42\" class=\"data row4 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row4_col43\" class=\"data row4 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row4_col44\" class=\"data row4 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row4_col45\" class=\"data row4 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row4_col46\" class=\"data row4 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row4_col47\" class=\"data row4 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row4_col48\" class=\"data row4 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row4_col49\" class=\"data row4 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row4_col50\" class=\"data row4 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row4_col51\" class=\"data row4 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row4_col52\" class=\"data row4 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row4_col53\" class=\"data row4 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row4_col54\" class=\"data row4 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row4_col55\" class=\"data row4 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row4_col56\" class=\"data row4 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row4_col57\" class=\"data row4 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row4_col58\" class=\"data row4 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row4_col59\" class=\"data row4 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row4_col60\" class=\"data row4 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row4_col61\" class=\"data row4 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row4_col62\" class=\"data row4 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row4_col63\" class=\"data row4 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row4_col64\" class=\"data row4 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row4_col65\" class=\"data row4 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row4_col66\" class=\"data row4 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row4_col67\" class=\"data row4 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row4_col68\" class=\"data row4 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row4_col69\" class=\"data row4 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row4_col70\" class=\"data row4 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row4_col71\" class=\"data row4 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row4_col72\" class=\"data row4 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row4_col73\" class=\"data row4 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row4_col74\" class=\"data row4 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row4_col75\" class=\"data row4 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row4_col76\" class=\"data row4 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row4_col77\" class=\"data row4 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row4_col78\" class=\"data row4 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row4_col79\" class=\"data row4 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row4_col80\" class=\"data row4 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row4_col81\" class=\"data row4 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row4_col82\" class=\"data row4 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row4_col83\" class=\"data row4 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row4_col84\" class=\"data row4 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row4_col85\" class=\"data row4 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row4_col86\" class=\"data row4 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row4_col87\" class=\"data row4 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row4_col88\" class=\"data row4 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row4_col89\" class=\"data row4 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row4_col90\" class=\"data row4 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row4_col91\" class=\"data row4 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row4_col92\" class=\"data row4 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row4_col93\" class=\"data row4 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row4_col94\" class=\"data row4 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row4_col95\" class=\"data row4 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row4_col96\" class=\"data row4 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row4_col97\" class=\"data row4 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row4_col98\" class=\"data row4 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row4_col99\" class=\"data row4 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row4_col100\" class=\"data row4 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row4_col101\" class=\"data row4 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_4b03d_row5_col0\" class=\"data row5 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row5_col1\" class=\"data row5 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row5_col2\" class=\"data row5 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row5_col3\" class=\"data row5 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row5_col4\" class=\"data row5 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row5_col5\" class=\"data row5 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row5_col6\" class=\"data row5 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row5_col7\" class=\"data row5 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row5_col8\" class=\"data row5 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row5_col9\" class=\"data row5 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row5_col10\" class=\"data row5 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row5_col11\" class=\"data row5 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row5_col12\" class=\"data row5 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row5_col13\" class=\"data row5 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row5_col14\" class=\"data row5 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row5_col15\" class=\"data row5 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row5_col16\" class=\"data row5 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row5_col17\" class=\"data row5 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row5_col18\" class=\"data row5 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row5_col19\" class=\"data row5 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row5_col20\" class=\"data row5 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row5_col21\" class=\"data row5 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row5_col22\" class=\"data row5 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row5_col23\" class=\"data row5 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row5_col24\" class=\"data row5 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row5_col25\" class=\"data row5 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row5_col26\" class=\"data row5 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row5_col27\" class=\"data row5 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row5_col28\" class=\"data row5 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row5_col29\" class=\"data row5 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row5_col30\" class=\"data row5 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row5_col31\" class=\"data row5 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row5_col32\" class=\"data row5 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row5_col33\" class=\"data row5 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row5_col34\" class=\"data row5 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row5_col35\" class=\"data row5 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row5_col36\" class=\"data row5 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row5_col37\" class=\"data row5 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row5_col38\" class=\"data row5 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row5_col39\" class=\"data row5 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row5_col40\" class=\"data row5 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row5_col41\" class=\"data row5 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row5_col42\" class=\"data row5 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row5_col43\" class=\"data row5 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row5_col44\" class=\"data row5 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row5_col45\" class=\"data row5 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row5_col46\" class=\"data row5 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row5_col47\" class=\"data row5 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row5_col48\" class=\"data row5 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row5_col49\" class=\"data row5 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row5_col50\" class=\"data row5 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row5_col51\" class=\"data row5 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row5_col52\" class=\"data row5 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row5_col53\" class=\"data row5 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row5_col54\" class=\"data row5 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row5_col55\" class=\"data row5 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row5_col56\" class=\"data row5 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row5_col57\" class=\"data row5 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row5_col58\" class=\"data row5 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row5_col59\" class=\"data row5 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row5_col60\" class=\"data row5 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row5_col61\" class=\"data row5 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row5_col62\" class=\"data row5 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row5_col63\" class=\"data row5 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row5_col64\" class=\"data row5 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row5_col65\" class=\"data row5 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row5_col66\" class=\"data row5 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row5_col67\" class=\"data row5 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row5_col68\" class=\"data row5 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row5_col69\" class=\"data row5 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row5_col70\" class=\"data row5 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row5_col71\" class=\"data row5 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row5_col72\" class=\"data row5 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row5_col73\" class=\"data row5 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row5_col74\" class=\"data row5 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row5_col75\" class=\"data row5 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row5_col76\" class=\"data row5 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row5_col77\" class=\"data row5 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row5_col78\" class=\"data row5 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row5_col79\" class=\"data row5 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row5_col80\" class=\"data row5 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row5_col81\" class=\"data row5 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row5_col82\" class=\"data row5 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row5_col83\" class=\"data row5 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row5_col84\" class=\"data row5 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row5_col85\" class=\"data row5 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row5_col86\" class=\"data row5 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row5_col87\" class=\"data row5 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row5_col88\" class=\"data row5 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row5_col89\" class=\"data row5 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row5_col90\" class=\"data row5 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row5_col91\" class=\"data row5 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row5_col92\" class=\"data row5 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row5_col93\" class=\"data row5 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row5_col94\" class=\"data row5 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row5_col95\" class=\"data row5 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row5_col96\" class=\"data row5 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row5_col97\" class=\"data row5 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row5_col98\" class=\"data row5 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row5_col99\" class=\"data row5 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row5_col100\" class=\"data row5 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row5_col101\" class=\"data row5 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_4b03d_row6_col0\" class=\"data row6 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row6_col1\" class=\"data row6 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row6_col2\" class=\"data row6 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row6_col3\" class=\"data row6 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row6_col4\" class=\"data row6 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row6_col5\" class=\"data row6 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row6_col6\" class=\"data row6 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row6_col7\" class=\"data row6 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row6_col8\" class=\"data row6 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row6_col9\" class=\"data row6 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row6_col10\" class=\"data row6 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row6_col11\" class=\"data row6 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row6_col12\" class=\"data row6 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row6_col13\" class=\"data row6 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row6_col14\" class=\"data row6 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row6_col15\" class=\"data row6 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row6_col16\" class=\"data row6 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row6_col17\" class=\"data row6 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row6_col18\" class=\"data row6 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row6_col19\" class=\"data row6 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row6_col20\" class=\"data row6 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row6_col21\" class=\"data row6 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row6_col22\" class=\"data row6 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row6_col23\" class=\"data row6 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row6_col24\" class=\"data row6 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row6_col25\" class=\"data row6 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row6_col26\" class=\"data row6 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row6_col27\" class=\"data row6 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row6_col28\" class=\"data row6 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row6_col29\" class=\"data row6 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row6_col30\" class=\"data row6 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row6_col31\" class=\"data row6 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row6_col32\" class=\"data row6 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row6_col33\" class=\"data row6 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row6_col34\" class=\"data row6 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row6_col35\" class=\"data row6 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row6_col36\" class=\"data row6 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row6_col37\" class=\"data row6 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row6_col38\" class=\"data row6 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row6_col39\" class=\"data row6 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row6_col40\" class=\"data row6 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row6_col41\" class=\"data row6 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row6_col42\" class=\"data row6 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row6_col43\" class=\"data row6 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row6_col44\" class=\"data row6 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row6_col45\" class=\"data row6 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row6_col46\" class=\"data row6 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row6_col47\" class=\"data row6 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row6_col48\" class=\"data row6 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row6_col49\" class=\"data row6 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row6_col50\" class=\"data row6 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row6_col51\" class=\"data row6 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row6_col52\" class=\"data row6 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row6_col53\" class=\"data row6 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row6_col54\" class=\"data row6 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row6_col55\" class=\"data row6 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row6_col56\" class=\"data row6 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row6_col57\" class=\"data row6 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row6_col58\" class=\"data row6 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row6_col59\" class=\"data row6 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row6_col60\" class=\"data row6 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row6_col61\" class=\"data row6 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row6_col62\" class=\"data row6 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row6_col63\" class=\"data row6 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row6_col64\" class=\"data row6 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row6_col65\" class=\"data row6 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row6_col66\" class=\"data row6 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row6_col67\" class=\"data row6 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row6_col68\" class=\"data row6 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row6_col69\" class=\"data row6 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row6_col70\" class=\"data row6 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row6_col71\" class=\"data row6 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row6_col72\" class=\"data row6 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row6_col73\" class=\"data row6 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row6_col74\" class=\"data row6 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row6_col75\" class=\"data row6 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row6_col76\" class=\"data row6 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row6_col77\" class=\"data row6 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row6_col78\" class=\"data row6 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row6_col79\" class=\"data row6 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row6_col80\" class=\"data row6 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row6_col81\" class=\"data row6 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row6_col82\" class=\"data row6 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row6_col83\" class=\"data row6 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row6_col84\" class=\"data row6 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row6_col85\" class=\"data row6 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row6_col86\" class=\"data row6 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row6_col87\" class=\"data row6 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row6_col88\" class=\"data row6 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row6_col89\" class=\"data row6 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row6_col90\" class=\"data row6 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row6_col91\" class=\"data row6 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row6_col92\" class=\"data row6 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row6_col93\" class=\"data row6 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row6_col94\" class=\"data row6 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row6_col95\" class=\"data row6 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row6_col96\" class=\"data row6 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row6_col97\" class=\"data row6 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row6_col98\" class=\"data row6 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row6_col99\" class=\"data row6 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row6_col100\" class=\"data row6 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row6_col101\" class=\"data row6 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_4b03d_row7_col0\" class=\"data row7 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row7_col1\" class=\"data row7 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row7_col2\" class=\"data row7 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row7_col3\" class=\"data row7 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row7_col4\" class=\"data row7 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row7_col5\" class=\"data row7 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row7_col6\" class=\"data row7 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row7_col7\" class=\"data row7 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row7_col8\" class=\"data row7 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row7_col9\" class=\"data row7 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row7_col10\" class=\"data row7 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row7_col11\" class=\"data row7 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row7_col12\" class=\"data row7 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row7_col13\" class=\"data row7 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row7_col14\" class=\"data row7 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row7_col15\" class=\"data row7 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row7_col16\" class=\"data row7 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row7_col17\" class=\"data row7 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row7_col18\" class=\"data row7 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row7_col19\" class=\"data row7 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row7_col20\" class=\"data row7 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row7_col21\" class=\"data row7 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row7_col22\" class=\"data row7 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row7_col23\" class=\"data row7 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row7_col24\" class=\"data row7 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row7_col25\" class=\"data row7 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row7_col26\" class=\"data row7 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row7_col27\" class=\"data row7 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row7_col28\" class=\"data row7 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row7_col29\" class=\"data row7 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row7_col30\" class=\"data row7 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row7_col31\" class=\"data row7 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row7_col32\" class=\"data row7 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row7_col33\" class=\"data row7 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row7_col34\" class=\"data row7 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row7_col35\" class=\"data row7 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row7_col36\" class=\"data row7 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row7_col37\" class=\"data row7 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row7_col38\" class=\"data row7 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row7_col39\" class=\"data row7 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row7_col40\" class=\"data row7 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row7_col41\" class=\"data row7 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row7_col42\" class=\"data row7 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row7_col43\" class=\"data row7 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row7_col44\" class=\"data row7 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row7_col45\" class=\"data row7 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row7_col46\" class=\"data row7 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row7_col47\" class=\"data row7 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row7_col48\" class=\"data row7 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row7_col49\" class=\"data row7 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row7_col50\" class=\"data row7 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row7_col51\" class=\"data row7 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row7_col52\" class=\"data row7 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row7_col53\" class=\"data row7 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row7_col54\" class=\"data row7 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row7_col55\" class=\"data row7 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row7_col56\" class=\"data row7 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row7_col57\" class=\"data row7 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row7_col58\" class=\"data row7 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row7_col59\" class=\"data row7 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row7_col60\" class=\"data row7 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row7_col61\" class=\"data row7 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row7_col62\" class=\"data row7 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row7_col63\" class=\"data row7 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row7_col64\" class=\"data row7 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row7_col65\" class=\"data row7 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row7_col66\" class=\"data row7 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row7_col67\" class=\"data row7 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row7_col68\" class=\"data row7 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row7_col69\" class=\"data row7 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row7_col70\" class=\"data row7 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row7_col71\" class=\"data row7 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row7_col72\" class=\"data row7 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row7_col73\" class=\"data row7 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row7_col74\" class=\"data row7 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row7_col75\" class=\"data row7 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row7_col76\" class=\"data row7 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row7_col77\" class=\"data row7 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row7_col78\" class=\"data row7 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row7_col79\" class=\"data row7 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row7_col80\" class=\"data row7 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row7_col81\" class=\"data row7 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row7_col82\" class=\"data row7 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row7_col83\" class=\"data row7 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row7_col84\" class=\"data row7 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row7_col85\" class=\"data row7 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row7_col86\" class=\"data row7 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row7_col87\" class=\"data row7 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row7_col88\" class=\"data row7 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row7_col89\" class=\"data row7 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row7_col90\" class=\"data row7 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row7_col91\" class=\"data row7 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row7_col92\" class=\"data row7 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row7_col93\" class=\"data row7 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row7_col94\" class=\"data row7 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row7_col95\" class=\"data row7 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row7_col96\" class=\"data row7 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row7_col97\" class=\"data row7 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row7_col98\" class=\"data row7 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row7_col99\" class=\"data row7 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row7_col100\" class=\"data row7 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row7_col101\" class=\"data row7 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_4b03d_row8_col0\" class=\"data row8 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row8_col1\" class=\"data row8 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row8_col2\" class=\"data row8 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row8_col3\" class=\"data row8 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row8_col4\" class=\"data row8 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row8_col5\" class=\"data row8 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row8_col6\" class=\"data row8 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row8_col7\" class=\"data row8 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row8_col8\" class=\"data row8 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row8_col9\" class=\"data row8 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row8_col10\" class=\"data row8 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row8_col11\" class=\"data row8 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row8_col12\" class=\"data row8 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row8_col13\" class=\"data row8 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row8_col14\" class=\"data row8 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row8_col15\" class=\"data row8 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row8_col16\" class=\"data row8 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row8_col17\" class=\"data row8 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row8_col18\" class=\"data row8 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row8_col19\" class=\"data row8 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row8_col20\" class=\"data row8 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row8_col21\" class=\"data row8 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row8_col22\" class=\"data row8 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row8_col23\" class=\"data row8 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row8_col24\" class=\"data row8 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row8_col25\" class=\"data row8 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row8_col26\" class=\"data row8 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row8_col27\" class=\"data row8 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row8_col28\" class=\"data row8 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row8_col29\" class=\"data row8 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row8_col30\" class=\"data row8 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row8_col31\" class=\"data row8 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row8_col32\" class=\"data row8 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row8_col33\" class=\"data row8 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row8_col34\" class=\"data row8 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row8_col35\" class=\"data row8 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row8_col36\" class=\"data row8 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row8_col37\" class=\"data row8 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row8_col38\" class=\"data row8 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row8_col39\" class=\"data row8 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row8_col40\" class=\"data row8 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row8_col41\" class=\"data row8 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row8_col42\" class=\"data row8 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row8_col43\" class=\"data row8 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row8_col44\" class=\"data row8 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row8_col45\" class=\"data row8 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row8_col46\" class=\"data row8 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row8_col47\" class=\"data row8 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row8_col48\" class=\"data row8 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row8_col49\" class=\"data row8 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row8_col50\" class=\"data row8 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row8_col51\" class=\"data row8 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row8_col52\" class=\"data row8 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row8_col53\" class=\"data row8 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row8_col54\" class=\"data row8 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row8_col55\" class=\"data row8 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row8_col56\" class=\"data row8 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row8_col57\" class=\"data row8 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row8_col58\" class=\"data row8 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row8_col59\" class=\"data row8 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row8_col60\" class=\"data row8 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row8_col61\" class=\"data row8 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row8_col62\" class=\"data row8 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row8_col63\" class=\"data row8 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row8_col64\" class=\"data row8 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row8_col65\" class=\"data row8 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row8_col66\" class=\"data row8 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row8_col67\" class=\"data row8 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row8_col68\" class=\"data row8 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row8_col69\" class=\"data row8 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row8_col70\" class=\"data row8 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row8_col71\" class=\"data row8 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row8_col72\" class=\"data row8 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row8_col73\" class=\"data row8 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row8_col74\" class=\"data row8 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row8_col75\" class=\"data row8 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row8_col76\" class=\"data row8 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row8_col77\" class=\"data row8 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row8_col78\" class=\"data row8 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row8_col79\" class=\"data row8 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row8_col80\" class=\"data row8 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row8_col81\" class=\"data row8 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row8_col82\" class=\"data row8 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row8_col83\" class=\"data row8 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row8_col84\" class=\"data row8 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row8_col85\" class=\"data row8 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row8_col86\" class=\"data row8 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row8_col87\" class=\"data row8 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row8_col88\" class=\"data row8 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row8_col89\" class=\"data row8 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row8_col90\" class=\"data row8 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row8_col91\" class=\"data row8 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row8_col92\" class=\"data row8 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row8_col93\" class=\"data row8 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row8_col94\" class=\"data row8 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row8_col95\" class=\"data row8 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row8_col96\" class=\"data row8 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row8_col97\" class=\"data row8 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row8_col98\" class=\"data row8 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row8_col99\" class=\"data row8 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row8_col100\" class=\"data row8 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row8_col101\" class=\"data row8 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_4b03d_row9_col0\" class=\"data row9 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row9_col1\" class=\"data row9 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row9_col2\" class=\"data row9 col2\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row9_col3\" class=\"data row9 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row9_col4\" class=\"data row9 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row9_col5\" class=\"data row9 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row9_col6\" class=\"data row9 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row9_col7\" class=\"data row9 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row9_col8\" class=\"data row9 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row9_col9\" class=\"data row9 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row9_col10\" class=\"data row9 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row9_col11\" class=\"data row9 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row9_col12\" class=\"data row9 col12\" >0.369026</td>\n",
              "      <td id=\"T_4b03d_row9_col13\" class=\"data row9 col13\" >0.313214</td>\n",
              "      <td id=\"T_4b03d_row9_col14\" class=\"data row9 col14\" >0.686472</td>\n",
              "      <td id=\"T_4b03d_row9_col15\" class=\"data row9 col15\" >0.007988</td>\n",
              "      <td id=\"T_4b03d_row9_col16\" class=\"data row9 col16\" >0.057606</td>\n",
              "      <td id=\"T_4b03d_row9_col17\" class=\"data row9 col17\" >0.042368</td>\n",
              "      <td id=\"T_4b03d_row9_col18\" class=\"data row9 col18\" >0.114900</td>\n",
              "      <td id=\"T_4b03d_row9_col19\" class=\"data row9 col19\" >0.024853</td>\n",
              "      <td id=\"T_4b03d_row9_col20\" class=\"data row9 col20\" >0.061521</td>\n",
              "      <td id=\"T_4b03d_row9_col21\" class=\"data row9 col21\" >0.630606</td>\n",
              "      <td id=\"T_4b03d_row9_col22\" class=\"data row9 col22\" >0.601277</td>\n",
              "      <td id=\"T_4b03d_row9_col23\" class=\"data row9 col23\" >0.086117</td>\n",
              "      <td id=\"T_4b03d_row9_col24\" class=\"data row9 col24\" >0.334512</td>\n",
              "      <td id=\"T_4b03d_row9_col25\" class=\"data row9 col25\" >0.796276</td>\n",
              "      <td id=\"T_4b03d_row9_col26\" class=\"data row9 col26\" >0.979034</td>\n",
              "      <td id=\"T_4b03d_row9_col27\" class=\"data row9 col27\" >0.205691</td>\n",
              "      <td id=\"T_4b03d_row9_col28\" class=\"data row9 col28\" >0.420263</td>\n",
              "      <td id=\"T_4b03d_row9_col29\" class=\"data row9 col29\" >0.047349</td>\n",
              "      <td id=\"T_4b03d_row9_col30\" class=\"data row9 col30\" >0.254458</td>\n",
              "      <td id=\"T_4b03d_row9_col31\" class=\"data row9 col31\" >0.028368</td>\n",
              "      <td id=\"T_4b03d_row9_col32\" class=\"data row9 col32\" >0.196054</td>\n",
              "      <td id=\"T_4b03d_row9_col33\" class=\"data row9 col33\" >0.082704</td>\n",
              "      <td id=\"T_4b03d_row9_col34\" class=\"data row9 col34\" >0.496009</td>\n",
              "      <td id=\"T_4b03d_row9_col35\" class=\"data row9 col35\" >0.743860</td>\n",
              "      <td id=\"T_4b03d_row9_col36\" class=\"data row9 col36\" >0.299541</td>\n",
              "      <td id=\"T_4b03d_row9_col37\" class=\"data row9 col37\" >0.261965</td>\n",
              "      <td id=\"T_4b03d_row9_col38\" class=\"data row9 col38\" >0.037576</td>\n",
              "      <td id=\"T_4b03d_row9_col39\" class=\"data row9 col39\" >0.894732</td>\n",
              "      <td id=\"T_4b03d_row9_col40\" class=\"data row9 col40\" >0.298632</td>\n",
              "      <td id=\"T_4b03d_row9_col41\" class=\"data row9 col41\" >0.082203</td>\n",
              "      <td id=\"T_4b03d_row9_col42\" class=\"data row9 col42\" >0.604351</td>\n",
              "      <td id=\"T_4b03d_row9_col43\" class=\"data row9 col43\" >-5.646112</td>\n",
              "      <td id=\"T_4b03d_row9_col44\" class=\"data row9 col44\" >127.149407</td>\n",
              "      <td id=\"T_4b03d_row9_col45\" class=\"data row9 col45\" >97.401829</td>\n",
              "      <td id=\"T_4b03d_row9_col46\" class=\"data row9 col46\" >45.362657</td>\n",
              "      <td id=\"T_4b03d_row9_col47\" class=\"data row9 col47\" >-11.105821</td>\n",
              "      <td id=\"T_4b03d_row9_col48\" class=\"data row9 col48\" >4.349726</td>\n",
              "      <td id=\"T_4b03d_row9_col49\" class=\"data row9 col49\" >2.591541</td>\n",
              "      <td id=\"T_4b03d_row9_col50\" class=\"data row9 col50\" >99.711003</td>\n",
              "      <td id=\"T_4b03d_row9_col51\" class=\"data row9 col51\" >-5.970253</td>\n",
              "      <td id=\"T_4b03d_row9_col52\" class=\"data row9 col52\" >3.308127</td>\n",
              "      <td id=\"T_4b03d_row9_col53\" class=\"data row9 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row9_col54\" class=\"data row9 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row9_col55\" class=\"data row9 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row9_col56\" class=\"data row9 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row9_col57\" class=\"data row9 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row9_col58\" class=\"data row9 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row9_col59\" class=\"data row9 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row9_col60\" class=\"data row9 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row9_col61\" class=\"data row9 col61\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row9_col62\" class=\"data row9 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row9_col63\" class=\"data row9 col63\" >34.587049</td>\n",
              "      <td id=\"T_4b03d_row9_col64\" class=\"data row9 col64\" >31.058550</td>\n",
              "      <td id=\"T_4b03d_row9_col65\" class=\"data row9 col65\" >40.635905</td>\n",
              "      <td id=\"T_4b03d_row9_col66\" class=\"data row9 col66\" >15.664986</td>\n",
              "      <td id=\"T_4b03d_row9_col67\" class=\"data row9 col67\" >67.545560</td>\n",
              "      <td id=\"T_4b03d_row9_col68\" class=\"data row9 col68\" >18.224118</td>\n",
              "      <td id=\"T_4b03d_row9_col69\" class=\"data row9 col69\" >54.013183</td>\n",
              "      <td id=\"T_4b03d_row9_col70\" class=\"data row9 col70\" >64.249709</td>\n",
              "      <td id=\"T_4b03d_row9_col71\" class=\"data row9 col71\" >83.869717</td>\n",
              "      <td id=\"T_4b03d_row9_col72\" class=\"data row9 col72\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row9_col73\" class=\"data row9 col73\" >78.557580</td>\n",
              "      <td id=\"T_4b03d_row9_col74\" class=\"data row9 col74\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row9_col75\" class=\"data row9 col75\" >36.021714</td>\n",
              "      <td id=\"T_4b03d_row9_col76\" class=\"data row9 col76\" >93.136875</td>\n",
              "      <td id=\"T_4b03d_row9_col77\" class=\"data row9 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row9_col78\" class=\"data row9 col78\" >60.178364</td>\n",
              "      <td id=\"T_4b03d_row9_col79\" class=\"data row9 col79\" >88.910430</td>\n",
              "      <td id=\"T_4b03d_row9_col80\" class=\"data row9 col80\" >63.435440</td>\n",
              "      <td id=\"T_4b03d_row9_col81\" class=\"data row9 col81\" >79.410624</td>\n",
              "      <td id=\"T_4b03d_row9_col82\" class=\"data row9 col82\" >89.414502</td>\n",
              "      <td id=\"T_4b03d_row9_col83\" class=\"data row9 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row9_col84\" class=\"data row9 col84\" >85.808453</td>\n",
              "      <td id=\"T_4b03d_row9_col85\" class=\"data row9 col85\" >71.384257</td>\n",
              "      <td id=\"T_4b03d_row9_col86\" class=\"data row9 col86\" >95.812330</td>\n",
              "      <td id=\"T_4b03d_row9_col87\" class=\"data row9 col87\" >69.716945</td>\n",
              "      <td id=\"T_4b03d_row9_col88\" class=\"data row9 col88\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row9_col89\" class=\"data row9 col89\" >90.306320</td>\n",
              "      <td id=\"T_4b03d_row9_col90\" class=\"data row9 col90\" >3.179527</td>\n",
              "      <td id=\"T_4b03d_row9_col91\" class=\"data row9 col91\" >68.437379</td>\n",
              "      <td id=\"T_4b03d_row9_col92\" class=\"data row9 col92\" >19.658782</td>\n",
              "      <td id=\"T_4b03d_row9_col93\" class=\"data row9 col93\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row9_col94\" class=\"data row9 col94\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row9_col95\" class=\"data row9 col95\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row9_col96\" class=\"data row9 col96\" >35.091121</td>\n",
              "      <td id=\"T_4b03d_row9_col97\" class=\"data row9 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row9_col98\" class=\"data row9 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row9_col99\" class=\"data row9 col99\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row9_col100\" class=\"data row9 col100\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row9_col101\" class=\"data row9 col101\" >63.241566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_4b03d_row10_col0\" class=\"data row10 col0\" >Ring Modification</td>\n",
              "      <td id=\"T_4b03d_row10_col1\" class=\"data row10 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@@](C)(OC(C)=O)[C@H](O)C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row10_col2\" class=\"data row10 col2\" >2.590862</td>\n",
              "      <td id=\"T_4b03d_row10_col3\" class=\"data row10 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@@](C)(OC(C)=O)[C@H](O)C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row10_col4\" class=\"data row10 col4\" >855.934000</td>\n",
              "      <td id=\"T_4b03d_row10_col5\" class=\"data row10 col5\" >3.717700</td>\n",
              "      <td id=\"T_4b03d_row10_col6\" class=\"data row10 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row10_col7\" class=\"data row10 col7\" >5</td>\n",
              "      <td id=\"T_4b03d_row10_col8\" class=\"data row10 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row10_col9\" class=\"data row10 col9\" >0.111243</td>\n",
              "      <td id=\"T_4b03d_row10_col10\" class=\"data row10 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row10_col11\" class=\"data row10 col11\" >232.290000</td>\n",
              "      <td id=\"T_4b03d_row10_col12\" class=\"data row10 col12\" >0.252834</td>\n",
              "      <td id=\"T_4b03d_row10_col13\" class=\"data row10 col13\" >0.339033</td>\n",
              "      <td id=\"T_4b03d_row10_col14\" class=\"data row10 col14\" >0.720982</td>\n",
              "      <td id=\"T_4b03d_row10_col15\" class=\"data row10 col15\" >0.009973</td>\n",
              "      <td id=\"T_4b03d_row10_col16\" class=\"data row10 col16\" >0.049978</td>\n",
              "      <td id=\"T_4b03d_row10_col17\" class=\"data row10 col17\" >0.053044</td>\n",
              "      <td id=\"T_4b03d_row10_col18\" class=\"data row10 col18\" >0.116874</td>\n",
              "      <td id=\"T_4b03d_row10_col19\" class=\"data row10 col19\" >0.023047</td>\n",
              "      <td id=\"T_4b03d_row10_col20\" class=\"data row10 col20\" >0.063729</td>\n",
              "      <td id=\"T_4b03d_row10_col21\" class=\"data row10 col21\" >0.643861</td>\n",
              "      <td id=\"T_4b03d_row10_col22\" class=\"data row10 col22\" >0.529407</td>\n",
              "      <td id=\"T_4b03d_row10_col23\" class=\"data row10 col23\" >0.113396</td>\n",
              "      <td id=\"T_4b03d_row10_col24\" class=\"data row10 col24\" >0.225275</td>\n",
              "      <td id=\"T_4b03d_row10_col25\" class=\"data row10 col25\" >0.743966</td>\n",
              "      <td id=\"T_4b03d_row10_col26\" class=\"data row10 col26\" >0.978592</td>\n",
              "      <td id=\"T_4b03d_row10_col27\" class=\"data row10 col27\" >0.245964</td>\n",
              "      <td id=\"T_4b03d_row10_col28\" class=\"data row10 col28\" >0.452014</td>\n",
              "      <td id=\"T_4b03d_row10_col29\" class=\"data row10 col29\" >0.048844</td>\n",
              "      <td id=\"T_4b03d_row10_col30\" class=\"data row10 col30\" >0.176582</td>\n",
              "      <td id=\"T_4b03d_row10_col31\" class=\"data row10 col31\" >0.020565</td>\n",
              "      <td id=\"T_4b03d_row10_col32\" class=\"data row10 col32\" >0.173009</td>\n",
              "      <td id=\"T_4b03d_row10_col33\" class=\"data row10 col33\" >0.040746</td>\n",
              "      <td id=\"T_4b03d_row10_col34\" class=\"data row10 col34\" >0.502376</td>\n",
              "      <td id=\"T_4b03d_row10_col35\" class=\"data row10 col35\" >0.772962</td>\n",
              "      <td id=\"T_4b03d_row10_col36\" class=\"data row10 col36\" >0.186875</td>\n",
              "      <td id=\"T_4b03d_row10_col37\" class=\"data row10 col37\" >0.150437</td>\n",
              "      <td id=\"T_4b03d_row10_col38\" class=\"data row10 col38\" >0.028314</td>\n",
              "      <td id=\"T_4b03d_row10_col39\" class=\"data row10 col39\" >0.858301</td>\n",
              "      <td id=\"T_4b03d_row10_col40\" class=\"data row10 col40\" >0.162694</td>\n",
              "      <td id=\"T_4b03d_row10_col41\" class=\"data row10 col41\" >0.064394</td>\n",
              "      <td id=\"T_4b03d_row10_col42\" class=\"data row10 col42\" >0.558520</td>\n",
              "      <td id=\"T_4b03d_row10_col43\" class=\"data row10 col43\" >-5.462139</td>\n",
              "      <td id=\"T_4b03d_row10_col44\" class=\"data row10 col44\" >127.107043</td>\n",
              "      <td id=\"T_4b03d_row10_col45\" class=\"data row10 col45\" >96.942095</td>\n",
              "      <td id=\"T_4b03d_row10_col46\" class=\"data row10 col46\" >49.592098</td>\n",
              "      <td id=\"T_4b03d_row10_col47\" class=\"data row10 col47\" >-10.944226</td>\n",
              "      <td id=\"T_4b03d_row10_col48\" class=\"data row10 col48\" >4.000161</td>\n",
              "      <td id=\"T_4b03d_row10_col49\" class=\"data row10 col49\" >2.590862</td>\n",
              "      <td id=\"T_4b03d_row10_col50\" class=\"data row10 col50\" >100.777685</td>\n",
              "      <td id=\"T_4b03d_row10_col51\" class=\"data row10 col51\" >-5.923362</td>\n",
              "      <td id=\"T_4b03d_row10_col52\" class=\"data row10 col52\" >2.250534</td>\n",
              "      <td id=\"T_4b03d_row10_col53\" class=\"data row10 col53\" >95.191935</td>\n",
              "      <td id=\"T_4b03d_row10_col54\" class=\"data row10 col54\" >72.974021</td>\n",
              "      <td id=\"T_4b03d_row10_col55\" class=\"data row10 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row10_col56\" class=\"data row10 col56\" >90.752230</td>\n",
              "      <td id=\"T_4b03d_row10_col57\" class=\"data row10 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row10_col58\" class=\"data row10 col58\" >6.242730</td>\n",
              "      <td id=\"T_4b03d_row10_col59\" class=\"data row10 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row10_col60\" class=\"data row10 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row10_col61\" class=\"data row10 col61\" >60.721210</td>\n",
              "      <td id=\"T_4b03d_row10_col62\" class=\"data row10 col62\" >20.434277</td>\n",
              "      <td id=\"T_4b03d_row10_col63\" class=\"data row10 col63\" >39.356340</td>\n",
              "      <td id=\"T_4b03d_row10_col64\" class=\"data row10 col64\" >33.772780</td>\n",
              "      <td id=\"T_4b03d_row10_col65\" class=\"data row10 col65\" >38.076774</td>\n",
              "      <td id=\"T_4b03d_row10_col66\" class=\"data row10 col66\" >19.271035</td>\n",
              "      <td id=\"T_4b03d_row10_col67\" class=\"data row10 col67\" >67.894533</td>\n",
              "      <td id=\"T_4b03d_row10_col68\" class=\"data row10 col68\" >16.905777</td>\n",
              "      <td id=\"T_4b03d_row10_col69\" class=\"data row10 col69\" >54.556029</td>\n",
              "      <td id=\"T_4b03d_row10_col70\" class=\"data row10 col70\" >66.304769</td>\n",
              "      <td id=\"T_4b03d_row10_col71\" class=\"data row10 col71\" >81.116712</td>\n",
              "      <td id=\"T_4b03d_row10_col72\" class=\"data row10 col72\" >41.566499</td>\n",
              "      <td id=\"T_4b03d_row10_col73\" class=\"data row10 col73\" >69.096549</td>\n",
              "      <td id=\"T_4b03d_row10_col74\" class=\"data row10 col74\" >67.429236</td>\n",
              "      <td id=\"T_4b03d_row10_col75\" class=\"data row10 col75\" >35.827840</td>\n",
              "      <td id=\"T_4b03d_row10_col76\" class=\"data row10 col76\" >93.563397</td>\n",
              "      <td id=\"T_4b03d_row10_col77\" class=\"data row10 col77\" >94.610314</td>\n",
              "      <td id=\"T_4b03d_row10_col78\" class=\"data row10 col78\" >60.643660</td>\n",
              "      <td id=\"T_4b03d_row10_col79\" class=\"data row10 col79\" >83.132997</td>\n",
              "      <td id=\"T_4b03d_row10_col80\" class=\"data row10 col80\" >54.207057</td>\n",
              "      <td id=\"T_4b03d_row10_col81\" class=\"data row10 col81\" >75.339279</td>\n",
              "      <td id=\"T_4b03d_row10_col82\" class=\"data row10 col82\" >81.271811</td>\n",
              "      <td id=\"T_4b03d_row10_col83\" class=\"data row10 col83\" >34.238077</td>\n",
              "      <td id=\"T_4b03d_row10_col84\" class=\"data row10 col84\" >86.971694</td>\n",
              "      <td id=\"T_4b03d_row10_col85\" class=\"data row10 col85\" >59.596743</td>\n",
              "      <td id=\"T_4b03d_row10_col86\" class=\"data row10 col86\" >91.702210</td>\n",
              "      <td id=\"T_4b03d_row10_col87\" class=\"data row10 col87\" >64.133385</td>\n",
              "      <td id=\"T_4b03d_row10_col88\" class=\"data row10 col88\" >96.742924</td>\n",
              "      <td id=\"T_4b03d_row10_col89\" class=\"data row10 col89\" >82.086080</td>\n",
              "      <td id=\"T_4b03d_row10_col90\" class=\"data row10 col90\" >1.706088</td>\n",
              "      <td id=\"T_4b03d_row10_col91\" class=\"data row10 col91\" >67.002714</td>\n",
              "      <td id=\"T_4b03d_row10_col92\" class=\"data row10 col92\" >24.660721</td>\n",
              "      <td id=\"T_4b03d_row10_col93\" class=\"data row10 col93\" >96.432726</td>\n",
              "      <td id=\"T_4b03d_row10_col94\" class=\"data row10 col94\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row10_col95\" class=\"data row10 col95\" >87.049244</td>\n",
              "      <td id=\"T_4b03d_row10_col96\" class=\"data row10 col96\" >36.913532</td>\n",
              "      <td id=\"T_4b03d_row10_col97\" class=\"data row10 col97\" >97.518418</td>\n",
              "      <td id=\"T_4b03d_row10_col98\" class=\"data row10 col98\" >72.043428</td>\n",
              "      <td id=\"T_4b03d_row10_col99\" class=\"data row10 col99\" >92.012408</td>\n",
              "      <td id=\"T_4b03d_row10_col100\" class=\"data row10 col100\" >5.854983</td>\n",
              "      <td id=\"T_4b03d_row10_col101\" class=\"data row10 col101\" >56.107018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_4b03d_row11_col0\" class=\"data row11 col0\" >Substructure Replacement</td>\n",
              "      <td id=\"T_4b03d_row11_col1\" class=\"data row11 col1\" >CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H](N)C[C@H]3OC[C@@]3(OC(C)=O)[C@H]2[C@H](OC(=O)c2ccccc2)[C@]2(N)C[C@H](OC(=O)[C@@H](N)[C@@H](NC(=O)c3ccccc3)c3ccccc3)C(C)=C1C2(C)C</td>\n",
              "      <td id=\"T_4b03d_row11_col2\" class=\"data row11 col2\" >1.933215</td>\n",
              "      <td id=\"T_4b03d_row11_col3\" class=\"data row11 col3\" >CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H](N)C[C@H]3OC[C@@]3(OC(C)=O)[C@H]2[C@H](OC(=O)c2ccccc2)[C@]2(N)C[C@H](OC(=O)[C@@H](N)[C@@H](NC(=O)c3ccccc3)c3ccccc3)C(C)=C1C2(C)C</td>\n",
              "      <td id=\"T_4b03d_row11_col4\" class=\"data row11 col4\" >850.966000</td>\n",
              "      <td id=\"T_4b03d_row11_col5\" class=\"data row11 col5\" >3.634900</td>\n",
              "      <td id=\"T_4b03d_row11_col6\" class=\"data row11 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row11_col7\" class=\"data row11 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row11_col8\" class=\"data row11 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row11_col9\" class=\"data row11 col9\" >0.129542</td>\n",
              "      <td id=\"T_4b03d_row11_col10\" class=\"data row11 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row11_col11\" class=\"data row11 col11\" >238.660000</td>\n",
              "      <td id=\"T_4b03d_row11_col12\" class=\"data row11 col12\" >0.557778</td>\n",
              "      <td id=\"T_4b03d_row11_col13\" class=\"data row11 col13\" >0.467080</td>\n",
              "      <td id=\"T_4b03d_row11_col14\" class=\"data row11 col14\" >0.834084</td>\n",
              "      <td id=\"T_4b03d_row11_col15\" class=\"data row11 col15\" >0.011681</td>\n",
              "      <td id=\"T_4b03d_row11_col16\" class=\"data row11 col16\" >0.091479</td>\n",
              "      <td id=\"T_4b03d_row11_col17\" class=\"data row11 col17\" >0.038012</td>\n",
              "      <td id=\"T_4b03d_row11_col18\" class=\"data row11 col18\" >0.119117</td>\n",
              "      <td id=\"T_4b03d_row11_col19\" class=\"data row11 col19\" >0.054424</td>\n",
              "      <td id=\"T_4b03d_row11_col20\" class=\"data row11 col20\" >0.152637</td>\n",
              "      <td id=\"T_4b03d_row11_col21\" class=\"data row11 col21\" >0.649458</td>\n",
              "      <td id=\"T_4b03d_row11_col22\" class=\"data row11 col22\" >0.889251</td>\n",
              "      <td id=\"T_4b03d_row11_col23\" class=\"data row11 col23\" >0.137266</td>\n",
              "      <td id=\"T_4b03d_row11_col24\" class=\"data row11 col24\" >0.632797</td>\n",
              "      <td id=\"T_4b03d_row11_col25\" class=\"data row11 col25\" >0.721369</td>\n",
              "      <td id=\"T_4b03d_row11_col26\" class=\"data row11 col26\" >0.978758</td>\n",
              "      <td id=\"T_4b03d_row11_col27\" class=\"data row11 col27\" >0.152563</td>\n",
              "      <td id=\"T_4b03d_row11_col28\" class=\"data row11 col28\" >0.293198</td>\n",
              "      <td id=\"T_4b03d_row11_col29\" class=\"data row11 col29\" >0.072912</td>\n",
              "      <td id=\"T_4b03d_row11_col30\" class=\"data row11 col30\" >0.201601</td>\n",
              "      <td id=\"T_4b03d_row11_col31\" class=\"data row11 col31\" >0.040696</td>\n",
              "      <td id=\"T_4b03d_row11_col32\" class=\"data row11 col32\" >0.159001</td>\n",
              "      <td id=\"T_4b03d_row11_col33\" class=\"data row11 col33\" >0.101000</td>\n",
              "      <td id=\"T_4b03d_row11_col34\" class=\"data row11 col34\" >0.555813</td>\n",
              "      <td id=\"T_4b03d_row11_col35\" class=\"data row11 col35\" >0.661817</td>\n",
              "      <td id=\"T_4b03d_row11_col36\" class=\"data row11 col36\" >0.297911</td>\n",
              "      <td id=\"T_4b03d_row11_col37\" class=\"data row11 col37\" >0.341749</td>\n",
              "      <td id=\"T_4b03d_row11_col38\" class=\"data row11 col38\" >0.033292</td>\n",
              "      <td id=\"T_4b03d_row11_col39\" class=\"data row11 col39\" >0.799097</td>\n",
              "      <td id=\"T_4b03d_row11_col40\" class=\"data row11 col40\" >0.313653</td>\n",
              "      <td id=\"T_4b03d_row11_col41\" class=\"data row11 col41\" >0.135045</td>\n",
              "      <td id=\"T_4b03d_row11_col42\" class=\"data row11 col42\" >0.725760</td>\n",
              "      <td id=\"T_4b03d_row11_col43\" class=\"data row11 col43\" >-5.927420</td>\n",
              "      <td id=\"T_4b03d_row11_col44\" class=\"data row11 col44\" >117.749788</td>\n",
              "      <td id=\"T_4b03d_row11_col45\" class=\"data row11 col45\" >87.181244</td>\n",
              "      <td id=\"T_4b03d_row11_col46\" class=\"data row11 col46\" >44.967471</td>\n",
              "      <td id=\"T_4b03d_row11_col47\" class=\"data row11 col47\" >-9.867604</td>\n",
              "      <td id=\"T_4b03d_row11_col48\" class=\"data row11 col48\" >4.250175</td>\n",
              "      <td id=\"T_4b03d_row11_col49\" class=\"data row11 col49\" >1.933215</td>\n",
              "      <td id=\"T_4b03d_row11_col50\" class=\"data row11 col50\" >91.508862</td>\n",
              "      <td id=\"T_4b03d_row11_col51\" class=\"data row11 col51\" >-5.625337</td>\n",
              "      <td id=\"T_4b03d_row11_col52\" class=\"data row11 col52\" >5.150765</td>\n",
              "      <td id=\"T_4b03d_row11_col53\" class=\"data row11 col53\" >95.114385</td>\n",
              "      <td id=\"T_4b03d_row11_col54\" class=\"data row11 col54\" >71.849554</td>\n",
              "      <td id=\"T_4b03d_row11_col55\" class=\"data row11 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row11_col56\" class=\"data row11 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row11_col57\" class=\"data row11 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row11_col58\" class=\"data row11 col58\" >7.134548</td>\n",
              "      <td id=\"T_4b03d_row11_col59\" class=\"data row11 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row11_col60\" class=\"data row11 col60\" >94.261342</td>\n",
              "      <td id=\"T_4b03d_row11_col61\" class=\"data row11 col61\" >86.118651</td>\n",
              "      <td id=\"T_4b03d_row11_col62\" class=\"data row11 col62\" >28.538193</td>\n",
              "      <td id=\"T_4b03d_row11_col63\" class=\"data row11 col63\" >57.309035</td>\n",
              "      <td id=\"T_4b03d_row11_col64\" class=\"data row11 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row11_col65\" class=\"data row11 col65\" >48.003102</td>\n",
              "      <td id=\"T_4b03d_row11_col66\" class=\"data row11 col66\" >13.726250</td>\n",
              "      <td id=\"T_4b03d_row11_col67\" class=\"data row11 col67\" >68.321055</td>\n",
              "      <td id=\"T_4b03d_row11_col68\" class=\"data row11 col68\" >35.595192</td>\n",
              "      <td id=\"T_4b03d_row11_col69\" class=\"data row11 col69\" >69.445522</td>\n",
              "      <td id=\"T_4b03d_row11_col70\" class=\"data row11 col70\" >66.963940</td>\n",
              "      <td id=\"T_4b03d_row11_col71\" class=\"data row11 col71\" >94.649089</td>\n",
              "      <td id=\"T_4b03d_row11_col72\" class=\"data row11 col72\" >46.529663</td>\n",
              "      <td id=\"T_4b03d_row11_col73\" class=\"data row11 col73\" >95.036836</td>\n",
              "      <td id=\"T_4b03d_row11_col74\" class=\"data row11 col74\" >65.839473</td>\n",
              "      <td id=\"T_4b03d_row11_col75\" class=\"data row11 col75\" >35.827840</td>\n",
              "      <td id=\"T_4b03d_row11_col76\" class=\"data row11 col76\" >92.400155</td>\n",
              "      <td id=\"T_4b03d_row11_col77\" class=\"data row11 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row11_col78\" class=\"data row11 col78\" >66.886390</td>\n",
              "      <td id=\"T_4b03d_row11_col79\" class=\"data row11 col79\" >85.459480</td>\n",
              "      <td id=\"T_4b03d_row11_col80\" class=\"data row11 col80\" >71.733230</td>\n",
              "      <td id=\"T_4b03d_row11_col81\" class=\"data row11 col81\" >72.857697</td>\n",
              "      <td id=\"T_4b03d_row11_col82\" class=\"data row11 col82\" >91.353238</td>\n",
              "      <td id=\"T_4b03d_row11_col83\" class=\"data row11 col83\" >36.797208</td>\n",
              "      <td id=\"T_4b03d_row11_col84\" class=\"data row11 col84\" >82.435052</td>\n",
              "      <td id=\"T_4b03d_row11_col85\" class=\"data row11 col85\" >71.229159</td>\n",
              "      <td id=\"T_4b03d_row11_col86\" class=\"data row11 col86\" >97.285770</td>\n",
              "      <td id=\"T_4b03d_row11_col87\" class=\"data row11 col87\" >67.506786</td>\n",
              "      <td id=\"T_4b03d_row11_col88\" class=\"data row11 col88\" >95.269484</td>\n",
              "      <td id=\"T_4b03d_row11_col89\" class=\"data row11 col89\" >90.965491</td>\n",
              "      <td id=\"T_4b03d_row11_col90\" class=\"data row11 col90\" >10.973245</td>\n",
              "      <td id=\"T_4b03d_row11_col91\" class=\"data row11 col91\" >73.710741</td>\n",
              "      <td id=\"T_4b03d_row11_col92\" class=\"data row11 col92\" >13.493602</td>\n",
              "      <td id=\"T_4b03d_row11_col93\" class=\"data row11 col93\" >94.920512</td>\n",
              "      <td id=\"T_4b03d_row11_col94\" class=\"data row11 col94\" >93.098100</td>\n",
              "      <td id=\"T_4b03d_row11_col95\" class=\"data row11 col95\" >85.110508</td>\n",
              "      <td id=\"T_4b03d_row11_col96\" class=\"data row11 col96\" >47.731679</td>\n",
              "      <td id=\"T_4b03d_row11_col97\" class=\"data row11 col97\" >98.797984</td>\n",
              "      <td id=\"T_4b03d_row11_col98\" class=\"data row11 col98\" >58.860023</td>\n",
              "      <td id=\"T_4b03d_row11_col99\" class=\"data row11 col99\" >76.386196</td>\n",
              "      <td id=\"T_4b03d_row11_col100\" class=\"data row11 col100\" >7.754944</td>\n",
              "      <td id=\"T_4b03d_row11_col101\" class=\"data row11 col101\" >75.571927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_4b03d_row12_col0\" class=\"data row12 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row12_col1\" class=\"data row12 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](NC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](OC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row12_col2\" class=\"data row12 col2\" >2.582616</td>\n",
              "      <td id=\"T_4b03d_row12_col3\" class=\"data row12 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@H](NC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](OC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row12_col4\" class=\"data row12 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row12_col5\" class=\"data row12 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row12_col6\" class=\"data row12 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row12_col7\" class=\"data row12 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row12_col8\" class=\"data row12 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row12_col9\" class=\"data row12 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row12_col10\" class=\"data row12 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row12_col11\" class=\"data row12 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row12_col12\" class=\"data row12 col12\" >0.373621</td>\n",
              "      <td id=\"T_4b03d_row12_col13\" class=\"data row12 col13\" >0.360248</td>\n",
              "      <td id=\"T_4b03d_row12_col14\" class=\"data row12 col14\" >0.681673</td>\n",
              "      <td id=\"T_4b03d_row12_col15\" class=\"data row12 col15\" >0.008400</td>\n",
              "      <td id=\"T_4b03d_row12_col16\" class=\"data row12 col16\" >0.061486</td>\n",
              "      <td id=\"T_4b03d_row12_col17\" class=\"data row12 col17\" >0.041370</td>\n",
              "      <td id=\"T_4b03d_row12_col18\" class=\"data row12 col18\" >0.119724</td>\n",
              "      <td id=\"T_4b03d_row12_col19\" class=\"data row12 col19\" >0.024227</td>\n",
              "      <td id=\"T_4b03d_row12_col20\" class=\"data row12 col20\" >0.071439</td>\n",
              "      <td id=\"T_4b03d_row12_col21\" class=\"data row12 col21\" >0.634315</td>\n",
              "      <td id=\"T_4b03d_row12_col22\" class=\"data row12 col22\" >0.647176</td>\n",
              "      <td id=\"T_4b03d_row12_col23\" class=\"data row12 col23\" >0.083745</td>\n",
              "      <td id=\"T_4b03d_row12_col24\" class=\"data row12 col24\" >0.335851</td>\n",
              "      <td id=\"T_4b03d_row12_col25\" class=\"data row12 col25\" >0.787976</td>\n",
              "      <td id=\"T_4b03d_row12_col26\" class=\"data row12 col26\" >0.982662</td>\n",
              "      <td id=\"T_4b03d_row12_col27\" class=\"data row12 col27\" >0.195603</td>\n",
              "      <td id=\"T_4b03d_row12_col28\" class=\"data row12 col28\" >0.400982</td>\n",
              "      <td id=\"T_4b03d_row12_col29\" class=\"data row12 col29\" >0.053546</td>\n",
              "      <td id=\"T_4b03d_row12_col30\" class=\"data row12 col30\" >0.253153</td>\n",
              "      <td id=\"T_4b03d_row12_col31\" class=\"data row12 col31\" >0.024913</td>\n",
              "      <td id=\"T_4b03d_row12_col32\" class=\"data row12 col32\" >0.178538</td>\n",
              "      <td id=\"T_4b03d_row12_col33\" class=\"data row12 col33\" >0.072778</td>\n",
              "      <td id=\"T_4b03d_row12_col34\" class=\"data row12 col34\" >0.515746</td>\n",
              "      <td id=\"T_4b03d_row12_col35\" class=\"data row12 col35\" >0.749279</td>\n",
              "      <td id=\"T_4b03d_row12_col36\" class=\"data row12 col36\" >0.292104</td>\n",
              "      <td id=\"T_4b03d_row12_col37\" class=\"data row12 col37\" >0.242235</td>\n",
              "      <td id=\"T_4b03d_row12_col38\" class=\"data row12 col38\" >0.037757</td>\n",
              "      <td id=\"T_4b03d_row12_col39\" class=\"data row12 col39\" >0.897167</td>\n",
              "      <td id=\"T_4b03d_row12_col40\" class=\"data row12 col40\" >0.288761</td>\n",
              "      <td id=\"T_4b03d_row12_col41\" class=\"data row12 col41\" >0.087051</td>\n",
              "      <td id=\"T_4b03d_row12_col42\" class=\"data row12 col42\" >0.599480</td>\n",
              "      <td id=\"T_4b03d_row12_col43\" class=\"data row12 col43\" >-5.622192</td>\n",
              "      <td id=\"T_4b03d_row12_col44\" class=\"data row12 col44\" >126.767149</td>\n",
              "      <td id=\"T_4b03d_row12_col45\" class=\"data row12 col45\" >97.644773</td>\n",
              "      <td id=\"T_4b03d_row12_col46\" class=\"data row12 col46\" >40.511506</td>\n",
              "      <td id=\"T_4b03d_row12_col47\" class=\"data row12 col47\" >-10.966529</td>\n",
              "      <td id=\"T_4b03d_row12_col48\" class=\"data row12 col48\" >4.314676</td>\n",
              "      <td id=\"T_4b03d_row12_col49\" class=\"data row12 col49\" >2.582617</td>\n",
              "      <td id=\"T_4b03d_row12_col50\" class=\"data row12 col50\" >100.220959</td>\n",
              "      <td id=\"T_4b03d_row12_col51\" class=\"data row12 col51\" >-5.869617</td>\n",
              "      <td id=\"T_4b03d_row12_col52\" class=\"data row12 col52\" >3.145149</td>\n",
              "      <td id=\"T_4b03d_row12_col53\" class=\"data row12 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row12_col54\" class=\"data row12 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row12_col55\" class=\"data row12 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row12_col56\" class=\"data row12 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row12_col57\" class=\"data row12 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row12_col58\" class=\"data row12 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row12_col59\" class=\"data row12 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row12_col60\" class=\"data row12 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row12_col61\" class=\"data row12 col61\" >73.710741</td>\n",
              "      <td id=\"T_4b03d_row12_col62\" class=\"data row12 col62\" >21.907716</td>\n",
              "      <td id=\"T_4b03d_row12_col63\" class=\"data row12 col63\" >33.734005</td>\n",
              "      <td id=\"T_4b03d_row12_col64\" class=\"data row12 col64\" >31.601396</td>\n",
              "      <td id=\"T_4b03d_row12_col65\" class=\"data row12 col65\" >41.527724</td>\n",
              "      <td id=\"T_4b03d_row12_col66\" class=\"data row12 col66\" >15.277239</td>\n",
              "      <td id=\"T_4b03d_row12_col67\" class=\"data row12 col67\" >68.398604</td>\n",
              "      <td id=\"T_4b03d_row12_col68\" class=\"data row12 col68\" >17.913920</td>\n",
              "      <td id=\"T_4b03d_row12_col69\" class=\"data row12 col69\" >56.533540</td>\n",
              "      <td id=\"T_4b03d_row12_col70\" class=\"data row12 col70\" >64.831330</td>\n",
              "      <td id=\"T_4b03d_row12_col71\" class=\"data row12 col71\" >84.877860</td>\n",
              "      <td id=\"T_4b03d_row12_col72\" class=\"data row12 col72\" >32.531989</td>\n",
              "      <td id=\"T_4b03d_row12_col73\" class=\"data row12 col73\" >78.712679</td>\n",
              "      <td id=\"T_4b03d_row12_col74\" class=\"data row12 col74\" >70.686313</td>\n",
              "      <td id=\"T_4b03d_row12_col75\" class=\"data row12 col75\" >37.417604</td>\n",
              "      <td id=\"T_4b03d_row12_col76\" class=\"data row12 col76\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row12_col77\" class=\"data row12 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row12_col78\" class=\"data row12 col78\" >62.117100</td>\n",
              "      <td id=\"T_4b03d_row12_col79\" class=\"data row12 col79\" >88.871656</td>\n",
              "      <td id=\"T_4b03d_row12_col80\" class=\"data row12 col80\" >59.713067</td>\n",
              "      <td id=\"T_4b03d_row12_col81\" class=\"data row12 col81\" >76.541295</td>\n",
              "      <td id=\"T_4b03d_row12_col82\" class=\"data row12 col82\" >88.290035</td>\n",
              "      <td id=\"T_4b03d_row12_col83\" class=\"data row12 col83\" >34.936022</td>\n",
              "      <td id=\"T_4b03d_row12_col84\" class=\"data row12 col84\" >85.924777</td>\n",
              "      <td id=\"T_4b03d_row12_col85\" class=\"data row12 col85\" >70.841411</td>\n",
              "      <td id=\"T_4b03d_row12_col86\" class=\"data row12 col86\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row12_col87\" class=\"data row12 col87\" >69.910818</td>\n",
              "      <td id=\"T_4b03d_row12_col88\" class=\"data row12 col88\" >97.634742</td>\n",
              "      <td id=\"T_4b03d_row12_col89\" class=\"data row12 col89\" >89.918573</td>\n",
              "      <td id=\"T_4b03d_row12_col90\" class=\"data row12 col90\" >3.916247</td>\n",
              "      <td id=\"T_4b03d_row12_col91\" class=\"data row12 col91\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row12_col92\" class=\"data row12 col92\" >20.201629</td>\n",
              "      <td id=\"T_4b03d_row12_col93\" class=\"data row12 col93\" >96.316402</td>\n",
              "      <td id=\"T_4b03d_row12_col94\" class=\"data row12 col94\" >95.308259</td>\n",
              "      <td id=\"T_4b03d_row12_col95\" class=\"data row12 col95\" >82.667701</td>\n",
              "      <td id=\"T_4b03d_row12_col96\" class=\"data row12 col96\" >36.719659</td>\n",
              "      <td id=\"T_4b03d_row12_col97\" class=\"data row12 col97\" >99.069407</td>\n",
              "      <td id=\"T_4b03d_row12_col98\" class=\"data row12 col98\" >71.927104</td>\n",
              "      <td id=\"T_4b03d_row12_col99\" class=\"data row12 col99\" >91.314463</td>\n",
              "      <td id=\"T_4b03d_row12_col100\" class=\"data row12 col100\" >6.126406</td>\n",
              "      <td id=\"T_4b03d_row12_col101\" class=\"data row12 col101\" >62.000775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_4b03d_row13_col0\" class=\"data row13 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row13_col1\" class=\"data row13 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row13_col2\" class=\"data row13 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row13_col3\" class=\"data row13 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row13_col4\" class=\"data row13 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row13_col5\" class=\"data row13 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row13_col6\" class=\"data row13 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row13_col7\" class=\"data row13 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row13_col8\" class=\"data row13 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row13_col9\" class=\"data row13 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row13_col10\" class=\"data row13 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row13_col11\" class=\"data row13 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row13_col12\" class=\"data row13 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row13_col13\" class=\"data row13 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row13_col14\" class=\"data row13 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row13_col15\" class=\"data row13 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row13_col16\" class=\"data row13 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row13_col17\" class=\"data row13 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row13_col18\" class=\"data row13 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row13_col19\" class=\"data row13 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row13_col20\" class=\"data row13 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row13_col21\" class=\"data row13 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row13_col22\" class=\"data row13 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row13_col23\" class=\"data row13 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row13_col24\" class=\"data row13 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row13_col25\" class=\"data row13 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row13_col26\" class=\"data row13 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row13_col27\" class=\"data row13 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row13_col28\" class=\"data row13 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row13_col29\" class=\"data row13 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row13_col30\" class=\"data row13 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row13_col31\" class=\"data row13 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row13_col32\" class=\"data row13 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row13_col33\" class=\"data row13 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row13_col34\" class=\"data row13 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row13_col35\" class=\"data row13 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row13_col36\" class=\"data row13 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row13_col37\" class=\"data row13 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row13_col38\" class=\"data row13 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row13_col39\" class=\"data row13 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row13_col40\" class=\"data row13 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row13_col41\" class=\"data row13 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row13_col42\" class=\"data row13 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row13_col43\" class=\"data row13 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row13_col44\" class=\"data row13 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row13_col45\" class=\"data row13 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row13_col46\" class=\"data row13 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row13_col47\" class=\"data row13 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row13_col48\" class=\"data row13 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row13_col49\" class=\"data row13 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row13_col50\" class=\"data row13 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row13_col51\" class=\"data row13 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row13_col52\" class=\"data row13 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row13_col53\" class=\"data row13 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row13_col54\" class=\"data row13 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row13_col55\" class=\"data row13 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row13_col56\" class=\"data row13 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row13_col57\" class=\"data row13 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row13_col58\" class=\"data row13 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row13_col59\" class=\"data row13 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row13_col60\" class=\"data row13 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row13_col61\" class=\"data row13 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row13_col62\" class=\"data row13 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row13_col63\" class=\"data row13 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row13_col64\" class=\"data row13 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row13_col65\" class=\"data row13 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row13_col66\" class=\"data row13 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row13_col67\" class=\"data row13 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row13_col68\" class=\"data row13 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row13_col69\" class=\"data row13 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row13_col70\" class=\"data row13 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row13_col71\" class=\"data row13 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row13_col72\" class=\"data row13 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row13_col73\" class=\"data row13 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row13_col74\" class=\"data row13 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row13_col75\" class=\"data row13 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row13_col76\" class=\"data row13 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row13_col77\" class=\"data row13 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row13_col78\" class=\"data row13 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row13_col79\" class=\"data row13 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row13_col80\" class=\"data row13 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row13_col81\" class=\"data row13 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row13_col82\" class=\"data row13 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row13_col83\" class=\"data row13 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row13_col84\" class=\"data row13 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row13_col85\" class=\"data row13 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row13_col86\" class=\"data row13 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row13_col87\" class=\"data row13 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row13_col88\" class=\"data row13 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row13_col89\" class=\"data row13 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row13_col90\" class=\"data row13 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row13_col91\" class=\"data row13 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row13_col92\" class=\"data row13 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row13_col93\" class=\"data row13 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row13_col94\" class=\"data row13 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row13_col95\" class=\"data row13 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row13_col96\" class=\"data row13 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row13_col97\" class=\"data row13 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row13_col98\" class=\"data row13 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row13_col99\" class=\"data row13 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row13_col100\" class=\"data row13 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row13_col101\" class=\"data row13 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_4b03d_row14_col0\" class=\"data row14 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row14_col1\" class=\"data row14 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row14_col2\" class=\"data row14 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row14_col3\" class=\"data row14 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row14_col4\" class=\"data row14 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row14_col5\" class=\"data row14 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row14_col6\" class=\"data row14 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row14_col7\" class=\"data row14 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row14_col8\" class=\"data row14 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row14_col9\" class=\"data row14 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row14_col10\" class=\"data row14 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row14_col11\" class=\"data row14 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row14_col12\" class=\"data row14 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row14_col13\" class=\"data row14 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row14_col14\" class=\"data row14 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row14_col15\" class=\"data row14 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row14_col16\" class=\"data row14 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row14_col17\" class=\"data row14 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row14_col18\" class=\"data row14 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row14_col19\" class=\"data row14 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row14_col20\" class=\"data row14 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row14_col21\" class=\"data row14 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row14_col22\" class=\"data row14 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row14_col23\" class=\"data row14 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row14_col24\" class=\"data row14 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row14_col25\" class=\"data row14 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row14_col26\" class=\"data row14 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row14_col27\" class=\"data row14 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row14_col28\" class=\"data row14 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row14_col29\" class=\"data row14 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row14_col30\" class=\"data row14 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row14_col31\" class=\"data row14 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row14_col32\" class=\"data row14 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row14_col33\" class=\"data row14 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row14_col34\" class=\"data row14 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row14_col35\" class=\"data row14 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row14_col36\" class=\"data row14 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row14_col37\" class=\"data row14 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row14_col38\" class=\"data row14 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row14_col39\" class=\"data row14 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row14_col40\" class=\"data row14 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row14_col41\" class=\"data row14 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row14_col42\" class=\"data row14 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row14_col43\" class=\"data row14 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row14_col44\" class=\"data row14 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row14_col45\" class=\"data row14 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row14_col46\" class=\"data row14 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row14_col47\" class=\"data row14 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row14_col48\" class=\"data row14 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row14_col49\" class=\"data row14 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row14_col50\" class=\"data row14 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row14_col51\" class=\"data row14 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row14_col52\" class=\"data row14 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row14_col53\" class=\"data row14 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row14_col54\" class=\"data row14 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row14_col55\" class=\"data row14 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row14_col56\" class=\"data row14 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row14_col57\" class=\"data row14 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row14_col58\" class=\"data row14 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row14_col59\" class=\"data row14 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row14_col60\" class=\"data row14 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row14_col61\" class=\"data row14 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row14_col62\" class=\"data row14 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row14_col63\" class=\"data row14 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row14_col64\" class=\"data row14 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row14_col65\" class=\"data row14 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row14_col66\" class=\"data row14 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row14_col67\" class=\"data row14 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row14_col68\" class=\"data row14 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row14_col69\" class=\"data row14 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row14_col70\" class=\"data row14 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row14_col71\" class=\"data row14 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row14_col72\" class=\"data row14 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row14_col73\" class=\"data row14 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row14_col74\" class=\"data row14 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row14_col75\" class=\"data row14 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row14_col76\" class=\"data row14 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row14_col77\" class=\"data row14 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row14_col78\" class=\"data row14 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row14_col79\" class=\"data row14 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row14_col80\" class=\"data row14 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row14_col81\" class=\"data row14 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row14_col82\" class=\"data row14 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row14_col83\" class=\"data row14 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row14_col84\" class=\"data row14 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row14_col85\" class=\"data row14 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row14_col86\" class=\"data row14 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row14_col87\" class=\"data row14 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row14_col88\" class=\"data row14 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row14_col89\" class=\"data row14 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row14_col90\" class=\"data row14 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row14_col91\" class=\"data row14 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row14_col92\" class=\"data row14 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row14_col93\" class=\"data row14 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row14_col94\" class=\"data row14 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row14_col95\" class=\"data row14 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row14_col96\" class=\"data row14 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row14_col97\" class=\"data row14 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row14_col98\" class=\"data row14 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row14_col99\" class=\"data row14 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row14_col100\" class=\"data row14 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row14_col101\" class=\"data row14 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_4b03d_row15_col0\" class=\"data row15 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row15_col1\" class=\"data row15 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row15_col2\" class=\"data row15 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row15_col3\" class=\"data row15 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row15_col4\" class=\"data row15 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row15_col5\" class=\"data row15 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row15_col6\" class=\"data row15 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row15_col7\" class=\"data row15 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row15_col8\" class=\"data row15 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row15_col9\" class=\"data row15 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row15_col10\" class=\"data row15 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row15_col11\" class=\"data row15 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row15_col12\" class=\"data row15 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row15_col13\" class=\"data row15 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row15_col14\" class=\"data row15 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row15_col15\" class=\"data row15 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row15_col16\" class=\"data row15 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row15_col17\" class=\"data row15 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row15_col18\" class=\"data row15 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row15_col19\" class=\"data row15 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row15_col20\" class=\"data row15 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row15_col21\" class=\"data row15 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row15_col22\" class=\"data row15 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row15_col23\" class=\"data row15 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row15_col24\" class=\"data row15 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row15_col25\" class=\"data row15 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row15_col26\" class=\"data row15 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row15_col27\" class=\"data row15 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row15_col28\" class=\"data row15 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row15_col29\" class=\"data row15 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row15_col30\" class=\"data row15 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row15_col31\" class=\"data row15 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row15_col32\" class=\"data row15 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row15_col33\" class=\"data row15 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row15_col34\" class=\"data row15 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row15_col35\" class=\"data row15 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row15_col36\" class=\"data row15 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row15_col37\" class=\"data row15 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row15_col38\" class=\"data row15 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row15_col39\" class=\"data row15 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row15_col40\" class=\"data row15 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row15_col41\" class=\"data row15 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row15_col42\" class=\"data row15 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row15_col43\" class=\"data row15 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row15_col44\" class=\"data row15 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row15_col45\" class=\"data row15 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row15_col46\" class=\"data row15 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row15_col47\" class=\"data row15 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row15_col48\" class=\"data row15 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row15_col49\" class=\"data row15 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row15_col50\" class=\"data row15 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row15_col51\" class=\"data row15 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row15_col52\" class=\"data row15 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row15_col53\" class=\"data row15 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row15_col54\" class=\"data row15 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row15_col55\" class=\"data row15 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row15_col56\" class=\"data row15 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row15_col57\" class=\"data row15 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row15_col58\" class=\"data row15 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row15_col59\" class=\"data row15 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row15_col60\" class=\"data row15 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row15_col61\" class=\"data row15 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row15_col62\" class=\"data row15 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row15_col63\" class=\"data row15 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row15_col64\" class=\"data row15 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row15_col65\" class=\"data row15 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row15_col66\" class=\"data row15 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row15_col67\" class=\"data row15 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row15_col68\" class=\"data row15 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row15_col69\" class=\"data row15 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row15_col70\" class=\"data row15 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row15_col71\" class=\"data row15 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row15_col72\" class=\"data row15 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row15_col73\" class=\"data row15 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row15_col74\" class=\"data row15 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row15_col75\" class=\"data row15 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row15_col76\" class=\"data row15 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row15_col77\" class=\"data row15 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row15_col78\" class=\"data row15 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row15_col79\" class=\"data row15 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row15_col80\" class=\"data row15 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row15_col81\" class=\"data row15 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row15_col82\" class=\"data row15 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row15_col83\" class=\"data row15 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row15_col84\" class=\"data row15 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row15_col85\" class=\"data row15 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row15_col86\" class=\"data row15 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row15_col87\" class=\"data row15 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row15_col88\" class=\"data row15 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row15_col89\" class=\"data row15 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row15_col90\" class=\"data row15 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row15_col91\" class=\"data row15 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row15_col92\" class=\"data row15 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row15_col93\" class=\"data row15 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row15_col94\" class=\"data row15 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row15_col95\" class=\"data row15 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row15_col96\" class=\"data row15 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row15_col97\" class=\"data row15 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row15_col98\" class=\"data row15 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row15_col99\" class=\"data row15 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row15_col100\" class=\"data row15 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row15_col101\" class=\"data row15 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_4b03d_row16_col0\" class=\"data row16 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row16_col1\" class=\"data row16 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row16_col2\" class=\"data row16 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row16_col3\" class=\"data row16 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row16_col4\" class=\"data row16 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row16_col5\" class=\"data row16 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row16_col6\" class=\"data row16 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row16_col7\" class=\"data row16 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row16_col8\" class=\"data row16 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row16_col9\" class=\"data row16 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row16_col10\" class=\"data row16 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row16_col11\" class=\"data row16 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row16_col12\" class=\"data row16 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row16_col13\" class=\"data row16 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row16_col14\" class=\"data row16 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row16_col15\" class=\"data row16 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row16_col16\" class=\"data row16 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row16_col17\" class=\"data row16 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row16_col18\" class=\"data row16 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row16_col19\" class=\"data row16 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row16_col20\" class=\"data row16 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row16_col21\" class=\"data row16 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row16_col22\" class=\"data row16 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row16_col23\" class=\"data row16 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row16_col24\" class=\"data row16 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row16_col25\" class=\"data row16 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row16_col26\" class=\"data row16 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row16_col27\" class=\"data row16 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row16_col28\" class=\"data row16 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row16_col29\" class=\"data row16 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row16_col30\" class=\"data row16 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row16_col31\" class=\"data row16 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row16_col32\" class=\"data row16 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row16_col33\" class=\"data row16 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row16_col34\" class=\"data row16 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row16_col35\" class=\"data row16 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row16_col36\" class=\"data row16 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row16_col37\" class=\"data row16 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row16_col38\" class=\"data row16 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row16_col39\" class=\"data row16 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row16_col40\" class=\"data row16 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row16_col41\" class=\"data row16 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row16_col42\" class=\"data row16 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row16_col43\" class=\"data row16 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row16_col44\" class=\"data row16 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row16_col45\" class=\"data row16 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row16_col46\" class=\"data row16 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row16_col47\" class=\"data row16 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row16_col48\" class=\"data row16 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row16_col49\" class=\"data row16 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row16_col50\" class=\"data row16 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row16_col51\" class=\"data row16 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row16_col52\" class=\"data row16 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row16_col53\" class=\"data row16 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row16_col54\" class=\"data row16 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row16_col55\" class=\"data row16 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row16_col56\" class=\"data row16 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row16_col57\" class=\"data row16 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row16_col58\" class=\"data row16 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row16_col59\" class=\"data row16 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row16_col60\" class=\"data row16 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row16_col61\" class=\"data row16 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row16_col62\" class=\"data row16 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row16_col63\" class=\"data row16 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row16_col64\" class=\"data row16 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row16_col65\" class=\"data row16 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row16_col66\" class=\"data row16 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row16_col67\" class=\"data row16 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row16_col68\" class=\"data row16 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row16_col69\" class=\"data row16 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row16_col70\" class=\"data row16 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row16_col71\" class=\"data row16 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row16_col72\" class=\"data row16 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row16_col73\" class=\"data row16 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row16_col74\" class=\"data row16 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row16_col75\" class=\"data row16 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row16_col76\" class=\"data row16 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row16_col77\" class=\"data row16 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row16_col78\" class=\"data row16 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row16_col79\" class=\"data row16 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row16_col80\" class=\"data row16 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row16_col81\" class=\"data row16 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row16_col82\" class=\"data row16 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row16_col83\" class=\"data row16 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row16_col84\" class=\"data row16 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row16_col85\" class=\"data row16 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row16_col86\" class=\"data row16 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row16_col87\" class=\"data row16 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row16_col88\" class=\"data row16 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row16_col89\" class=\"data row16 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row16_col90\" class=\"data row16 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row16_col91\" class=\"data row16 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row16_col92\" class=\"data row16 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row16_col93\" class=\"data row16 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row16_col94\" class=\"data row16 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row16_col95\" class=\"data row16 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row16_col96\" class=\"data row16 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row16_col97\" class=\"data row16 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row16_col98\" class=\"data row16 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row16_col99\" class=\"data row16 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row16_col100\" class=\"data row16 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row16_col101\" class=\"data row16 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_4b03d_row17_col0\" class=\"data row17 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row17_col1\" class=\"data row17 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row17_col2\" class=\"data row17 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row17_col3\" class=\"data row17 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row17_col4\" class=\"data row17 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row17_col5\" class=\"data row17 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row17_col6\" class=\"data row17 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row17_col7\" class=\"data row17 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row17_col8\" class=\"data row17 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row17_col9\" class=\"data row17 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row17_col10\" class=\"data row17 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row17_col11\" class=\"data row17 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row17_col12\" class=\"data row17 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row17_col13\" class=\"data row17 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row17_col14\" class=\"data row17 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row17_col15\" class=\"data row17 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row17_col16\" class=\"data row17 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row17_col17\" class=\"data row17 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row17_col18\" class=\"data row17 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row17_col19\" class=\"data row17 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row17_col20\" class=\"data row17 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row17_col21\" class=\"data row17 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row17_col22\" class=\"data row17 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row17_col23\" class=\"data row17 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row17_col24\" class=\"data row17 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row17_col25\" class=\"data row17 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row17_col26\" class=\"data row17 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row17_col27\" class=\"data row17 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row17_col28\" class=\"data row17 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row17_col29\" class=\"data row17 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row17_col30\" class=\"data row17 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row17_col31\" class=\"data row17 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row17_col32\" class=\"data row17 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row17_col33\" class=\"data row17 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row17_col34\" class=\"data row17 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row17_col35\" class=\"data row17 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row17_col36\" class=\"data row17 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row17_col37\" class=\"data row17 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row17_col38\" class=\"data row17 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row17_col39\" class=\"data row17 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row17_col40\" class=\"data row17 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row17_col41\" class=\"data row17 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row17_col42\" class=\"data row17 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row17_col43\" class=\"data row17 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row17_col44\" class=\"data row17 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row17_col45\" class=\"data row17 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row17_col46\" class=\"data row17 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row17_col47\" class=\"data row17 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row17_col48\" class=\"data row17 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row17_col49\" class=\"data row17 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row17_col50\" class=\"data row17 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row17_col51\" class=\"data row17 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row17_col52\" class=\"data row17 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row17_col53\" class=\"data row17 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row17_col54\" class=\"data row17 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row17_col55\" class=\"data row17 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row17_col56\" class=\"data row17 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row17_col57\" class=\"data row17 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row17_col58\" class=\"data row17 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row17_col59\" class=\"data row17 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row17_col60\" class=\"data row17 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row17_col61\" class=\"data row17 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row17_col62\" class=\"data row17 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row17_col63\" class=\"data row17 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row17_col64\" class=\"data row17 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row17_col65\" class=\"data row17 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row17_col66\" class=\"data row17 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row17_col67\" class=\"data row17 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row17_col68\" class=\"data row17 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row17_col69\" class=\"data row17 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row17_col70\" class=\"data row17 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row17_col71\" class=\"data row17 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row17_col72\" class=\"data row17 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row17_col73\" class=\"data row17 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row17_col74\" class=\"data row17 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row17_col75\" class=\"data row17 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row17_col76\" class=\"data row17 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row17_col77\" class=\"data row17 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row17_col78\" class=\"data row17 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row17_col79\" class=\"data row17 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row17_col80\" class=\"data row17 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row17_col81\" class=\"data row17 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row17_col82\" class=\"data row17 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row17_col83\" class=\"data row17 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row17_col84\" class=\"data row17 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row17_col85\" class=\"data row17 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row17_col86\" class=\"data row17 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row17_col87\" class=\"data row17 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row17_col88\" class=\"data row17 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row17_col89\" class=\"data row17 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row17_col90\" class=\"data row17 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row17_col91\" class=\"data row17 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row17_col92\" class=\"data row17 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row17_col93\" class=\"data row17 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row17_col94\" class=\"data row17 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row17_col95\" class=\"data row17 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row17_col96\" class=\"data row17 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row17_col97\" class=\"data row17 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row17_col98\" class=\"data row17 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row17_col99\" class=\"data row17 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row17_col100\" class=\"data row17 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row17_col101\" class=\"data row17 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_4b03d_row18_col0\" class=\"data row18 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row18_col1\" class=\"data row18 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row18_col2\" class=\"data row18 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row18_col3\" class=\"data row18 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row18_col4\" class=\"data row18 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row18_col5\" class=\"data row18 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row18_col6\" class=\"data row18 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row18_col7\" class=\"data row18 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row18_col8\" class=\"data row18 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row18_col9\" class=\"data row18 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row18_col10\" class=\"data row18 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row18_col11\" class=\"data row18 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row18_col12\" class=\"data row18 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row18_col13\" class=\"data row18 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row18_col14\" class=\"data row18 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row18_col15\" class=\"data row18 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row18_col16\" class=\"data row18 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row18_col17\" class=\"data row18 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row18_col18\" class=\"data row18 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row18_col19\" class=\"data row18 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row18_col20\" class=\"data row18 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row18_col21\" class=\"data row18 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row18_col22\" class=\"data row18 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row18_col23\" class=\"data row18 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row18_col24\" class=\"data row18 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row18_col25\" class=\"data row18 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row18_col26\" class=\"data row18 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row18_col27\" class=\"data row18 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row18_col28\" class=\"data row18 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row18_col29\" class=\"data row18 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row18_col30\" class=\"data row18 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row18_col31\" class=\"data row18 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row18_col32\" class=\"data row18 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row18_col33\" class=\"data row18 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row18_col34\" class=\"data row18 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row18_col35\" class=\"data row18 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row18_col36\" class=\"data row18 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row18_col37\" class=\"data row18 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row18_col38\" class=\"data row18 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row18_col39\" class=\"data row18 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row18_col40\" class=\"data row18 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row18_col41\" class=\"data row18 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row18_col42\" class=\"data row18 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row18_col43\" class=\"data row18 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row18_col44\" class=\"data row18 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row18_col45\" class=\"data row18 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row18_col46\" class=\"data row18 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row18_col47\" class=\"data row18 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row18_col48\" class=\"data row18 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row18_col49\" class=\"data row18 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row18_col50\" class=\"data row18 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row18_col51\" class=\"data row18 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row18_col52\" class=\"data row18 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row18_col53\" class=\"data row18 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row18_col54\" class=\"data row18 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row18_col55\" class=\"data row18 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row18_col56\" class=\"data row18 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row18_col57\" class=\"data row18 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row18_col58\" class=\"data row18 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row18_col59\" class=\"data row18 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row18_col60\" class=\"data row18 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row18_col61\" class=\"data row18 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row18_col62\" class=\"data row18 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row18_col63\" class=\"data row18 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row18_col64\" class=\"data row18 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row18_col65\" class=\"data row18 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row18_col66\" class=\"data row18 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row18_col67\" class=\"data row18 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row18_col68\" class=\"data row18 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row18_col69\" class=\"data row18 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row18_col70\" class=\"data row18 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row18_col71\" class=\"data row18 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row18_col72\" class=\"data row18 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row18_col73\" class=\"data row18 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row18_col74\" class=\"data row18 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row18_col75\" class=\"data row18 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row18_col76\" class=\"data row18 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row18_col77\" class=\"data row18 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row18_col78\" class=\"data row18 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row18_col79\" class=\"data row18 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row18_col80\" class=\"data row18 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row18_col81\" class=\"data row18 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row18_col82\" class=\"data row18 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row18_col83\" class=\"data row18 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row18_col84\" class=\"data row18 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row18_col85\" class=\"data row18 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row18_col86\" class=\"data row18 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row18_col87\" class=\"data row18 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row18_col88\" class=\"data row18 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row18_col89\" class=\"data row18 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row18_col90\" class=\"data row18 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row18_col91\" class=\"data row18 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row18_col92\" class=\"data row18 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row18_col93\" class=\"data row18 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row18_col94\" class=\"data row18 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row18_col95\" class=\"data row18 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row18_col96\" class=\"data row18 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row18_col97\" class=\"data row18 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row18_col98\" class=\"data row18 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row18_col99\" class=\"data row18 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row18_col100\" class=\"data row18 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row18_col101\" class=\"data row18 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_4b03d_row19_col0\" class=\"data row19 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row19_col1\" class=\"data row19 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row19_col2\" class=\"data row19 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row19_col3\" class=\"data row19 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row19_col4\" class=\"data row19 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row19_col5\" class=\"data row19 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row19_col6\" class=\"data row19 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row19_col7\" class=\"data row19 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row19_col8\" class=\"data row19 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row19_col9\" class=\"data row19 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row19_col10\" class=\"data row19 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row19_col11\" class=\"data row19 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row19_col12\" class=\"data row19 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row19_col13\" class=\"data row19 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row19_col14\" class=\"data row19 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row19_col15\" class=\"data row19 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row19_col16\" class=\"data row19 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row19_col17\" class=\"data row19 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row19_col18\" class=\"data row19 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row19_col19\" class=\"data row19 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row19_col20\" class=\"data row19 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row19_col21\" class=\"data row19 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row19_col22\" class=\"data row19 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row19_col23\" class=\"data row19 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row19_col24\" class=\"data row19 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row19_col25\" class=\"data row19 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row19_col26\" class=\"data row19 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row19_col27\" class=\"data row19 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row19_col28\" class=\"data row19 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row19_col29\" class=\"data row19 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row19_col30\" class=\"data row19 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row19_col31\" class=\"data row19 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row19_col32\" class=\"data row19 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row19_col33\" class=\"data row19 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row19_col34\" class=\"data row19 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row19_col35\" class=\"data row19 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row19_col36\" class=\"data row19 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row19_col37\" class=\"data row19 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row19_col38\" class=\"data row19 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row19_col39\" class=\"data row19 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row19_col40\" class=\"data row19 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row19_col41\" class=\"data row19 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row19_col42\" class=\"data row19 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row19_col43\" class=\"data row19 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row19_col44\" class=\"data row19 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row19_col45\" class=\"data row19 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row19_col46\" class=\"data row19 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row19_col47\" class=\"data row19 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row19_col48\" class=\"data row19 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row19_col49\" class=\"data row19 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row19_col50\" class=\"data row19 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row19_col51\" class=\"data row19 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row19_col52\" class=\"data row19 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row19_col53\" class=\"data row19 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row19_col54\" class=\"data row19 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row19_col55\" class=\"data row19 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row19_col56\" class=\"data row19 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row19_col57\" class=\"data row19 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row19_col58\" class=\"data row19 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row19_col59\" class=\"data row19 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row19_col60\" class=\"data row19 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row19_col61\" class=\"data row19 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row19_col62\" class=\"data row19 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row19_col63\" class=\"data row19 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row19_col64\" class=\"data row19 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row19_col65\" class=\"data row19 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row19_col66\" class=\"data row19 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row19_col67\" class=\"data row19 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row19_col68\" class=\"data row19 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row19_col69\" class=\"data row19 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row19_col70\" class=\"data row19 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row19_col71\" class=\"data row19 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row19_col72\" class=\"data row19 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row19_col73\" class=\"data row19 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row19_col74\" class=\"data row19 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row19_col75\" class=\"data row19 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row19_col76\" class=\"data row19 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row19_col77\" class=\"data row19 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row19_col78\" class=\"data row19 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row19_col79\" class=\"data row19 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row19_col80\" class=\"data row19 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row19_col81\" class=\"data row19 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row19_col82\" class=\"data row19 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row19_col83\" class=\"data row19 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row19_col84\" class=\"data row19 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row19_col85\" class=\"data row19 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row19_col86\" class=\"data row19 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row19_col87\" class=\"data row19 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row19_col88\" class=\"data row19 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row19_col89\" class=\"data row19 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row19_col90\" class=\"data row19 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row19_col91\" class=\"data row19 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row19_col92\" class=\"data row19 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row19_col93\" class=\"data row19 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row19_col94\" class=\"data row19 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row19_col95\" class=\"data row19 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row19_col96\" class=\"data row19 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row19_col97\" class=\"data row19 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row19_col98\" class=\"data row19 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row19_col99\" class=\"data row19 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row19_col100\" class=\"data row19 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row19_col101\" class=\"data row19 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_4b03d_row20_col0\" class=\"data row20 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row20_col1\" class=\"data row20 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row20_col2\" class=\"data row20 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row20_col3\" class=\"data row20 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row20_col4\" class=\"data row20 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row20_col5\" class=\"data row20 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row20_col6\" class=\"data row20 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row20_col7\" class=\"data row20 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row20_col8\" class=\"data row20 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row20_col9\" class=\"data row20 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row20_col10\" class=\"data row20 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row20_col11\" class=\"data row20 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row20_col12\" class=\"data row20 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row20_col13\" class=\"data row20 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row20_col14\" class=\"data row20 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row20_col15\" class=\"data row20 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row20_col16\" class=\"data row20 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row20_col17\" class=\"data row20 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row20_col18\" class=\"data row20 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row20_col19\" class=\"data row20 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row20_col20\" class=\"data row20 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row20_col21\" class=\"data row20 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row20_col22\" class=\"data row20 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row20_col23\" class=\"data row20 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row20_col24\" class=\"data row20 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row20_col25\" class=\"data row20 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row20_col26\" class=\"data row20 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row20_col27\" class=\"data row20 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row20_col28\" class=\"data row20 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row20_col29\" class=\"data row20 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row20_col30\" class=\"data row20 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row20_col31\" class=\"data row20 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row20_col32\" class=\"data row20 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row20_col33\" class=\"data row20 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row20_col34\" class=\"data row20 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row20_col35\" class=\"data row20 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row20_col36\" class=\"data row20 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row20_col37\" class=\"data row20 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row20_col38\" class=\"data row20 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row20_col39\" class=\"data row20 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row20_col40\" class=\"data row20 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row20_col41\" class=\"data row20 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row20_col42\" class=\"data row20 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row20_col43\" class=\"data row20 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row20_col44\" class=\"data row20 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row20_col45\" class=\"data row20 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row20_col46\" class=\"data row20 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row20_col47\" class=\"data row20 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row20_col48\" class=\"data row20 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row20_col49\" class=\"data row20 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row20_col50\" class=\"data row20 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row20_col51\" class=\"data row20 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row20_col52\" class=\"data row20 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row20_col53\" class=\"data row20 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row20_col54\" class=\"data row20 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row20_col55\" class=\"data row20 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row20_col56\" class=\"data row20 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row20_col57\" class=\"data row20 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row20_col58\" class=\"data row20 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row20_col59\" class=\"data row20 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row20_col60\" class=\"data row20 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row20_col61\" class=\"data row20 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row20_col62\" class=\"data row20 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row20_col63\" class=\"data row20 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row20_col64\" class=\"data row20 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row20_col65\" class=\"data row20 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row20_col66\" class=\"data row20 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row20_col67\" class=\"data row20 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row20_col68\" class=\"data row20 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row20_col69\" class=\"data row20 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row20_col70\" class=\"data row20 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row20_col71\" class=\"data row20 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row20_col72\" class=\"data row20 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row20_col73\" class=\"data row20 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row20_col74\" class=\"data row20 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row20_col75\" class=\"data row20 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row20_col76\" class=\"data row20 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row20_col77\" class=\"data row20 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row20_col78\" class=\"data row20 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row20_col79\" class=\"data row20 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row20_col80\" class=\"data row20 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row20_col81\" class=\"data row20 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row20_col82\" class=\"data row20 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row20_col83\" class=\"data row20 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row20_col84\" class=\"data row20 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row20_col85\" class=\"data row20 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row20_col86\" class=\"data row20 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row20_col87\" class=\"data row20 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row20_col88\" class=\"data row20 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row20_col89\" class=\"data row20 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row20_col90\" class=\"data row20 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row20_col91\" class=\"data row20 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row20_col92\" class=\"data row20 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row20_col93\" class=\"data row20 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row20_col94\" class=\"data row20 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row20_col95\" class=\"data row20 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row20_col96\" class=\"data row20 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row20_col97\" class=\"data row20 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row20_col98\" class=\"data row20 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row20_col99\" class=\"data row20 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row20_col100\" class=\"data row20 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row20_col101\" class=\"data row20 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_4b03d_row21_col0\" class=\"data row21 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row21_col1\" class=\"data row21 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row21_col2\" class=\"data row21 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row21_col3\" class=\"data row21 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row21_col4\" class=\"data row21 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row21_col5\" class=\"data row21 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row21_col6\" class=\"data row21 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row21_col7\" class=\"data row21 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row21_col8\" class=\"data row21 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row21_col9\" class=\"data row21 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row21_col10\" class=\"data row21 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row21_col11\" class=\"data row21 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row21_col12\" class=\"data row21 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row21_col13\" class=\"data row21 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row21_col14\" class=\"data row21 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row21_col15\" class=\"data row21 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row21_col16\" class=\"data row21 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row21_col17\" class=\"data row21 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row21_col18\" class=\"data row21 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row21_col19\" class=\"data row21 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row21_col20\" class=\"data row21 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row21_col21\" class=\"data row21 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row21_col22\" class=\"data row21 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row21_col23\" class=\"data row21 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row21_col24\" class=\"data row21 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row21_col25\" class=\"data row21 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row21_col26\" class=\"data row21 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row21_col27\" class=\"data row21 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row21_col28\" class=\"data row21 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row21_col29\" class=\"data row21 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row21_col30\" class=\"data row21 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row21_col31\" class=\"data row21 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row21_col32\" class=\"data row21 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row21_col33\" class=\"data row21 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row21_col34\" class=\"data row21 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row21_col35\" class=\"data row21 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row21_col36\" class=\"data row21 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row21_col37\" class=\"data row21 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row21_col38\" class=\"data row21 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row21_col39\" class=\"data row21 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row21_col40\" class=\"data row21 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row21_col41\" class=\"data row21 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row21_col42\" class=\"data row21 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row21_col43\" class=\"data row21 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row21_col44\" class=\"data row21 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row21_col45\" class=\"data row21 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row21_col46\" class=\"data row21 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row21_col47\" class=\"data row21 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row21_col48\" class=\"data row21 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row21_col49\" class=\"data row21 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row21_col50\" class=\"data row21 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row21_col51\" class=\"data row21 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row21_col52\" class=\"data row21 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row21_col53\" class=\"data row21 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row21_col54\" class=\"data row21 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row21_col55\" class=\"data row21 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row21_col56\" class=\"data row21 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row21_col57\" class=\"data row21 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row21_col58\" class=\"data row21 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row21_col59\" class=\"data row21 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row21_col60\" class=\"data row21 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row21_col61\" class=\"data row21 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row21_col62\" class=\"data row21 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row21_col63\" class=\"data row21 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row21_col64\" class=\"data row21 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row21_col65\" class=\"data row21 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row21_col66\" class=\"data row21 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row21_col67\" class=\"data row21 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row21_col68\" class=\"data row21 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row21_col69\" class=\"data row21 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row21_col70\" class=\"data row21 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row21_col71\" class=\"data row21 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row21_col72\" class=\"data row21 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row21_col73\" class=\"data row21 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row21_col74\" class=\"data row21 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row21_col75\" class=\"data row21 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row21_col76\" class=\"data row21 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row21_col77\" class=\"data row21 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row21_col78\" class=\"data row21 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row21_col79\" class=\"data row21 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row21_col80\" class=\"data row21 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row21_col81\" class=\"data row21 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row21_col82\" class=\"data row21 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row21_col83\" class=\"data row21 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row21_col84\" class=\"data row21 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row21_col85\" class=\"data row21 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row21_col86\" class=\"data row21 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row21_col87\" class=\"data row21 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row21_col88\" class=\"data row21 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row21_col89\" class=\"data row21 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row21_col90\" class=\"data row21 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row21_col91\" class=\"data row21 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row21_col92\" class=\"data row21 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row21_col93\" class=\"data row21 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row21_col94\" class=\"data row21 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row21_col95\" class=\"data row21 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row21_col96\" class=\"data row21 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row21_col97\" class=\"data row21 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row21_col98\" class=\"data row21 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row21_col99\" class=\"data row21 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row21_col100\" class=\"data row21 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row21_col101\" class=\"data row21 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_4b03d_row22_col0\" class=\"data row22 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row22_col1\" class=\"data row22 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row22_col2\" class=\"data row22 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row22_col3\" class=\"data row22 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row22_col4\" class=\"data row22 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row22_col5\" class=\"data row22 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row22_col6\" class=\"data row22 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row22_col7\" class=\"data row22 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row22_col8\" class=\"data row22 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row22_col9\" class=\"data row22 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row22_col10\" class=\"data row22 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row22_col11\" class=\"data row22 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row22_col12\" class=\"data row22 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row22_col13\" class=\"data row22 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row22_col14\" class=\"data row22 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row22_col15\" class=\"data row22 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row22_col16\" class=\"data row22 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row22_col17\" class=\"data row22 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row22_col18\" class=\"data row22 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row22_col19\" class=\"data row22 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row22_col20\" class=\"data row22 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row22_col21\" class=\"data row22 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row22_col22\" class=\"data row22 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row22_col23\" class=\"data row22 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row22_col24\" class=\"data row22 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row22_col25\" class=\"data row22 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row22_col26\" class=\"data row22 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row22_col27\" class=\"data row22 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row22_col28\" class=\"data row22 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row22_col29\" class=\"data row22 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row22_col30\" class=\"data row22 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row22_col31\" class=\"data row22 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row22_col32\" class=\"data row22 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row22_col33\" class=\"data row22 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row22_col34\" class=\"data row22 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row22_col35\" class=\"data row22 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row22_col36\" class=\"data row22 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row22_col37\" class=\"data row22 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row22_col38\" class=\"data row22 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row22_col39\" class=\"data row22 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row22_col40\" class=\"data row22 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row22_col41\" class=\"data row22 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row22_col42\" class=\"data row22 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row22_col43\" class=\"data row22 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row22_col44\" class=\"data row22 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row22_col45\" class=\"data row22 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row22_col46\" class=\"data row22 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row22_col47\" class=\"data row22 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row22_col48\" class=\"data row22 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row22_col49\" class=\"data row22 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row22_col50\" class=\"data row22 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row22_col51\" class=\"data row22 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row22_col52\" class=\"data row22 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row22_col53\" class=\"data row22 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row22_col54\" class=\"data row22 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row22_col55\" class=\"data row22 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row22_col56\" class=\"data row22 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row22_col57\" class=\"data row22 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row22_col58\" class=\"data row22 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row22_col59\" class=\"data row22 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row22_col60\" class=\"data row22 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row22_col61\" class=\"data row22 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row22_col62\" class=\"data row22 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row22_col63\" class=\"data row22 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row22_col64\" class=\"data row22 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row22_col65\" class=\"data row22 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row22_col66\" class=\"data row22 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row22_col67\" class=\"data row22 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row22_col68\" class=\"data row22 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row22_col69\" class=\"data row22 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row22_col70\" class=\"data row22 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row22_col71\" class=\"data row22 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row22_col72\" class=\"data row22 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row22_col73\" class=\"data row22 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row22_col74\" class=\"data row22 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row22_col75\" class=\"data row22 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row22_col76\" class=\"data row22 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row22_col77\" class=\"data row22 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row22_col78\" class=\"data row22 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row22_col79\" class=\"data row22 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row22_col80\" class=\"data row22 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row22_col81\" class=\"data row22 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row22_col82\" class=\"data row22 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row22_col83\" class=\"data row22 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row22_col84\" class=\"data row22 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row22_col85\" class=\"data row22 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row22_col86\" class=\"data row22 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row22_col87\" class=\"data row22 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row22_col88\" class=\"data row22 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row22_col89\" class=\"data row22 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row22_col90\" class=\"data row22 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row22_col91\" class=\"data row22 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row22_col92\" class=\"data row22 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row22_col93\" class=\"data row22 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row22_col94\" class=\"data row22 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row22_col95\" class=\"data row22 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row22_col96\" class=\"data row22 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row22_col97\" class=\"data row22 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row22_col98\" class=\"data row22 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row22_col99\" class=\"data row22 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row22_col100\" class=\"data row22 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row22_col101\" class=\"data row22 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_4b03d_row23_col0\" class=\"data row23 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row23_col1\" class=\"data row23 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row23_col2\" class=\"data row23 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row23_col3\" class=\"data row23 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row23_col4\" class=\"data row23 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row23_col5\" class=\"data row23 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row23_col6\" class=\"data row23 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row23_col7\" class=\"data row23 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row23_col8\" class=\"data row23 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row23_col9\" class=\"data row23 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row23_col10\" class=\"data row23 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row23_col11\" class=\"data row23 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row23_col12\" class=\"data row23 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row23_col13\" class=\"data row23 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row23_col14\" class=\"data row23 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row23_col15\" class=\"data row23 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row23_col16\" class=\"data row23 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row23_col17\" class=\"data row23 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row23_col18\" class=\"data row23 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row23_col19\" class=\"data row23 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row23_col20\" class=\"data row23 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row23_col21\" class=\"data row23 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row23_col22\" class=\"data row23 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row23_col23\" class=\"data row23 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row23_col24\" class=\"data row23 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row23_col25\" class=\"data row23 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row23_col26\" class=\"data row23 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row23_col27\" class=\"data row23 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row23_col28\" class=\"data row23 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row23_col29\" class=\"data row23 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row23_col30\" class=\"data row23 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row23_col31\" class=\"data row23 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row23_col32\" class=\"data row23 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row23_col33\" class=\"data row23 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row23_col34\" class=\"data row23 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row23_col35\" class=\"data row23 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row23_col36\" class=\"data row23 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row23_col37\" class=\"data row23 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row23_col38\" class=\"data row23 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row23_col39\" class=\"data row23 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row23_col40\" class=\"data row23 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row23_col41\" class=\"data row23 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row23_col42\" class=\"data row23 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row23_col43\" class=\"data row23 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row23_col44\" class=\"data row23 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row23_col45\" class=\"data row23 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row23_col46\" class=\"data row23 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row23_col47\" class=\"data row23 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row23_col48\" class=\"data row23 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row23_col49\" class=\"data row23 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row23_col50\" class=\"data row23 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row23_col51\" class=\"data row23 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row23_col52\" class=\"data row23 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row23_col53\" class=\"data row23 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row23_col54\" class=\"data row23 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row23_col55\" class=\"data row23 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row23_col56\" class=\"data row23 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row23_col57\" class=\"data row23 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row23_col58\" class=\"data row23 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row23_col59\" class=\"data row23 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row23_col60\" class=\"data row23 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row23_col61\" class=\"data row23 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row23_col62\" class=\"data row23 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row23_col63\" class=\"data row23 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row23_col64\" class=\"data row23 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row23_col65\" class=\"data row23 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row23_col66\" class=\"data row23 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row23_col67\" class=\"data row23 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row23_col68\" class=\"data row23 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row23_col69\" class=\"data row23 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row23_col70\" class=\"data row23 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row23_col71\" class=\"data row23 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row23_col72\" class=\"data row23 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row23_col73\" class=\"data row23 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row23_col74\" class=\"data row23 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row23_col75\" class=\"data row23 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row23_col76\" class=\"data row23 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row23_col77\" class=\"data row23 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row23_col78\" class=\"data row23 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row23_col79\" class=\"data row23 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row23_col80\" class=\"data row23 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row23_col81\" class=\"data row23 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row23_col82\" class=\"data row23 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row23_col83\" class=\"data row23 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row23_col84\" class=\"data row23 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row23_col85\" class=\"data row23 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row23_col86\" class=\"data row23 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row23_col87\" class=\"data row23 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row23_col88\" class=\"data row23 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row23_col89\" class=\"data row23 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row23_col90\" class=\"data row23 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row23_col91\" class=\"data row23 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row23_col92\" class=\"data row23 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row23_col93\" class=\"data row23 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row23_col94\" class=\"data row23 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row23_col95\" class=\"data row23 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row23_col96\" class=\"data row23 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row23_col97\" class=\"data row23 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row23_col98\" class=\"data row23 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row23_col99\" class=\"data row23 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row23_col100\" class=\"data row23 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row23_col101\" class=\"data row23 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_4b03d_row24_col0\" class=\"data row24 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row24_col1\" class=\"data row24 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row24_col2\" class=\"data row24 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row24_col3\" class=\"data row24 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row24_col4\" class=\"data row24 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row24_col5\" class=\"data row24 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row24_col6\" class=\"data row24 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row24_col7\" class=\"data row24 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row24_col8\" class=\"data row24 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row24_col9\" class=\"data row24 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row24_col10\" class=\"data row24 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row24_col11\" class=\"data row24 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row24_col12\" class=\"data row24 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row24_col13\" class=\"data row24 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row24_col14\" class=\"data row24 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row24_col15\" class=\"data row24 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row24_col16\" class=\"data row24 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row24_col17\" class=\"data row24 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row24_col18\" class=\"data row24 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row24_col19\" class=\"data row24 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row24_col20\" class=\"data row24 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row24_col21\" class=\"data row24 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row24_col22\" class=\"data row24 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row24_col23\" class=\"data row24 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row24_col24\" class=\"data row24 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row24_col25\" class=\"data row24 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row24_col26\" class=\"data row24 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row24_col27\" class=\"data row24 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row24_col28\" class=\"data row24 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row24_col29\" class=\"data row24 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row24_col30\" class=\"data row24 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row24_col31\" class=\"data row24 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row24_col32\" class=\"data row24 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row24_col33\" class=\"data row24 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row24_col34\" class=\"data row24 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row24_col35\" class=\"data row24 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row24_col36\" class=\"data row24 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row24_col37\" class=\"data row24 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row24_col38\" class=\"data row24 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row24_col39\" class=\"data row24 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row24_col40\" class=\"data row24 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row24_col41\" class=\"data row24 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row24_col42\" class=\"data row24 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row24_col43\" class=\"data row24 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row24_col44\" class=\"data row24 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row24_col45\" class=\"data row24 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row24_col46\" class=\"data row24 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row24_col47\" class=\"data row24 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row24_col48\" class=\"data row24 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row24_col49\" class=\"data row24 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row24_col50\" class=\"data row24 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row24_col51\" class=\"data row24 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row24_col52\" class=\"data row24 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row24_col53\" class=\"data row24 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row24_col54\" class=\"data row24 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row24_col55\" class=\"data row24 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row24_col56\" class=\"data row24 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row24_col57\" class=\"data row24 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row24_col58\" class=\"data row24 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row24_col59\" class=\"data row24 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row24_col60\" class=\"data row24 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row24_col61\" class=\"data row24 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row24_col62\" class=\"data row24 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row24_col63\" class=\"data row24 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row24_col64\" class=\"data row24 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row24_col65\" class=\"data row24 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row24_col66\" class=\"data row24 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row24_col67\" class=\"data row24 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row24_col68\" class=\"data row24 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row24_col69\" class=\"data row24 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row24_col70\" class=\"data row24 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row24_col71\" class=\"data row24 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row24_col72\" class=\"data row24 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row24_col73\" class=\"data row24 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row24_col74\" class=\"data row24 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row24_col75\" class=\"data row24 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row24_col76\" class=\"data row24 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row24_col77\" class=\"data row24 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row24_col78\" class=\"data row24 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row24_col79\" class=\"data row24 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row24_col80\" class=\"data row24 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row24_col81\" class=\"data row24 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row24_col82\" class=\"data row24 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row24_col83\" class=\"data row24 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row24_col84\" class=\"data row24 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row24_col85\" class=\"data row24 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row24_col86\" class=\"data row24 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row24_col87\" class=\"data row24 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row24_col88\" class=\"data row24 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row24_col89\" class=\"data row24 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row24_col90\" class=\"data row24 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row24_col91\" class=\"data row24 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row24_col92\" class=\"data row24 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row24_col93\" class=\"data row24 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row24_col94\" class=\"data row24 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row24_col95\" class=\"data row24 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row24_col96\" class=\"data row24 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row24_col97\" class=\"data row24 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row24_col98\" class=\"data row24 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row24_col99\" class=\"data row24 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row24_col100\" class=\"data row24 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row24_col101\" class=\"data row24 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_4b03d_row25_col0\" class=\"data row25 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row25_col1\" class=\"data row25 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row25_col2\" class=\"data row25 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row25_col3\" class=\"data row25 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row25_col4\" class=\"data row25 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row25_col5\" class=\"data row25 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row25_col6\" class=\"data row25 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row25_col7\" class=\"data row25 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row25_col8\" class=\"data row25 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row25_col9\" class=\"data row25 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row25_col10\" class=\"data row25 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row25_col11\" class=\"data row25 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row25_col12\" class=\"data row25 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row25_col13\" class=\"data row25 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row25_col14\" class=\"data row25 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row25_col15\" class=\"data row25 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row25_col16\" class=\"data row25 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row25_col17\" class=\"data row25 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row25_col18\" class=\"data row25 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row25_col19\" class=\"data row25 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row25_col20\" class=\"data row25 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row25_col21\" class=\"data row25 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row25_col22\" class=\"data row25 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row25_col23\" class=\"data row25 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row25_col24\" class=\"data row25 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row25_col25\" class=\"data row25 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row25_col26\" class=\"data row25 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row25_col27\" class=\"data row25 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row25_col28\" class=\"data row25 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row25_col29\" class=\"data row25 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row25_col30\" class=\"data row25 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row25_col31\" class=\"data row25 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row25_col32\" class=\"data row25 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row25_col33\" class=\"data row25 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row25_col34\" class=\"data row25 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row25_col35\" class=\"data row25 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row25_col36\" class=\"data row25 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row25_col37\" class=\"data row25 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row25_col38\" class=\"data row25 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row25_col39\" class=\"data row25 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row25_col40\" class=\"data row25 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row25_col41\" class=\"data row25 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row25_col42\" class=\"data row25 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row25_col43\" class=\"data row25 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row25_col44\" class=\"data row25 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row25_col45\" class=\"data row25 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row25_col46\" class=\"data row25 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row25_col47\" class=\"data row25 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row25_col48\" class=\"data row25 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row25_col49\" class=\"data row25 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row25_col50\" class=\"data row25 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row25_col51\" class=\"data row25 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row25_col52\" class=\"data row25 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row25_col53\" class=\"data row25 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row25_col54\" class=\"data row25 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row25_col55\" class=\"data row25 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row25_col56\" class=\"data row25 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row25_col57\" class=\"data row25 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row25_col58\" class=\"data row25 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row25_col59\" class=\"data row25 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row25_col60\" class=\"data row25 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row25_col61\" class=\"data row25 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row25_col62\" class=\"data row25 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row25_col63\" class=\"data row25 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row25_col64\" class=\"data row25 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row25_col65\" class=\"data row25 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row25_col66\" class=\"data row25 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row25_col67\" class=\"data row25 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row25_col68\" class=\"data row25 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row25_col69\" class=\"data row25 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row25_col70\" class=\"data row25 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row25_col71\" class=\"data row25 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row25_col72\" class=\"data row25 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row25_col73\" class=\"data row25 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row25_col74\" class=\"data row25 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row25_col75\" class=\"data row25 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row25_col76\" class=\"data row25 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row25_col77\" class=\"data row25 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row25_col78\" class=\"data row25 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row25_col79\" class=\"data row25 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row25_col80\" class=\"data row25 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row25_col81\" class=\"data row25 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row25_col82\" class=\"data row25 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row25_col83\" class=\"data row25 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row25_col84\" class=\"data row25 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row25_col85\" class=\"data row25 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row25_col86\" class=\"data row25 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row25_col87\" class=\"data row25 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row25_col88\" class=\"data row25 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row25_col89\" class=\"data row25 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row25_col90\" class=\"data row25 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row25_col91\" class=\"data row25 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row25_col92\" class=\"data row25 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row25_col93\" class=\"data row25 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row25_col94\" class=\"data row25 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row25_col95\" class=\"data row25 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row25_col96\" class=\"data row25 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row25_col97\" class=\"data row25 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row25_col98\" class=\"data row25 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row25_col99\" class=\"data row25 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row25_col100\" class=\"data row25 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row25_col101\" class=\"data row25 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_4b03d_row26_col0\" class=\"data row26 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row26_col1\" class=\"data row26 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row26_col2\" class=\"data row26 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row26_col3\" class=\"data row26 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row26_col4\" class=\"data row26 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row26_col5\" class=\"data row26 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row26_col6\" class=\"data row26 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row26_col7\" class=\"data row26 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row26_col8\" class=\"data row26 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row26_col9\" class=\"data row26 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row26_col10\" class=\"data row26 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row26_col11\" class=\"data row26 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row26_col12\" class=\"data row26 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row26_col13\" class=\"data row26 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row26_col14\" class=\"data row26 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row26_col15\" class=\"data row26 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row26_col16\" class=\"data row26 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row26_col17\" class=\"data row26 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row26_col18\" class=\"data row26 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row26_col19\" class=\"data row26 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row26_col20\" class=\"data row26 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row26_col21\" class=\"data row26 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row26_col22\" class=\"data row26 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row26_col23\" class=\"data row26 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row26_col24\" class=\"data row26 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row26_col25\" class=\"data row26 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row26_col26\" class=\"data row26 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row26_col27\" class=\"data row26 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row26_col28\" class=\"data row26 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row26_col29\" class=\"data row26 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row26_col30\" class=\"data row26 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row26_col31\" class=\"data row26 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row26_col32\" class=\"data row26 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row26_col33\" class=\"data row26 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row26_col34\" class=\"data row26 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row26_col35\" class=\"data row26 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row26_col36\" class=\"data row26 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row26_col37\" class=\"data row26 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row26_col38\" class=\"data row26 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row26_col39\" class=\"data row26 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row26_col40\" class=\"data row26 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row26_col41\" class=\"data row26 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row26_col42\" class=\"data row26 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row26_col43\" class=\"data row26 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row26_col44\" class=\"data row26 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row26_col45\" class=\"data row26 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row26_col46\" class=\"data row26 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row26_col47\" class=\"data row26 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row26_col48\" class=\"data row26 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row26_col49\" class=\"data row26 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row26_col50\" class=\"data row26 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row26_col51\" class=\"data row26 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row26_col52\" class=\"data row26 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row26_col53\" class=\"data row26 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row26_col54\" class=\"data row26 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row26_col55\" class=\"data row26 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row26_col56\" class=\"data row26 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row26_col57\" class=\"data row26 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row26_col58\" class=\"data row26 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row26_col59\" class=\"data row26 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row26_col60\" class=\"data row26 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row26_col61\" class=\"data row26 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row26_col62\" class=\"data row26 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row26_col63\" class=\"data row26 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row26_col64\" class=\"data row26 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row26_col65\" class=\"data row26 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row26_col66\" class=\"data row26 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row26_col67\" class=\"data row26 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row26_col68\" class=\"data row26 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row26_col69\" class=\"data row26 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row26_col70\" class=\"data row26 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row26_col71\" class=\"data row26 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row26_col72\" class=\"data row26 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row26_col73\" class=\"data row26 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row26_col74\" class=\"data row26 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row26_col75\" class=\"data row26 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row26_col76\" class=\"data row26 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row26_col77\" class=\"data row26 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row26_col78\" class=\"data row26 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row26_col79\" class=\"data row26 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row26_col80\" class=\"data row26 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row26_col81\" class=\"data row26 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row26_col82\" class=\"data row26 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row26_col83\" class=\"data row26 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row26_col84\" class=\"data row26 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row26_col85\" class=\"data row26 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row26_col86\" class=\"data row26 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row26_col87\" class=\"data row26 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row26_col88\" class=\"data row26 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row26_col89\" class=\"data row26 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row26_col90\" class=\"data row26 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row26_col91\" class=\"data row26 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row26_col92\" class=\"data row26 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row26_col93\" class=\"data row26 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row26_col94\" class=\"data row26 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row26_col95\" class=\"data row26 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row26_col96\" class=\"data row26 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row26_col97\" class=\"data row26 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row26_col98\" class=\"data row26 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row26_col99\" class=\"data row26 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row26_col100\" class=\"data row26 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row26_col101\" class=\"data row26 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_4b03d_row27_col0\" class=\"data row27 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row27_col1\" class=\"data row27 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row27_col2\" class=\"data row27 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row27_col3\" class=\"data row27 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row27_col4\" class=\"data row27 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row27_col5\" class=\"data row27 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row27_col6\" class=\"data row27 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row27_col7\" class=\"data row27 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row27_col8\" class=\"data row27 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row27_col9\" class=\"data row27 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row27_col10\" class=\"data row27 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row27_col11\" class=\"data row27 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row27_col12\" class=\"data row27 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row27_col13\" class=\"data row27 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row27_col14\" class=\"data row27 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row27_col15\" class=\"data row27 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row27_col16\" class=\"data row27 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row27_col17\" class=\"data row27 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row27_col18\" class=\"data row27 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row27_col19\" class=\"data row27 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row27_col20\" class=\"data row27 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row27_col21\" class=\"data row27 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row27_col22\" class=\"data row27 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row27_col23\" class=\"data row27 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row27_col24\" class=\"data row27 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row27_col25\" class=\"data row27 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row27_col26\" class=\"data row27 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row27_col27\" class=\"data row27 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row27_col28\" class=\"data row27 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row27_col29\" class=\"data row27 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row27_col30\" class=\"data row27 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row27_col31\" class=\"data row27 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row27_col32\" class=\"data row27 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row27_col33\" class=\"data row27 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row27_col34\" class=\"data row27 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row27_col35\" class=\"data row27 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row27_col36\" class=\"data row27 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row27_col37\" class=\"data row27 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row27_col38\" class=\"data row27 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row27_col39\" class=\"data row27 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row27_col40\" class=\"data row27 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row27_col41\" class=\"data row27 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row27_col42\" class=\"data row27 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row27_col43\" class=\"data row27 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row27_col44\" class=\"data row27 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row27_col45\" class=\"data row27 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row27_col46\" class=\"data row27 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row27_col47\" class=\"data row27 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row27_col48\" class=\"data row27 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row27_col49\" class=\"data row27 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row27_col50\" class=\"data row27 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row27_col51\" class=\"data row27 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row27_col52\" class=\"data row27 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row27_col53\" class=\"data row27 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row27_col54\" class=\"data row27 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row27_col55\" class=\"data row27 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row27_col56\" class=\"data row27 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row27_col57\" class=\"data row27 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row27_col58\" class=\"data row27 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row27_col59\" class=\"data row27 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row27_col60\" class=\"data row27 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row27_col61\" class=\"data row27 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row27_col62\" class=\"data row27 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row27_col63\" class=\"data row27 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row27_col64\" class=\"data row27 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row27_col65\" class=\"data row27 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row27_col66\" class=\"data row27 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row27_col67\" class=\"data row27 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row27_col68\" class=\"data row27 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row27_col69\" class=\"data row27 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row27_col70\" class=\"data row27 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row27_col71\" class=\"data row27 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row27_col72\" class=\"data row27 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row27_col73\" class=\"data row27 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row27_col74\" class=\"data row27 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row27_col75\" class=\"data row27 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row27_col76\" class=\"data row27 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row27_col77\" class=\"data row27 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row27_col78\" class=\"data row27 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row27_col79\" class=\"data row27 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row27_col80\" class=\"data row27 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row27_col81\" class=\"data row27 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row27_col82\" class=\"data row27 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row27_col83\" class=\"data row27 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row27_col84\" class=\"data row27 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row27_col85\" class=\"data row27 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row27_col86\" class=\"data row27 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row27_col87\" class=\"data row27 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row27_col88\" class=\"data row27 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row27_col89\" class=\"data row27 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row27_col90\" class=\"data row27 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row27_col91\" class=\"data row27 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row27_col92\" class=\"data row27 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row27_col93\" class=\"data row27 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row27_col94\" class=\"data row27 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row27_col95\" class=\"data row27 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row27_col96\" class=\"data row27 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row27_col97\" class=\"data row27 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row27_col98\" class=\"data row27 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row27_col99\" class=\"data row27 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row27_col100\" class=\"data row27 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row27_col101\" class=\"data row27 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_4b03d_row28_col0\" class=\"data row28 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row28_col1\" class=\"data row28 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row28_col2\" class=\"data row28 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row28_col3\" class=\"data row28 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row28_col4\" class=\"data row28 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row28_col5\" class=\"data row28 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row28_col6\" class=\"data row28 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row28_col7\" class=\"data row28 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row28_col8\" class=\"data row28 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row28_col9\" class=\"data row28 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row28_col10\" class=\"data row28 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row28_col11\" class=\"data row28 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row28_col12\" class=\"data row28 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row28_col13\" class=\"data row28 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row28_col14\" class=\"data row28 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row28_col15\" class=\"data row28 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row28_col16\" class=\"data row28 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row28_col17\" class=\"data row28 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row28_col18\" class=\"data row28 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row28_col19\" class=\"data row28 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row28_col20\" class=\"data row28 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row28_col21\" class=\"data row28 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row28_col22\" class=\"data row28 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row28_col23\" class=\"data row28 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row28_col24\" class=\"data row28 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row28_col25\" class=\"data row28 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row28_col26\" class=\"data row28 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row28_col27\" class=\"data row28 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row28_col28\" class=\"data row28 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row28_col29\" class=\"data row28 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row28_col30\" class=\"data row28 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row28_col31\" class=\"data row28 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row28_col32\" class=\"data row28 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row28_col33\" class=\"data row28 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row28_col34\" class=\"data row28 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row28_col35\" class=\"data row28 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row28_col36\" class=\"data row28 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row28_col37\" class=\"data row28 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row28_col38\" class=\"data row28 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row28_col39\" class=\"data row28 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row28_col40\" class=\"data row28 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row28_col41\" class=\"data row28 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row28_col42\" class=\"data row28 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row28_col43\" class=\"data row28 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row28_col44\" class=\"data row28 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row28_col45\" class=\"data row28 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row28_col46\" class=\"data row28 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row28_col47\" class=\"data row28 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row28_col48\" class=\"data row28 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row28_col49\" class=\"data row28 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row28_col50\" class=\"data row28 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row28_col51\" class=\"data row28 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row28_col52\" class=\"data row28 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row28_col53\" class=\"data row28 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row28_col54\" class=\"data row28 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row28_col55\" class=\"data row28 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row28_col56\" class=\"data row28 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row28_col57\" class=\"data row28 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row28_col58\" class=\"data row28 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row28_col59\" class=\"data row28 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row28_col60\" class=\"data row28 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row28_col61\" class=\"data row28 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row28_col62\" class=\"data row28 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row28_col63\" class=\"data row28 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row28_col64\" class=\"data row28 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row28_col65\" class=\"data row28 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row28_col66\" class=\"data row28 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row28_col67\" class=\"data row28 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row28_col68\" class=\"data row28 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row28_col69\" class=\"data row28 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row28_col70\" class=\"data row28 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row28_col71\" class=\"data row28 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row28_col72\" class=\"data row28 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row28_col73\" class=\"data row28 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row28_col74\" class=\"data row28 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row28_col75\" class=\"data row28 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row28_col76\" class=\"data row28 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row28_col77\" class=\"data row28 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row28_col78\" class=\"data row28 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row28_col79\" class=\"data row28 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row28_col80\" class=\"data row28 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row28_col81\" class=\"data row28 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row28_col82\" class=\"data row28 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row28_col83\" class=\"data row28 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row28_col84\" class=\"data row28 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row28_col85\" class=\"data row28 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row28_col86\" class=\"data row28 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row28_col87\" class=\"data row28 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row28_col88\" class=\"data row28 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row28_col89\" class=\"data row28 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row28_col90\" class=\"data row28 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row28_col91\" class=\"data row28 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row28_col92\" class=\"data row28 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row28_col93\" class=\"data row28 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row28_col94\" class=\"data row28 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row28_col95\" class=\"data row28 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row28_col96\" class=\"data row28 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row28_col97\" class=\"data row28 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row28_col98\" class=\"data row28 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row28_col99\" class=\"data row28 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row28_col100\" class=\"data row28 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row28_col101\" class=\"data row28 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_4b03d_row29_col0\" class=\"data row29 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row29_col1\" class=\"data row29 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row29_col2\" class=\"data row29 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row29_col3\" class=\"data row29 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row29_col4\" class=\"data row29 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row29_col5\" class=\"data row29 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row29_col6\" class=\"data row29 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row29_col7\" class=\"data row29 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row29_col8\" class=\"data row29 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row29_col9\" class=\"data row29 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row29_col10\" class=\"data row29 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row29_col11\" class=\"data row29 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row29_col12\" class=\"data row29 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row29_col13\" class=\"data row29 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row29_col14\" class=\"data row29 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row29_col15\" class=\"data row29 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row29_col16\" class=\"data row29 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row29_col17\" class=\"data row29 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row29_col18\" class=\"data row29 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row29_col19\" class=\"data row29 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row29_col20\" class=\"data row29 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row29_col21\" class=\"data row29 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row29_col22\" class=\"data row29 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row29_col23\" class=\"data row29 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row29_col24\" class=\"data row29 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row29_col25\" class=\"data row29 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row29_col26\" class=\"data row29 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row29_col27\" class=\"data row29 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row29_col28\" class=\"data row29 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row29_col29\" class=\"data row29 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row29_col30\" class=\"data row29 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row29_col31\" class=\"data row29 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row29_col32\" class=\"data row29 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row29_col33\" class=\"data row29 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row29_col34\" class=\"data row29 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row29_col35\" class=\"data row29 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row29_col36\" class=\"data row29 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row29_col37\" class=\"data row29 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row29_col38\" class=\"data row29 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row29_col39\" class=\"data row29 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row29_col40\" class=\"data row29 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row29_col41\" class=\"data row29 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row29_col42\" class=\"data row29 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row29_col43\" class=\"data row29 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row29_col44\" class=\"data row29 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row29_col45\" class=\"data row29 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row29_col46\" class=\"data row29 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row29_col47\" class=\"data row29 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row29_col48\" class=\"data row29 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row29_col49\" class=\"data row29 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row29_col50\" class=\"data row29 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row29_col51\" class=\"data row29 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row29_col52\" class=\"data row29 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row29_col53\" class=\"data row29 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row29_col54\" class=\"data row29 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row29_col55\" class=\"data row29 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row29_col56\" class=\"data row29 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row29_col57\" class=\"data row29 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row29_col58\" class=\"data row29 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row29_col59\" class=\"data row29 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row29_col60\" class=\"data row29 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row29_col61\" class=\"data row29 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row29_col62\" class=\"data row29 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row29_col63\" class=\"data row29 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row29_col64\" class=\"data row29 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row29_col65\" class=\"data row29 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row29_col66\" class=\"data row29 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row29_col67\" class=\"data row29 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row29_col68\" class=\"data row29 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row29_col69\" class=\"data row29 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row29_col70\" class=\"data row29 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row29_col71\" class=\"data row29 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row29_col72\" class=\"data row29 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row29_col73\" class=\"data row29 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row29_col74\" class=\"data row29 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row29_col75\" class=\"data row29 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row29_col76\" class=\"data row29 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row29_col77\" class=\"data row29 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row29_col78\" class=\"data row29 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row29_col79\" class=\"data row29 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row29_col80\" class=\"data row29 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row29_col81\" class=\"data row29 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row29_col82\" class=\"data row29 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row29_col83\" class=\"data row29 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row29_col84\" class=\"data row29 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row29_col85\" class=\"data row29 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row29_col86\" class=\"data row29 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row29_col87\" class=\"data row29 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row29_col88\" class=\"data row29 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row29_col89\" class=\"data row29 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row29_col90\" class=\"data row29 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row29_col91\" class=\"data row29 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row29_col92\" class=\"data row29 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row29_col93\" class=\"data row29 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row29_col94\" class=\"data row29 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row29_col95\" class=\"data row29 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row29_col96\" class=\"data row29 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row29_col97\" class=\"data row29 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row29_col98\" class=\"data row29 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row29_col99\" class=\"data row29 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row29_col100\" class=\"data row29 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row29_col101\" class=\"data row29 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_4b03d_row30_col0\" class=\"data row30 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row30_col1\" class=\"data row30 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row30_col2\" class=\"data row30 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row30_col3\" class=\"data row30 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row30_col4\" class=\"data row30 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row30_col5\" class=\"data row30 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row30_col6\" class=\"data row30 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row30_col7\" class=\"data row30 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row30_col8\" class=\"data row30 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row30_col9\" class=\"data row30 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row30_col10\" class=\"data row30 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row30_col11\" class=\"data row30 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row30_col12\" class=\"data row30 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row30_col13\" class=\"data row30 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row30_col14\" class=\"data row30 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row30_col15\" class=\"data row30 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row30_col16\" class=\"data row30 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row30_col17\" class=\"data row30 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row30_col18\" class=\"data row30 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row30_col19\" class=\"data row30 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row30_col20\" class=\"data row30 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row30_col21\" class=\"data row30 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row30_col22\" class=\"data row30 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row30_col23\" class=\"data row30 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row30_col24\" class=\"data row30 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row30_col25\" class=\"data row30 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row30_col26\" class=\"data row30 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row30_col27\" class=\"data row30 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row30_col28\" class=\"data row30 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row30_col29\" class=\"data row30 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row30_col30\" class=\"data row30 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row30_col31\" class=\"data row30 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row30_col32\" class=\"data row30 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row30_col33\" class=\"data row30 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row30_col34\" class=\"data row30 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row30_col35\" class=\"data row30 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row30_col36\" class=\"data row30 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row30_col37\" class=\"data row30 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row30_col38\" class=\"data row30 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row30_col39\" class=\"data row30 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row30_col40\" class=\"data row30 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row30_col41\" class=\"data row30 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row30_col42\" class=\"data row30 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row30_col43\" class=\"data row30 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row30_col44\" class=\"data row30 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row30_col45\" class=\"data row30 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row30_col46\" class=\"data row30 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row30_col47\" class=\"data row30 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row30_col48\" class=\"data row30 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row30_col49\" class=\"data row30 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row30_col50\" class=\"data row30 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row30_col51\" class=\"data row30 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row30_col52\" class=\"data row30 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row30_col53\" class=\"data row30 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row30_col54\" class=\"data row30 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row30_col55\" class=\"data row30 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row30_col56\" class=\"data row30 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row30_col57\" class=\"data row30 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row30_col58\" class=\"data row30 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row30_col59\" class=\"data row30 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row30_col60\" class=\"data row30 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row30_col61\" class=\"data row30 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row30_col62\" class=\"data row30 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row30_col63\" class=\"data row30 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row30_col64\" class=\"data row30 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row30_col65\" class=\"data row30 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row30_col66\" class=\"data row30 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row30_col67\" class=\"data row30 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row30_col68\" class=\"data row30 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row30_col69\" class=\"data row30 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row30_col70\" class=\"data row30 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row30_col71\" class=\"data row30 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row30_col72\" class=\"data row30 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row30_col73\" class=\"data row30 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row30_col74\" class=\"data row30 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row30_col75\" class=\"data row30 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row30_col76\" class=\"data row30 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row30_col77\" class=\"data row30 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row30_col78\" class=\"data row30 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row30_col79\" class=\"data row30 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row30_col80\" class=\"data row30 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row30_col81\" class=\"data row30 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row30_col82\" class=\"data row30 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row30_col83\" class=\"data row30 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row30_col84\" class=\"data row30 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row30_col85\" class=\"data row30 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row30_col86\" class=\"data row30 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row30_col87\" class=\"data row30 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row30_col88\" class=\"data row30 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row30_col89\" class=\"data row30 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row30_col90\" class=\"data row30 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row30_col91\" class=\"data row30 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row30_col92\" class=\"data row30 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row30_col93\" class=\"data row30 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row30_col94\" class=\"data row30 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row30_col95\" class=\"data row30 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row30_col96\" class=\"data row30 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row30_col97\" class=\"data row30 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row30_col98\" class=\"data row30 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row30_col99\" class=\"data row30 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row30_col100\" class=\"data row30 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row30_col101\" class=\"data row30 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_4b03d_row31_col0\" class=\"data row31 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row31_col1\" class=\"data row31 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row31_col2\" class=\"data row31 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row31_col3\" class=\"data row31 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row31_col4\" class=\"data row31 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row31_col5\" class=\"data row31 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row31_col6\" class=\"data row31 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row31_col7\" class=\"data row31 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row31_col8\" class=\"data row31 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row31_col9\" class=\"data row31 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row31_col10\" class=\"data row31 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row31_col11\" class=\"data row31 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row31_col12\" class=\"data row31 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row31_col13\" class=\"data row31 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row31_col14\" class=\"data row31 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row31_col15\" class=\"data row31 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row31_col16\" class=\"data row31 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row31_col17\" class=\"data row31 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row31_col18\" class=\"data row31 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row31_col19\" class=\"data row31 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row31_col20\" class=\"data row31 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row31_col21\" class=\"data row31 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row31_col22\" class=\"data row31 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row31_col23\" class=\"data row31 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row31_col24\" class=\"data row31 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row31_col25\" class=\"data row31 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row31_col26\" class=\"data row31 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row31_col27\" class=\"data row31 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row31_col28\" class=\"data row31 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row31_col29\" class=\"data row31 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row31_col30\" class=\"data row31 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row31_col31\" class=\"data row31 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row31_col32\" class=\"data row31 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row31_col33\" class=\"data row31 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row31_col34\" class=\"data row31 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row31_col35\" class=\"data row31 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row31_col36\" class=\"data row31 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row31_col37\" class=\"data row31 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row31_col38\" class=\"data row31 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row31_col39\" class=\"data row31 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row31_col40\" class=\"data row31 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row31_col41\" class=\"data row31 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row31_col42\" class=\"data row31 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row31_col43\" class=\"data row31 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row31_col44\" class=\"data row31 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row31_col45\" class=\"data row31 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row31_col46\" class=\"data row31 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row31_col47\" class=\"data row31 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row31_col48\" class=\"data row31 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row31_col49\" class=\"data row31 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row31_col50\" class=\"data row31 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row31_col51\" class=\"data row31 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row31_col52\" class=\"data row31 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row31_col53\" class=\"data row31 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row31_col54\" class=\"data row31 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row31_col55\" class=\"data row31 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row31_col56\" class=\"data row31 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row31_col57\" class=\"data row31 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row31_col58\" class=\"data row31 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row31_col59\" class=\"data row31 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row31_col60\" class=\"data row31 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row31_col61\" class=\"data row31 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row31_col62\" class=\"data row31 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row31_col63\" class=\"data row31 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row31_col64\" class=\"data row31 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row31_col65\" class=\"data row31 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row31_col66\" class=\"data row31 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row31_col67\" class=\"data row31 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row31_col68\" class=\"data row31 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row31_col69\" class=\"data row31 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row31_col70\" class=\"data row31 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row31_col71\" class=\"data row31 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row31_col72\" class=\"data row31 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row31_col73\" class=\"data row31 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row31_col74\" class=\"data row31 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row31_col75\" class=\"data row31 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row31_col76\" class=\"data row31 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row31_col77\" class=\"data row31 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row31_col78\" class=\"data row31 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row31_col79\" class=\"data row31 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row31_col80\" class=\"data row31 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row31_col81\" class=\"data row31 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row31_col82\" class=\"data row31 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row31_col83\" class=\"data row31 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row31_col84\" class=\"data row31 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row31_col85\" class=\"data row31 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row31_col86\" class=\"data row31 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row31_col87\" class=\"data row31 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row31_col88\" class=\"data row31 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row31_col89\" class=\"data row31 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row31_col90\" class=\"data row31 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row31_col91\" class=\"data row31 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row31_col92\" class=\"data row31 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row31_col93\" class=\"data row31 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row31_col94\" class=\"data row31 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row31_col95\" class=\"data row31 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row31_col96\" class=\"data row31 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row31_col97\" class=\"data row31 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row31_col98\" class=\"data row31 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row31_col99\" class=\"data row31 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row31_col100\" class=\"data row31 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row31_col101\" class=\"data row31 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_4b03d_row32_col0\" class=\"data row32 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row32_col1\" class=\"data row32 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row32_col2\" class=\"data row32 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row32_col3\" class=\"data row32 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row32_col4\" class=\"data row32 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row32_col5\" class=\"data row32 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row32_col6\" class=\"data row32 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row32_col7\" class=\"data row32 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row32_col8\" class=\"data row32 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row32_col9\" class=\"data row32 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row32_col10\" class=\"data row32 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row32_col11\" class=\"data row32 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row32_col12\" class=\"data row32 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row32_col13\" class=\"data row32 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row32_col14\" class=\"data row32 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row32_col15\" class=\"data row32 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row32_col16\" class=\"data row32 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row32_col17\" class=\"data row32 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row32_col18\" class=\"data row32 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row32_col19\" class=\"data row32 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row32_col20\" class=\"data row32 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row32_col21\" class=\"data row32 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row32_col22\" class=\"data row32 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row32_col23\" class=\"data row32 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row32_col24\" class=\"data row32 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row32_col25\" class=\"data row32 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row32_col26\" class=\"data row32 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row32_col27\" class=\"data row32 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row32_col28\" class=\"data row32 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row32_col29\" class=\"data row32 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row32_col30\" class=\"data row32 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row32_col31\" class=\"data row32 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row32_col32\" class=\"data row32 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row32_col33\" class=\"data row32 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row32_col34\" class=\"data row32 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row32_col35\" class=\"data row32 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row32_col36\" class=\"data row32 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row32_col37\" class=\"data row32 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row32_col38\" class=\"data row32 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row32_col39\" class=\"data row32 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row32_col40\" class=\"data row32 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row32_col41\" class=\"data row32 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row32_col42\" class=\"data row32 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row32_col43\" class=\"data row32 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row32_col44\" class=\"data row32 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row32_col45\" class=\"data row32 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row32_col46\" class=\"data row32 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row32_col47\" class=\"data row32 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row32_col48\" class=\"data row32 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row32_col49\" class=\"data row32 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row32_col50\" class=\"data row32 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row32_col51\" class=\"data row32 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row32_col52\" class=\"data row32 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row32_col53\" class=\"data row32 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row32_col54\" class=\"data row32 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row32_col55\" class=\"data row32 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row32_col56\" class=\"data row32 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row32_col57\" class=\"data row32 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row32_col58\" class=\"data row32 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row32_col59\" class=\"data row32 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row32_col60\" class=\"data row32 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row32_col61\" class=\"data row32 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row32_col62\" class=\"data row32 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row32_col63\" class=\"data row32 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row32_col64\" class=\"data row32 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row32_col65\" class=\"data row32 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row32_col66\" class=\"data row32 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row32_col67\" class=\"data row32 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row32_col68\" class=\"data row32 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row32_col69\" class=\"data row32 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row32_col70\" class=\"data row32 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row32_col71\" class=\"data row32 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row32_col72\" class=\"data row32 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row32_col73\" class=\"data row32 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row32_col74\" class=\"data row32 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row32_col75\" class=\"data row32 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row32_col76\" class=\"data row32 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row32_col77\" class=\"data row32 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row32_col78\" class=\"data row32 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row32_col79\" class=\"data row32 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row32_col80\" class=\"data row32 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row32_col81\" class=\"data row32 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row32_col82\" class=\"data row32 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row32_col83\" class=\"data row32 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row32_col84\" class=\"data row32 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row32_col85\" class=\"data row32 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row32_col86\" class=\"data row32 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row32_col87\" class=\"data row32 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row32_col88\" class=\"data row32 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row32_col89\" class=\"data row32 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row32_col90\" class=\"data row32 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row32_col91\" class=\"data row32 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row32_col92\" class=\"data row32 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row32_col93\" class=\"data row32 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row32_col94\" class=\"data row32 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row32_col95\" class=\"data row32 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row32_col96\" class=\"data row32 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row32_col97\" class=\"data row32 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row32_col98\" class=\"data row32 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row32_col99\" class=\"data row32 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row32_col100\" class=\"data row32 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row32_col101\" class=\"data row32 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "      <td id=\"T_4b03d_row33_col0\" class=\"data row33 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row33_col1\" class=\"data row33 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row33_col2\" class=\"data row33 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row33_col3\" class=\"data row33 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row33_col4\" class=\"data row33 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row33_col5\" class=\"data row33 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row33_col6\" class=\"data row33 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row33_col7\" class=\"data row33 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row33_col8\" class=\"data row33 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row33_col9\" class=\"data row33 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row33_col10\" class=\"data row33 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row33_col11\" class=\"data row33 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row33_col12\" class=\"data row33 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row33_col13\" class=\"data row33 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row33_col14\" class=\"data row33 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row33_col15\" class=\"data row33 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row33_col16\" class=\"data row33 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row33_col17\" class=\"data row33 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row33_col18\" class=\"data row33 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row33_col19\" class=\"data row33 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row33_col20\" class=\"data row33 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row33_col21\" class=\"data row33 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row33_col22\" class=\"data row33 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row33_col23\" class=\"data row33 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row33_col24\" class=\"data row33 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row33_col25\" class=\"data row33 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row33_col26\" class=\"data row33 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row33_col27\" class=\"data row33 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row33_col28\" class=\"data row33 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row33_col29\" class=\"data row33 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row33_col30\" class=\"data row33 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row33_col31\" class=\"data row33 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row33_col32\" class=\"data row33 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row33_col33\" class=\"data row33 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row33_col34\" class=\"data row33 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row33_col35\" class=\"data row33 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row33_col36\" class=\"data row33 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row33_col37\" class=\"data row33 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row33_col38\" class=\"data row33 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row33_col39\" class=\"data row33 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row33_col40\" class=\"data row33 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row33_col41\" class=\"data row33 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row33_col42\" class=\"data row33 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row33_col43\" class=\"data row33 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row33_col44\" class=\"data row33 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row33_col45\" class=\"data row33 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row33_col46\" class=\"data row33 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row33_col47\" class=\"data row33 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row33_col48\" class=\"data row33 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row33_col49\" class=\"data row33 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row33_col50\" class=\"data row33 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row33_col51\" class=\"data row33 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row33_col52\" class=\"data row33 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row33_col53\" class=\"data row33 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row33_col54\" class=\"data row33 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row33_col55\" class=\"data row33 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row33_col56\" class=\"data row33 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row33_col57\" class=\"data row33 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row33_col58\" class=\"data row33 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row33_col59\" class=\"data row33 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row33_col60\" class=\"data row33 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row33_col61\" class=\"data row33 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row33_col62\" class=\"data row33 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row33_col63\" class=\"data row33 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row33_col64\" class=\"data row33 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row33_col65\" class=\"data row33 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row33_col66\" class=\"data row33 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row33_col67\" class=\"data row33 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row33_col68\" class=\"data row33 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row33_col69\" class=\"data row33 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row33_col70\" class=\"data row33 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row33_col71\" class=\"data row33 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row33_col72\" class=\"data row33 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row33_col73\" class=\"data row33 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row33_col74\" class=\"data row33 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row33_col75\" class=\"data row33 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row33_col76\" class=\"data row33 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row33_col77\" class=\"data row33 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row33_col78\" class=\"data row33 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row33_col79\" class=\"data row33 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row33_col80\" class=\"data row33 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row33_col81\" class=\"data row33 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row33_col82\" class=\"data row33 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row33_col83\" class=\"data row33 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row33_col84\" class=\"data row33 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row33_col85\" class=\"data row33 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row33_col86\" class=\"data row33 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row33_col87\" class=\"data row33 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row33_col88\" class=\"data row33 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row33_col89\" class=\"data row33 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row33_col90\" class=\"data row33 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row33_col91\" class=\"data row33 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row33_col92\" class=\"data row33 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row33_col93\" class=\"data row33 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row33_col94\" class=\"data row33 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row33_col95\" class=\"data row33 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row33_col96\" class=\"data row33 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row33_col97\" class=\"data row33 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row33_col98\" class=\"data row33 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row33_col99\" class=\"data row33 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row33_col100\" class=\"data row33 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row33_col101\" class=\"data row33 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "      <td id=\"T_4b03d_row34_col0\" class=\"data row34 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row34_col1\" class=\"data row34 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row34_col2\" class=\"data row34 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row34_col3\" class=\"data row34 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row34_col4\" class=\"data row34 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row34_col5\" class=\"data row34 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row34_col6\" class=\"data row34 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row34_col7\" class=\"data row34 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row34_col8\" class=\"data row34 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row34_col9\" class=\"data row34 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row34_col10\" class=\"data row34 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row34_col11\" class=\"data row34 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row34_col12\" class=\"data row34 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row34_col13\" class=\"data row34 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row34_col14\" class=\"data row34 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row34_col15\" class=\"data row34 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row34_col16\" class=\"data row34 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row34_col17\" class=\"data row34 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row34_col18\" class=\"data row34 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row34_col19\" class=\"data row34 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row34_col20\" class=\"data row34 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row34_col21\" class=\"data row34 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row34_col22\" class=\"data row34 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row34_col23\" class=\"data row34 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row34_col24\" class=\"data row34 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row34_col25\" class=\"data row34 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row34_col26\" class=\"data row34 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row34_col27\" class=\"data row34 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row34_col28\" class=\"data row34 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row34_col29\" class=\"data row34 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row34_col30\" class=\"data row34 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row34_col31\" class=\"data row34 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row34_col32\" class=\"data row34 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row34_col33\" class=\"data row34 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row34_col34\" class=\"data row34 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row34_col35\" class=\"data row34 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row34_col36\" class=\"data row34 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row34_col37\" class=\"data row34 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row34_col38\" class=\"data row34 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row34_col39\" class=\"data row34 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row34_col40\" class=\"data row34 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row34_col41\" class=\"data row34 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row34_col42\" class=\"data row34 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row34_col43\" class=\"data row34 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row34_col44\" class=\"data row34 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row34_col45\" class=\"data row34 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row34_col46\" class=\"data row34 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row34_col47\" class=\"data row34 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row34_col48\" class=\"data row34 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row34_col49\" class=\"data row34 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row34_col50\" class=\"data row34 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row34_col51\" class=\"data row34 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row34_col52\" class=\"data row34 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row34_col53\" class=\"data row34 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row34_col54\" class=\"data row34 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row34_col55\" class=\"data row34 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row34_col56\" class=\"data row34 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row34_col57\" class=\"data row34 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row34_col58\" class=\"data row34 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row34_col59\" class=\"data row34 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row34_col60\" class=\"data row34 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row34_col61\" class=\"data row34 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row34_col62\" class=\"data row34 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row34_col63\" class=\"data row34 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row34_col64\" class=\"data row34 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row34_col65\" class=\"data row34 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row34_col66\" class=\"data row34 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row34_col67\" class=\"data row34 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row34_col68\" class=\"data row34 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row34_col69\" class=\"data row34 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row34_col70\" class=\"data row34 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row34_col71\" class=\"data row34 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row34_col72\" class=\"data row34 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row34_col73\" class=\"data row34 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row34_col74\" class=\"data row34 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row34_col75\" class=\"data row34 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row34_col76\" class=\"data row34 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row34_col77\" class=\"data row34 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row34_col78\" class=\"data row34 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row34_col79\" class=\"data row34 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row34_col80\" class=\"data row34 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row34_col81\" class=\"data row34 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row34_col82\" class=\"data row34 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row34_col83\" class=\"data row34 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row34_col84\" class=\"data row34 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row34_col85\" class=\"data row34 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row34_col86\" class=\"data row34 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row34_col87\" class=\"data row34 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row34_col88\" class=\"data row34 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row34_col89\" class=\"data row34 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row34_col90\" class=\"data row34 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row34_col91\" class=\"data row34 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row34_col92\" class=\"data row34 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row34_col93\" class=\"data row34 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row34_col94\" class=\"data row34 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row34_col95\" class=\"data row34 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row34_col96\" class=\"data row34 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row34_col97\" class=\"data row34 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row34_col98\" class=\"data row34 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row34_col99\" class=\"data row34 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row34_col100\" class=\"data row34 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row34_col101\" class=\"data row34 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "      <td id=\"T_4b03d_row35_col0\" class=\"data row35 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row35_col1\" class=\"data row35 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row35_col2\" class=\"data row35 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row35_col3\" class=\"data row35 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row35_col4\" class=\"data row35 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row35_col5\" class=\"data row35 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row35_col6\" class=\"data row35 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row35_col7\" class=\"data row35 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row35_col8\" class=\"data row35 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row35_col9\" class=\"data row35 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row35_col10\" class=\"data row35 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row35_col11\" class=\"data row35 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row35_col12\" class=\"data row35 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row35_col13\" class=\"data row35 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row35_col14\" class=\"data row35 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row35_col15\" class=\"data row35 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row35_col16\" class=\"data row35 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row35_col17\" class=\"data row35 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row35_col18\" class=\"data row35 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row35_col19\" class=\"data row35 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row35_col20\" class=\"data row35 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row35_col21\" class=\"data row35 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row35_col22\" class=\"data row35 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row35_col23\" class=\"data row35 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row35_col24\" class=\"data row35 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row35_col25\" class=\"data row35 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row35_col26\" class=\"data row35 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row35_col27\" class=\"data row35 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row35_col28\" class=\"data row35 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row35_col29\" class=\"data row35 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row35_col30\" class=\"data row35 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row35_col31\" class=\"data row35 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row35_col32\" class=\"data row35 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row35_col33\" class=\"data row35 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row35_col34\" class=\"data row35 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row35_col35\" class=\"data row35 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row35_col36\" class=\"data row35 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row35_col37\" class=\"data row35 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row35_col38\" class=\"data row35 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row35_col39\" class=\"data row35 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row35_col40\" class=\"data row35 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row35_col41\" class=\"data row35 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row35_col42\" class=\"data row35 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row35_col43\" class=\"data row35 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row35_col44\" class=\"data row35 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row35_col45\" class=\"data row35 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row35_col46\" class=\"data row35 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row35_col47\" class=\"data row35 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row35_col48\" class=\"data row35 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row35_col49\" class=\"data row35 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row35_col50\" class=\"data row35 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row35_col51\" class=\"data row35 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row35_col52\" class=\"data row35 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row35_col53\" class=\"data row35 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row35_col54\" class=\"data row35 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row35_col55\" class=\"data row35 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row35_col56\" class=\"data row35 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row35_col57\" class=\"data row35 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row35_col58\" class=\"data row35 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row35_col59\" class=\"data row35 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row35_col60\" class=\"data row35 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row35_col61\" class=\"data row35 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row35_col62\" class=\"data row35 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row35_col63\" class=\"data row35 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row35_col64\" class=\"data row35 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row35_col65\" class=\"data row35 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row35_col66\" class=\"data row35 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row35_col67\" class=\"data row35 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row35_col68\" class=\"data row35 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row35_col69\" class=\"data row35 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row35_col70\" class=\"data row35 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row35_col71\" class=\"data row35 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row35_col72\" class=\"data row35 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row35_col73\" class=\"data row35 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row35_col74\" class=\"data row35 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row35_col75\" class=\"data row35 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row35_col76\" class=\"data row35 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row35_col77\" class=\"data row35 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row35_col78\" class=\"data row35 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row35_col79\" class=\"data row35 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row35_col80\" class=\"data row35 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row35_col81\" class=\"data row35 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row35_col82\" class=\"data row35 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row35_col83\" class=\"data row35 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row35_col84\" class=\"data row35 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row35_col85\" class=\"data row35 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row35_col86\" class=\"data row35 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row35_col87\" class=\"data row35 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row35_col88\" class=\"data row35 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row35_col89\" class=\"data row35 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row35_col90\" class=\"data row35 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row35_col91\" class=\"data row35 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row35_col92\" class=\"data row35 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row35_col93\" class=\"data row35 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row35_col94\" class=\"data row35 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row35_col95\" class=\"data row35 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row35_col96\" class=\"data row35 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row35_col97\" class=\"data row35 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row35_col98\" class=\"data row35 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row35_col99\" class=\"data row35 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row35_col100\" class=\"data row35 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row35_col101\" class=\"data row35 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "      <td id=\"T_4b03d_row36_col0\" class=\"data row36 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row36_col1\" class=\"data row36 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row36_col2\" class=\"data row36 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row36_col3\" class=\"data row36 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row36_col4\" class=\"data row36 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row36_col5\" class=\"data row36 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row36_col6\" class=\"data row36 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row36_col7\" class=\"data row36 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row36_col8\" class=\"data row36 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row36_col9\" class=\"data row36 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row36_col10\" class=\"data row36 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row36_col11\" class=\"data row36 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row36_col12\" class=\"data row36 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row36_col13\" class=\"data row36 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row36_col14\" class=\"data row36 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row36_col15\" class=\"data row36 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row36_col16\" class=\"data row36 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row36_col17\" class=\"data row36 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row36_col18\" class=\"data row36 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row36_col19\" class=\"data row36 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row36_col20\" class=\"data row36 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row36_col21\" class=\"data row36 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row36_col22\" class=\"data row36 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row36_col23\" class=\"data row36 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row36_col24\" class=\"data row36 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row36_col25\" class=\"data row36 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row36_col26\" class=\"data row36 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row36_col27\" class=\"data row36 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row36_col28\" class=\"data row36 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row36_col29\" class=\"data row36 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row36_col30\" class=\"data row36 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row36_col31\" class=\"data row36 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row36_col32\" class=\"data row36 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row36_col33\" class=\"data row36 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row36_col34\" class=\"data row36 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row36_col35\" class=\"data row36 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row36_col36\" class=\"data row36 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row36_col37\" class=\"data row36 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row36_col38\" class=\"data row36 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row36_col39\" class=\"data row36 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row36_col40\" class=\"data row36 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row36_col41\" class=\"data row36 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row36_col42\" class=\"data row36 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row36_col43\" class=\"data row36 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row36_col44\" class=\"data row36 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row36_col45\" class=\"data row36 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row36_col46\" class=\"data row36 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row36_col47\" class=\"data row36 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row36_col48\" class=\"data row36 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row36_col49\" class=\"data row36 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row36_col50\" class=\"data row36 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row36_col51\" class=\"data row36 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row36_col52\" class=\"data row36 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row36_col53\" class=\"data row36 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row36_col54\" class=\"data row36 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row36_col55\" class=\"data row36 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row36_col56\" class=\"data row36 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row36_col57\" class=\"data row36 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row36_col58\" class=\"data row36 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row36_col59\" class=\"data row36 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row36_col60\" class=\"data row36 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row36_col61\" class=\"data row36 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row36_col62\" class=\"data row36 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row36_col63\" class=\"data row36 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row36_col64\" class=\"data row36 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row36_col65\" class=\"data row36 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row36_col66\" class=\"data row36 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row36_col67\" class=\"data row36 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row36_col68\" class=\"data row36 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row36_col69\" class=\"data row36 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row36_col70\" class=\"data row36 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row36_col71\" class=\"data row36 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row36_col72\" class=\"data row36 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row36_col73\" class=\"data row36 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row36_col74\" class=\"data row36 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row36_col75\" class=\"data row36 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row36_col76\" class=\"data row36 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row36_col77\" class=\"data row36 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row36_col78\" class=\"data row36 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row36_col79\" class=\"data row36 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row36_col80\" class=\"data row36 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row36_col81\" class=\"data row36 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row36_col82\" class=\"data row36 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row36_col83\" class=\"data row36 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row36_col84\" class=\"data row36 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row36_col85\" class=\"data row36 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row36_col86\" class=\"data row36 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row36_col87\" class=\"data row36 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row36_col88\" class=\"data row36 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row36_col89\" class=\"data row36 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row36_col90\" class=\"data row36 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row36_col91\" class=\"data row36 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row36_col92\" class=\"data row36 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row36_col93\" class=\"data row36 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row36_col94\" class=\"data row36 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row36_col95\" class=\"data row36 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row36_col96\" class=\"data row36 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row36_col97\" class=\"data row36 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row36_col98\" class=\"data row36 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row36_col99\" class=\"data row36 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row36_col100\" class=\"data row36 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row36_col101\" class=\"data row36 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "      <td id=\"T_4b03d_row37_col0\" class=\"data row37 col0\" >Bond Order Modification</td>\n",
              "      <td id=\"T_4b03d_row37_col1\" class=\"data row37 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row37_col2\" class=\"data row37 col2\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row37_col3\" class=\"data row37 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COC(=O)c2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row37_col4\" class=\"data row37 col4\" >845.939000</td>\n",
              "      <td id=\"T_4b03d_row37_col5\" class=\"data row37 col5\" >4.043700</td>\n",
              "      <td id=\"T_4b03d_row37_col6\" class=\"data row37 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row37_col7\" class=\"data row37 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row37_col8\" class=\"data row37 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row37_col9\" class=\"data row37 col9\" >0.105117</td>\n",
              "      <td id=\"T_4b03d_row37_col10\" class=\"data row37 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row37_col11\" class=\"data row37 col11\" >232.560000</td>\n",
              "      <td id=\"T_4b03d_row37_col12\" class=\"data row37 col12\" >0.354950</td>\n",
              "      <td id=\"T_4b03d_row37_col13\" class=\"data row37 col13\" >0.328992</td>\n",
              "      <td id=\"T_4b03d_row37_col14\" class=\"data row37 col14\" >0.684066</td>\n",
              "      <td id=\"T_4b03d_row37_col15\" class=\"data row37 col15\" >0.011654</td>\n",
              "      <td id=\"T_4b03d_row37_col16\" class=\"data row37 col16\" >0.109883</td>\n",
              "      <td id=\"T_4b03d_row37_col17\" class=\"data row37 col17\" >0.060157</td>\n",
              "      <td id=\"T_4b03d_row37_col18\" class=\"data row37 col18\" >0.234820</td>\n",
              "      <td id=\"T_4b03d_row37_col19\" class=\"data row37 col19\" >0.031991</td>\n",
              "      <td id=\"T_4b03d_row37_col20\" class=\"data row37 col20\" >0.106238</td>\n",
              "      <td id=\"T_4b03d_row37_col21\" class=\"data row37 col21\" >0.671410</td>\n",
              "      <td id=\"T_4b03d_row37_col22\" class=\"data row37 col22\" >0.790138</td>\n",
              "      <td id=\"T_4b03d_row37_col23\" class=\"data row37 col23\" >0.125745</td>\n",
              "      <td id=\"T_4b03d_row37_col24\" class=\"data row37 col24\" >0.264813</td>\n",
              "      <td id=\"T_4b03d_row37_col25\" class=\"data row37 col25\" >0.799514</td>\n",
              "      <td id=\"T_4b03d_row37_col26\" class=\"data row37 col26\" >0.987738</td>\n",
              "      <td id=\"T_4b03d_row37_col27\" class=\"data row37 col27\" >0.115652</td>\n",
              "      <td id=\"T_4b03d_row37_col28\" class=\"data row37 col28\" >0.234478</td>\n",
              "      <td id=\"T_4b03d_row37_col29\" class=\"data row37 col29\" >0.080198</td>\n",
              "      <td id=\"T_4b03d_row37_col30\" class=\"data row37 col30\" >0.224710</td>\n",
              "      <td id=\"T_4b03d_row37_col31\" class=\"data row37 col31\" >0.020642</td>\n",
              "      <td id=\"T_4b03d_row37_col32\" class=\"data row37 col32\" >0.140019</td>\n",
              "      <td id=\"T_4b03d_row37_col33\" class=\"data row37 col33\" >0.079980</td>\n",
              "      <td id=\"T_4b03d_row37_col34\" class=\"data row37 col34\" >0.597461</td>\n",
              "      <td id=\"T_4b03d_row37_col35\" class=\"data row37 col35\" >0.812413</td>\n",
              "      <td id=\"T_4b03d_row37_col36\" class=\"data row37 col36\" >0.316719</td>\n",
              "      <td id=\"T_4b03d_row37_col37\" class=\"data row37 col37\" >0.186077</td>\n",
              "      <td id=\"T_4b03d_row37_col38\" class=\"data row37 col38\" >0.053383</td>\n",
              "      <td id=\"T_4b03d_row37_col39\" class=\"data row37 col39\" >0.906298</td>\n",
              "      <td id=\"T_4b03d_row37_col40\" class=\"data row37 col40\" >0.238380</td>\n",
              "      <td id=\"T_4b03d_row37_col41\" class=\"data row37 col41\" >0.094029</td>\n",
              "      <td id=\"T_4b03d_row37_col42\" class=\"data row37 col42\" >0.665380</td>\n",
              "      <td id=\"T_4b03d_row37_col43\" class=\"data row37 col43\" >-5.539634</td>\n",
              "      <td id=\"T_4b03d_row37_col44\" class=\"data row37 col44\" >131.166742</td>\n",
              "      <td id=\"T_4b03d_row37_col45\" class=\"data row37 col45\" >113.280659</td>\n",
              "      <td id=\"T_4b03d_row37_col46\" class=\"data row37 col46\" >37.475863</td>\n",
              "      <td id=\"T_4b03d_row37_col47\" class=\"data row37 col47\" >-10.433183</td>\n",
              "      <td id=\"T_4b03d_row37_col48\" class=\"data row37 col48\" >4.081983</td>\n",
              "      <td id=\"T_4b03d_row37_col49\" class=\"data row37 col49\" >2.899766</td>\n",
              "      <td id=\"T_4b03d_row37_col50\" class=\"data row37 col50\" >103.472366</td>\n",
              "      <td id=\"T_4b03d_row37_col51\" class=\"data row37 col51\" >-5.996270</td>\n",
              "      <td id=\"T_4b03d_row37_col52\" class=\"data row37 col52\" >3.497114</td>\n",
              "      <td id=\"T_4b03d_row37_col53\" class=\"data row37 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row37_col54\" class=\"data row37 col54\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row37_col55\" class=\"data row37 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row37_col56\" class=\"data row37 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row37_col57\" class=\"data row37 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row37_col58\" class=\"data row37 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row37_col59\" class=\"data row37 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row37_col60\" class=\"data row37 col60\" >94.106243</td>\n",
              "      <td id=\"T_4b03d_row37_col61\" class=\"data row37 col61\" >72.120977</td>\n",
              "      <td id=\"T_4b03d_row37_col62\" class=\"data row37 col62\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row37_col63\" class=\"data row37 col63\" >34.082978</td>\n",
              "      <td id=\"T_4b03d_row37_col64\" class=\"data row37 col64\" >35.246219</td>\n",
              "      <td id=\"T_4b03d_row37_col65\" class=\"data row37 col65\" >51.298953</td>\n",
              "      <td id=\"T_4b03d_row37_col66\" class=\"data row37 col66\" >21.713843</td>\n",
              "      <td id=\"T_4b03d_row37_col67\" class=\"data row37 col67\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row37_col68\" class=\"data row37 col68\" >23.613804</td>\n",
              "      <td id=\"T_4b03d_row37_col69\" class=\"data row37 col69\" >62.970143</td>\n",
              "      <td id=\"T_4b03d_row37_col70\" class=\"data row37 col70\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row37_col71\" class=\"data row37 col71\" >90.345095</td>\n",
              "      <td id=\"T_4b03d_row37_col72\" class=\"data row37 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row37_col73\" class=\"data row37 col73\" >73.478092</td>\n",
              "      <td id=\"T_4b03d_row37_col74\" class=\"data row37 col74\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row37_col75\" class=\"data row37 col75\" >40.170609</td>\n",
              "      <td id=\"T_4b03d_row37_col76\" class=\"data row37 col76\" >91.392012</td>\n",
              "      <td id=\"T_4b03d_row37_col77\" class=\"data row37 col77\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row37_col78\" class=\"data row37 col78\" >68.282280</td>\n",
              "      <td id=\"T_4b03d_row37_col79\" class=\"data row37 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row37_col80\" class=\"data row37 col80\" >54.284606</td>\n",
              "      <td id=\"T_4b03d_row37_col81\" class=\"data row37 col81\" >68.243505</td>\n",
              "      <td id=\"T_4b03d_row37_col82\" class=\"data row37 col82\" >89.181853</td>\n",
              "      <td id=\"T_4b03d_row37_col83\" class=\"data row37 col83\" >39.317565</td>\n",
              "      <td id=\"T_4b03d_row37_col84\" class=\"data row37 col84\" >88.367584</td>\n",
              "      <td id=\"T_4b03d_row37_col85\" class=\"data row37 col85\" >72.625048</td>\n",
              "      <td id=\"T_4b03d_row37_col86\" class=\"data row37 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row37_col87\" class=\"data row37 col87\" >75.571927</td>\n",
              "      <td id=\"T_4b03d_row37_col88\" class=\"data row37 col88\" >97.906165</td>\n",
              "      <td id=\"T_4b03d_row37_col89\" class=\"data row37 col89\" >87.824738</td>\n",
              "      <td id=\"T_4b03d_row37_col90\" class=\"data row37 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row37_col91\" class=\"data row37 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row37_col92\" class=\"data row37 col92\" >22.062815</td>\n",
              "      <td id=\"T_4b03d_row37_col93\" class=\"data row37 col93\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row37_col94\" class=\"data row37 col94\" >97.246995</td>\n",
              "      <td id=\"T_4b03d_row37_col95\" class=\"data row37 col95\" >80.767739</td>\n",
              "      <td id=\"T_4b03d_row37_col96\" class=\"data row37 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row37_col97\" class=\"data row37 col97\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row37_col98\" class=\"data row37 col98\" >78.053509</td>\n",
              "      <td id=\"T_4b03d_row37_col99\" class=\"data row37 col99\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row37_col100\" class=\"data row37 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row37_col101\" class=\"data row37 col101\" >64.753781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "      <td id=\"T_4b03d_row38_col0\" class=\"data row38 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row38_col1\" class=\"data row38 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](CO)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1</td>\n",
              "      <td id=\"T_4b03d_row38_col2\" class=\"data row38 col2\" >2.658800</td>\n",
              "      <td id=\"T_4b03d_row38_col3\" class=\"data row38 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](CO)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1</td>\n",
              "      <td id=\"T_4b03d_row38_col4\" class=\"data row38 col4\" >837.960000</td>\n",
              "      <td id=\"T_4b03d_row38_col5\" class=\"data row38 col5\" >3.040900</td>\n",
              "      <td id=\"T_4b03d_row38_col6\" class=\"data row38 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row38_col7\" class=\"data row38 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row38_col8\" class=\"data row38 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row38_col9\" class=\"data row38 col9\" >0.103835</td>\n",
              "      <td id=\"T_4b03d_row38_col10\" class=\"data row38 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row38_col11\" class=\"data row38 col11\" >257.990000</td>\n",
              "      <td id=\"T_4b03d_row38_col12\" class=\"data row38 col12\" >0.303418</td>\n",
              "      <td id=\"T_4b03d_row38_col13\" class=\"data row38 col13\" >0.262174</td>\n",
              "      <td id=\"T_4b03d_row38_col14\" class=\"data row38 col14\" >0.661619</td>\n",
              "      <td id=\"T_4b03d_row38_col15\" class=\"data row38 col15\" >0.014575</td>\n",
              "      <td id=\"T_4b03d_row38_col16\" class=\"data row38 col16\" >0.310586</td>\n",
              "      <td id=\"T_4b03d_row38_col17\" class=\"data row38 col17\" >0.134327</td>\n",
              "      <td id=\"T_4b03d_row38_col18\" class=\"data row38 col18\" >0.414101</td>\n",
              "      <td id=\"T_4b03d_row38_col19\" class=\"data row38 col19\" >0.064300</td>\n",
              "      <td id=\"T_4b03d_row38_col20\" class=\"data row38 col20\" >0.183172</td>\n",
              "      <td id=\"T_4b03d_row38_col21\" class=\"data row38 col21\" >0.680783</td>\n",
              "      <td id=\"T_4b03d_row38_col22\" class=\"data row38 col22\" >0.852423</td>\n",
              "      <td id=\"T_4b03d_row38_col23\" class=\"data row38 col23\" >0.159278</td>\n",
              "      <td id=\"T_4b03d_row38_col24\" class=\"data row38 col24\" >0.248542</td>\n",
              "      <td id=\"T_4b03d_row38_col25\" class=\"data row38 col25\" >0.762143</td>\n",
              "      <td id=\"T_4b03d_row38_col26\" class=\"data row38 col26\" >0.986938</td>\n",
              "      <td id=\"T_4b03d_row38_col27\" class=\"data row38 col27\" >0.046066</td>\n",
              "      <td id=\"T_4b03d_row38_col28\" class=\"data row38 col28\" >0.060450</td>\n",
              "      <td id=\"T_4b03d_row38_col29\" class=\"data row38 col29\" >0.053649</td>\n",
              "      <td id=\"T_4b03d_row38_col30\" class=\"data row38 col30\" >0.103505</td>\n",
              "      <td id=\"T_4b03d_row38_col31\" class=\"data row38 col31\" >0.015579</td>\n",
              "      <td id=\"T_4b03d_row38_col32\" class=\"data row38 col32\" >0.099682</td>\n",
              "      <td id=\"T_4b03d_row38_col33\" class=\"data row38 col33\" >0.044941</td>\n",
              "      <td id=\"T_4b03d_row38_col34\" class=\"data row38 col34\" >0.577148</td>\n",
              "      <td id=\"T_4b03d_row38_col35\" class=\"data row38 col35\" >0.902206</td>\n",
              "      <td id=\"T_4b03d_row38_col36\" class=\"data row38 col36\" >0.282196</td>\n",
              "      <td id=\"T_4b03d_row38_col37\" class=\"data row38 col37\" >0.081805</td>\n",
              "      <td id=\"T_4b03d_row38_col38\" class=\"data row38 col38\" >0.049415</td>\n",
              "      <td id=\"T_4b03d_row38_col39\" class=\"data row38 col39\" >0.820394</td>\n",
              "      <td id=\"T_4b03d_row38_col40\" class=\"data row38 col40\" >0.118501</td>\n",
              "      <td id=\"T_4b03d_row38_col41\" class=\"data row38 col41\" >0.112268</td>\n",
              "      <td id=\"T_4b03d_row38_col42\" class=\"data row38 col42\" >0.773427</td>\n",
              "      <td id=\"T_4b03d_row38_col43\" class=\"data row38 col43\" >-5.559864</td>\n",
              "      <td id=\"T_4b03d_row38_col44\" class=\"data row38 col44\" >113.429253</td>\n",
              "      <td id=\"T_4b03d_row38_col45\" class=\"data row38 col45\" >91.319709</td>\n",
              "      <td id=\"T_4b03d_row38_col46\" class=\"data row38 col46\" >36.673275</td>\n",
              "      <td id=\"T_4b03d_row38_col47\" class=\"data row38 col47\" >-11.336277</td>\n",
              "      <td id=\"T_4b03d_row38_col48\" class=\"data row38 col48\" >4.640446</td>\n",
              "      <td id=\"T_4b03d_row38_col49\" class=\"data row38 col49\" >2.658800</td>\n",
              "      <td id=\"T_4b03d_row38_col50\" class=\"data row38 col50\" >100.120279</td>\n",
              "      <td id=\"T_4b03d_row38_col51\" class=\"data row38 col51\" >-4.201170</td>\n",
              "      <td id=\"T_4b03d_row38_col52\" class=\"data row38 col52\" >2.242696</td>\n",
              "      <td id=\"T_4b03d_row38_col53\" class=\"data row38 col53\" >94.920512</td>\n",
              "      <td id=\"T_4b03d_row38_col54\" class=\"data row38 col54\" >62.349748</td>\n",
              "      <td id=\"T_4b03d_row38_col55\" class=\"data row38 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row38_col56\" class=\"data row38 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row38_col57\" class=\"data row38 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row38_col58\" class=\"data row38 col58\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row38_col59\" class=\"data row38 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row38_col60\" class=\"data row38 col60\" >94.687864</td>\n",
              "      <td id=\"T_4b03d_row38_col61\" class=\"data row38 col61\" >66.731291</td>\n",
              "      <td id=\"T_4b03d_row38_col62\" class=\"data row38 col62\" >16.362931</td>\n",
              "      <td id=\"T_4b03d_row38_col63\" class=\"data row38 col63\" >31.252423</td>\n",
              "      <td id=\"T_4b03d_row38_col64\" class=\"data row38 col64\" >37.533928</td>\n",
              "      <td id=\"T_4b03d_row38_col65\" class=\"data row38 col65\" >73.322993</td>\n",
              "      <td id=\"T_4b03d_row38_col66\" class=\"data row38 col66\" >46.103141</td>\n",
              "      <td id=\"T_4b03d_row38_col67\" class=\"data row38 col67\" >87.204343</td>\n",
              "      <td id=\"T_4b03d_row38_col68\" class=\"data row38 col68\" >38.852268</td>\n",
              "      <td id=\"T_4b03d_row38_col69\" class=\"data row38 col69\" >72.276076</td>\n",
              "      <td id=\"T_4b03d_row38_col70\" class=\"data row38 col70\" >70.453664</td>\n",
              "      <td id=\"T_4b03d_row38_col71\" class=\"data row38 col71\" >92.826677</td>\n",
              "      <td id=\"T_4b03d_row38_col72\" class=\"data row38 col72\" >50.833656</td>\n",
              "      <td id=\"T_4b03d_row38_col73\" class=\"data row38 col73\" >71.888329</td>\n",
              "      <td id=\"T_4b03d_row38_col74\" class=\"data row38 col74\" >68.786351</td>\n",
              "      <td id=\"T_4b03d_row38_col75\" class=\"data row38 col75\" >39.511439</td>\n",
              "      <td id=\"T_4b03d_row38_col76\" class=\"data row38 col76\" >83.404420</td>\n",
              "      <td id=\"T_4b03d_row38_col77\" class=\"data row38 col77\" >77.937185</td>\n",
              "      <td id=\"T_4b03d_row38_col78\" class=\"data row38 col78\" >62.155874</td>\n",
              "      <td id=\"T_4b03d_row38_col79\" class=\"data row38 col79\" >73.671966</td>\n",
              "      <td id=\"T_4b03d_row38_col80\" class=\"data row38 col80\" >46.645987</td>\n",
              "      <td id=\"T_4b03d_row38_col81\" class=\"data row38 col81\" >55.447848</td>\n",
              "      <td id=\"T_4b03d_row38_col82\" class=\"data row38 col82\" >82.318728</td>\n",
              "      <td id=\"T_4b03d_row38_col83\" class=\"data row38 col83\" >37.727801</td>\n",
              "      <td id=\"T_4b03d_row38_col84\" class=\"data row38 col84\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row38_col85\" class=\"data row38 col85\" >69.910818</td>\n",
              "      <td id=\"T_4b03d_row38_col86\" class=\"data row38 col86\" >86.079876</td>\n",
              "      <td id=\"T_4b03d_row38_col87\" class=\"data row38 col87\" >74.718883</td>\n",
              "      <td id=\"T_4b03d_row38_col88\" class=\"data row38 col88\" >95.967429</td>\n",
              "      <td id=\"T_4b03d_row38_col89\" class=\"data row38 col89\" >77.626987</td>\n",
              "      <td id=\"T_4b03d_row38_col90\" class=\"data row38 col90\" >7.056999</td>\n",
              "      <td id=\"T_4b03d_row38_col91\" class=\"data row38 col91\" >76.618845</td>\n",
              "      <td id=\"T_4b03d_row38_col92\" class=\"data row38 col92\" >21.558744</td>\n",
              "      <td id=\"T_4b03d_row38_col93\" class=\"data row38 col93\" >93.796045</td>\n",
              "      <td id=\"T_4b03d_row38_col94\" class=\"data row38 col94\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row38_col95\" class=\"data row38 col95\" >80.496316</td>\n",
              "      <td id=\"T_4b03d_row38_col96\" class=\"data row38 col96\" >33.113610</td>\n",
              "      <td id=\"T_4b03d_row38_col97\" class=\"data row38 col97\" >99.728577</td>\n",
              "      <td id=\"T_4b03d_row38_col98\" class=\"data row38 col98\" >73.167895</td>\n",
              "      <td id=\"T_4b03d_row38_col99\" class=\"data row38 col99\" >91.120589</td>\n",
              "      <td id=\"T_4b03d_row38_col100\" class=\"data row38 col100\" >27.568825</td>\n",
              "      <td id=\"T_4b03d_row38_col101\" class=\"data row38 col101\" >56.068244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "      <td id=\"T_4b03d_row39_col0\" class=\"data row39 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row39_col1\" class=\"data row39 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row39_col2\" class=\"data row39 col2\" >3.048938</td>\n",
              "      <td id=\"T_4b03d_row39_col3\" class=\"data row39 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row39_col4\" class=\"data row39 col4\" >831.956000</td>\n",
              "      <td id=\"T_4b03d_row39_col5\" class=\"data row39 col5\" >4.403500</td>\n",
              "      <td id=\"T_4b03d_row39_col6\" class=\"data row39 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row39_col7\" class=\"data row39 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row39_col8\" class=\"data row39 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row39_col9\" class=\"data row39 col9\" >0.098075</td>\n",
              "      <td id=\"T_4b03d_row39_col10\" class=\"data row39 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row39_col11\" class=\"data row39 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row39_col12\" class=\"data row39 col12\" >0.389305</td>\n",
              "      <td id=\"T_4b03d_row39_col13\" class=\"data row39 col13\" >0.307290</td>\n",
              "      <td id=\"T_4b03d_row39_col14\" class=\"data row39 col14\" >0.702164</td>\n",
              "      <td id=\"T_4b03d_row39_col15\" class=\"data row39 col15\" >0.009749</td>\n",
              "      <td id=\"T_4b03d_row39_col16\" class=\"data row39 col16\" >0.120593</td>\n",
              "      <td id=\"T_4b03d_row39_col17\" class=\"data row39 col17\" >0.054614</td>\n",
              "      <td id=\"T_4b03d_row39_col18\" class=\"data row39 col18\" >0.218046</td>\n",
              "      <td id=\"T_4b03d_row39_col19\" class=\"data row39 col19\" >0.036732</td>\n",
              "      <td id=\"T_4b03d_row39_col20\" class=\"data row39 col20\" >0.096605</td>\n",
              "      <td id=\"T_4b03d_row39_col21\" class=\"data row39 col21\" >0.689106</td>\n",
              "      <td id=\"T_4b03d_row39_col22\" class=\"data row39 col22\" >0.829982</td>\n",
              "      <td id=\"T_4b03d_row39_col23\" class=\"data row39 col23\" >0.124533</td>\n",
              "      <td id=\"T_4b03d_row39_col24\" class=\"data row39 col24\" >0.250544</td>\n",
              "      <td id=\"T_4b03d_row39_col25\" class=\"data row39 col25\" >0.735480</td>\n",
              "      <td id=\"T_4b03d_row39_col26\" class=\"data row39 col26\" >0.991387</td>\n",
              "      <td id=\"T_4b03d_row39_col27\" class=\"data row39 col27\" >0.130432</td>\n",
              "      <td id=\"T_4b03d_row39_col28\" class=\"data row39 col28\" >0.295479</td>\n",
              "      <td id=\"T_4b03d_row39_col29\" class=\"data row39 col29\" >0.083397</td>\n",
              "      <td id=\"T_4b03d_row39_col30\" class=\"data row39 col30\" >0.219811</td>\n",
              "      <td id=\"T_4b03d_row39_col31\" class=\"data row39 col31\" >0.023062</td>\n",
              "      <td id=\"T_4b03d_row39_col32\" class=\"data row39 col32\" >0.165130</td>\n",
              "      <td id=\"T_4b03d_row39_col33\" class=\"data row39 col33\" >0.072653</td>\n",
              "      <td id=\"T_4b03d_row39_col34\" class=\"data row39 col34\" >0.630727</td>\n",
              "      <td id=\"T_4b03d_row39_col35\" class=\"data row39 col35\" >0.828670</td>\n",
              "      <td id=\"T_4b03d_row39_col36\" class=\"data row39 col36\" >0.346321</td>\n",
              "      <td id=\"T_4b03d_row39_col37\" class=\"data row39 col37\" >0.183336</td>\n",
              "      <td id=\"T_4b03d_row39_col38\" class=\"data row39 col38\" >0.049563</td>\n",
              "      <td id=\"T_4b03d_row39_col39\" class=\"data row39 col39\" >0.918577</td>\n",
              "      <td id=\"T_4b03d_row39_col40\" class=\"data row39 col40\" >0.244177</td>\n",
              "      <td id=\"T_4b03d_row39_col41\" class=\"data row39 col41\" >0.100463</td>\n",
              "      <td id=\"T_4b03d_row39_col42\" class=\"data row39 col42\" >0.663048</td>\n",
              "      <td id=\"T_4b03d_row39_col43\" class=\"data row39 col43\" >-5.522251</td>\n",
              "      <td id=\"T_4b03d_row39_col44\" class=\"data row39 col44\" >131.653231</td>\n",
              "      <td id=\"T_4b03d_row39_col45\" class=\"data row39 col45\" >112.352111</td>\n",
              "      <td id=\"T_4b03d_row39_col46\" class=\"data row39 col46\" >34.266984</td>\n",
              "      <td id=\"T_4b03d_row39_col47\" class=\"data row39 col47\" >-10.434780</td>\n",
              "      <td id=\"T_4b03d_row39_col48\" class=\"data row39 col48\" >4.389839</td>\n",
              "      <td id=\"T_4b03d_row39_col49\" class=\"data row39 col49\" >3.048938</td>\n",
              "      <td id=\"T_4b03d_row39_col50\" class=\"data row39 col50\" >103.038977</td>\n",
              "      <td id=\"T_4b03d_row39_col51\" class=\"data row39 col51\" >-5.815164</td>\n",
              "      <td id=\"T_4b03d_row39_col52\" class=\"data row39 col52\" >3.782678</td>\n",
              "      <td id=\"T_4b03d_row39_col53\" class=\"data row39 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row39_col54\" class=\"data row39 col54\" >82.900349</td>\n",
              "      <td id=\"T_4b03d_row39_col55\" class=\"data row39 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row39_col56\" class=\"data row39 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row39_col57\" class=\"data row39 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row39_col58\" class=\"data row39 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row39_col59\" class=\"data row39 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row39_col60\" class=\"data row39 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row39_col61\" class=\"data row39 col61\" >75.106630</td>\n",
              "      <td id=\"T_4b03d_row39_col62\" class=\"data row39 col62\" >18.883288</td>\n",
              "      <td id=\"T_4b03d_row39_col63\" class=\"data row39 col63\" >36.835983</td>\n",
              "      <td id=\"T_4b03d_row39_col64\" class=\"data row39 col64\" >33.578907</td>\n",
              "      <td id=\"T_4b03d_row39_col65\" class=\"data row39 col65\" >53.082590</td>\n",
              "      <td id=\"T_4b03d_row39_col66\" class=\"data row39 col66\" >20.007755</td>\n",
              "      <td id=\"T_4b03d_row39_col67\" class=\"data row39 col67\" >77.122916</td>\n",
              "      <td id=\"T_4b03d_row39_col68\" class=\"data row39 col68\" >26.560682</td>\n",
              "      <td id=\"T_4b03d_row39_col69\" class=\"data row39 col69\" >61.535479</td>\n",
              "      <td id=\"T_4b03d_row39_col70\" class=\"data row39 col70\" >71.655680</td>\n",
              "      <td id=\"T_4b03d_row39_col71\" class=\"data row39 col71\" >91.857309</td>\n",
              "      <td id=\"T_4b03d_row39_col72\" class=\"data row39 col72\" >43.892982</td>\n",
              "      <td id=\"T_4b03d_row39_col73\" class=\"data row39 col73\" >72.082202</td>\n",
              "      <td id=\"T_4b03d_row39_col74\" class=\"data row39 col74\" >66.963940</td>\n",
              "      <td id=\"T_4b03d_row39_col75\" class=\"data row39 col75\" >43.001163</td>\n",
              "      <td id=\"T_4b03d_row39_col76\" class=\"data row39 col76\" >92.128732</td>\n",
              "      <td id=\"T_4b03d_row39_col77\" class=\"data row39 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row39_col78\" class=\"data row39 col78\" >68.863901</td>\n",
              "      <td id=\"T_4b03d_row39_col79\" class=\"data row39 col79\" >87.010469</td>\n",
              "      <td id=\"T_4b03d_row39_col80\" class=\"data row39 col80\" >57.347809</td>\n",
              "      <td id=\"T_4b03d_row39_col81\" class=\"data row39 col81\" >73.749515</td>\n",
              "      <td id=\"T_4b03d_row39_col82\" class=\"data row39 col82\" >88.290035</td>\n",
              "      <td id=\"T_4b03d_row39_col83\" class=\"data row39 col83\" >41.527724</td>\n",
              "      <td id=\"T_4b03d_row39_col84\" class=\"data row39 col84\" >89.220628</td>\n",
              "      <td id=\"T_4b03d_row39_col85\" class=\"data row39 col85\" >74.408686</td>\n",
              "      <td id=\"T_4b03d_row39_col86\" class=\"data row39 col86\" >93.214424</td>\n",
              "      <td id=\"T_4b03d_row39_col87\" class=\"data row39 col87\" >74.757658</td>\n",
              "      <td id=\"T_4b03d_row39_col88\" class=\"data row39 col88\" >98.371462</td>\n",
              "      <td id=\"T_4b03d_row39_col89\" class=\"data row39 col89\" >88.134936</td>\n",
              "      <td id=\"T_4b03d_row39_col90\" class=\"data row39 col90\" >5.583560</td>\n",
              "      <td id=\"T_4b03d_row39_col91\" class=\"data row39 col91\" >70.492439</td>\n",
              "      <td id=\"T_4b03d_row39_col92\" class=\"data row39 col92\" >22.489337</td>\n",
              "      <td id=\"T_4b03d_row39_col93\" class=\"data row39 col93\" >97.324544</td>\n",
              "      <td id=\"T_4b03d_row39_col94\" class=\"data row39 col94\" >97.169446</td>\n",
              "      <td id=\"T_4b03d_row39_col95\" class=\"data row39 col95\" >79.061652</td>\n",
              "      <td id=\"T_4b03d_row39_col96\" class=\"data row39 col96\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row39_col97\" class=\"data row39 col97\" >99.263280</td>\n",
              "      <td id=\"T_4b03d_row39_col98\" class=\"data row39 col98\" >81.892206</td>\n",
              "      <td id=\"T_4b03d_row39_col99\" class=\"data row39 col99\" >94.610314</td>\n",
              "      <td id=\"T_4b03d_row39_col100\" class=\"data row39 col100\" >6.436603</td>\n",
              "      <td id=\"T_4b03d_row39_col101\" class=\"data row39 col101\" >66.692516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "      <td id=\"T_4b03d_row40_col0\" class=\"data row40 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row40_col1\" class=\"data row40 col1\" >C.CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@](C)(C(=O)[C@H](O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C)[C@@H]2COC(=O)c1ccccc1.O.O</td>\n",
              "      <td id=\"T_4b03d_row40_col2\" class=\"data row40 col2\" >2.703746</td>\n",
              "      <td id=\"T_4b03d_row40_col3\" class=\"data row40 col3\" >C.CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@](C)(C(=O)[C@H](O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C)[C@@H]2COC(=O)c1ccccc1.O.O</td>\n",
              "      <td id=\"T_4b03d_row40_col4\" class=\"data row40 col4\" >837.960000</td>\n",
              "      <td id=\"T_4b03d_row40_col5\" class=\"data row40 col5\" >3.284300</td>\n",
              "      <td id=\"T_4b03d_row40_col6\" class=\"data row40 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row40_col7\" class=\"data row40 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row40_col8\" class=\"data row40 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row40_col9\" class=\"data row40 col9\" >0.097732</td>\n",
              "      <td id=\"T_4b03d_row40_col10\" class=\"data row40 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row40_col11\" class=\"data row40 col11\" >257.990000</td>\n",
              "      <td id=\"T_4b03d_row40_col12\" class=\"data row40 col12\" >0.225608</td>\n",
              "      <td id=\"T_4b03d_row40_col13\" class=\"data row40 col13\" >0.244811</td>\n",
              "      <td id=\"T_4b03d_row40_col14\" class=\"data row40 col14\" >0.676655</td>\n",
              "      <td id=\"T_4b03d_row40_col15\" class=\"data row40 col15\" >0.015118</td>\n",
              "      <td id=\"T_4b03d_row40_col16\" class=\"data row40 col16\" >0.284502</td>\n",
              "      <td id=\"T_4b03d_row40_col17\" class=\"data row40 col17\" >0.143336</td>\n",
              "      <td id=\"T_4b03d_row40_col18\" class=\"data row40 col18\" >0.466706</td>\n",
              "      <td id=\"T_4b03d_row40_col19\" class=\"data row40 col19\" >0.057480</td>\n",
              "      <td id=\"T_4b03d_row40_col20\" class=\"data row40 col20\" >0.177655</td>\n",
              "      <td id=\"T_4b03d_row40_col21\" class=\"data row40 col21\" >0.690809</td>\n",
              "      <td id=\"T_4b03d_row40_col22\" class=\"data row40 col22\" >0.884924</td>\n",
              "      <td id=\"T_4b03d_row40_col23\" class=\"data row40 col23\" >0.154338</td>\n",
              "      <td id=\"T_4b03d_row40_col24\" class=\"data row40 col24\" >0.246174</td>\n",
              "      <td id=\"T_4b03d_row40_col25\" class=\"data row40 col25\" >0.819489</td>\n",
              "      <td id=\"T_4b03d_row40_col26\" class=\"data row40 col26\" >0.987326</td>\n",
              "      <td id=\"T_4b03d_row40_col27\" class=\"data row40 col27\" >0.035947</td>\n",
              "      <td id=\"T_4b03d_row40_col28\" class=\"data row40 col28\" >0.044456</td>\n",
              "      <td id=\"T_4b03d_row40_col29\" class=\"data row40 col29\" >0.051297</td>\n",
              "      <td id=\"T_4b03d_row40_col30\" class=\"data row40 col30\" >0.124167</td>\n",
              "      <td id=\"T_4b03d_row40_col31\" class=\"data row40 col31\" >0.012220</td>\n",
              "      <td id=\"T_4b03d_row40_col32\" class=\"data row40 col32\" >0.078601</td>\n",
              "      <td id=\"T_4b03d_row40_col33\" class=\"data row40 col33\" >0.052009</td>\n",
              "      <td id=\"T_4b03d_row40_col34\" class=\"data row40 col34\" >0.552861</td>\n",
              "      <td id=\"T_4b03d_row40_col35\" class=\"data row40 col35\" >0.904416</td>\n",
              "      <td id=\"T_4b03d_row40_col36\" class=\"data row40 col36\" >0.249910</td>\n",
              "      <td id=\"T_4b03d_row40_col37\" class=\"data row40 col37\" >0.076793</td>\n",
              "      <td id=\"T_4b03d_row40_col38\" class=\"data row40 col38\" >0.045522</td>\n",
              "      <td id=\"T_4b03d_row40_col39\" class=\"data row40 col39\" >0.826039</td>\n",
              "      <td id=\"T_4b03d_row40_col40\" class=\"data row40 col40\" >0.116621</td>\n",
              "      <td id=\"T_4b03d_row40_col41\" class=\"data row40 col41\" >0.095350</td>\n",
              "      <td id=\"T_4b03d_row40_col42\" class=\"data row40 col42\" >0.802473</td>\n",
              "      <td id=\"T_4b03d_row40_col43\" class=\"data row40 col43\" >-5.535227</td>\n",
              "      <td id=\"T_4b03d_row40_col44\" class=\"data row40 col44\" >118.557681</td>\n",
              "      <td id=\"T_4b03d_row40_col45\" class=\"data row40 col45\" >96.537698</td>\n",
              "      <td id=\"T_4b03d_row40_col46\" class=\"data row40 col46\" >48.625157</td>\n",
              "      <td id=\"T_4b03d_row40_col47\" class=\"data row40 col47\" >-11.064259</td>\n",
              "      <td id=\"T_4b03d_row40_col48\" class=\"data row40 col48\" >4.342912</td>\n",
              "      <td id=\"T_4b03d_row40_col49\" class=\"data row40 col49\" >2.703746</td>\n",
              "      <td id=\"T_4b03d_row40_col50\" class=\"data row40 col50\" >101.491821</td>\n",
              "      <td id=\"T_4b03d_row40_col51\" class=\"data row40 col51\" >-4.826568</td>\n",
              "      <td id=\"T_4b03d_row40_col52\" class=\"data row40 col52\" >2.645787</td>\n",
              "      <td id=\"T_4b03d_row40_col53\" class=\"data row40 col53\" >94.920512</td>\n",
              "      <td id=\"T_4b03d_row40_col54\" class=\"data row40 col54\" >66.227220</td>\n",
              "      <td id=\"T_4b03d_row40_col55\" class=\"data row40 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row40_col56\" class=\"data row40 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row40_col57\" class=\"data row40 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row40_col58\" class=\"data row40 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row40_col59\" class=\"data row40 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row40_col60\" class=\"data row40 col60\" >94.687864</td>\n",
              "      <td id=\"T_4b03d_row40_col61\" class=\"data row40 col61\" >56.300892</td>\n",
              "      <td id=\"T_4b03d_row40_col62\" class=\"data row40 col62\" >15.316014</td>\n",
              "      <td id=\"T_4b03d_row40_col63\" class=\"data row40 col63\" >32.919736</td>\n",
              "      <td id=\"T_4b03d_row40_col64\" class=\"data row40 col64\" >38.115549</td>\n",
              "      <td id=\"T_4b03d_row40_col65\" class=\"data row40 col65\" >71.461807</td>\n",
              "      <td id=\"T_4b03d_row40_col66\" class=\"data row40 col66\" >48.933695</td>\n",
              "      <td id=\"T_4b03d_row40_col67\" class=\"data row40 col67\" >89.569601</td>\n",
              "      <td id=\"T_4b03d_row40_col68\" class=\"data row40 col68\" >36.719659</td>\n",
              "      <td id=\"T_4b03d_row40_col69\" class=\"data row40 col69\" >71.694455</td>\n",
              "      <td id=\"T_4b03d_row40_col70\" class=\"data row40 col70\" >71.655680</td>\n",
              "      <td id=\"T_4b03d_row40_col71\" class=\"data row40 col71\" >94.455215</td>\n",
              "      <td id=\"T_4b03d_row40_col72\" class=\"data row40 col72\" >49.941838</td>\n",
              "      <td id=\"T_4b03d_row40_col73\" class=\"data row40 col73\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row40_col74\" class=\"data row40 col74\" >73.245444</td>\n",
              "      <td id=\"T_4b03d_row40_col75\" class=\"data row40 col75\" >39.782862</td>\n",
              "      <td id=\"T_4b03d_row40_col76\" class=\"data row40 col76\" >79.255525</td>\n",
              "      <td id=\"T_4b03d_row40_col77\" class=\"data row40 col77\" >69.949593</td>\n",
              "      <td id=\"T_4b03d_row40_col78\" class=\"data row40 col78\" >61.574254</td>\n",
              "      <td id=\"T_4b03d_row40_col79\" class=\"data row40 col79\" >77.122916</td>\n",
              "      <td id=\"T_4b03d_row40_col80\" class=\"data row40 col80\" >40.403257</td>\n",
              "      <td id=\"T_4b03d_row40_col81\" class=\"data row40 col81\" >44.590927</td>\n",
              "      <td id=\"T_4b03d_row40_col82\" class=\"data row40 col82\" >84.412563</td>\n",
              "      <td id=\"T_4b03d_row40_col83\" class=\"data row40 col83\" >36.603335</td>\n",
              "      <td id=\"T_4b03d_row40_col84\" class=\"data row40 col84\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row40_col85\" class=\"data row40 col85\" >66.692516</td>\n",
              "      <td id=\"T_4b03d_row40_col86\" class=\"data row40 col86\" >85.537030</td>\n",
              "      <td id=\"T_4b03d_row40_col87\" class=\"data row40 col87\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row40_col88\" class=\"data row40 col88\" >96.122528</td>\n",
              "      <td id=\"T_4b03d_row40_col89\" class=\"data row40 col89\" >77.433114</td>\n",
              "      <td id=\"T_4b03d_row40_col90\" class=\"data row40 col90\" >4.924389</td>\n",
              "      <td id=\"T_4b03d_row40_col91\" class=\"data row40 col91\" >78.518806</td>\n",
              "      <td id=\"T_4b03d_row40_col92\" class=\"data row40 col92\" >22.295463</td>\n",
              "      <td id=\"T_4b03d_row40_col93\" class=\"data row40 col93\" >94.998061</td>\n",
              "      <td id=\"T_4b03d_row40_col94\" class=\"data row40 col94\" >94.920512</td>\n",
              "      <td id=\"T_4b03d_row40_col95\" class=\"data row40 col95\" >86.816596</td>\n",
              "      <td id=\"T_4b03d_row40_col96\" class=\"data row40 col96\" >35.672741</td>\n",
              "      <td id=\"T_4b03d_row40_col97\" class=\"data row40 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row40_col98\" class=\"data row40 col98\" >74.098488</td>\n",
              "      <td id=\"T_4b03d_row40_col99\" class=\"data row40 col99\" >92.904226</td>\n",
              "      <td id=\"T_4b03d_row40_col100\" class=\"data row40 col100\" >16.673129</td>\n",
              "      <td id=\"T_4b03d_row40_col101\" class=\"data row40 col101\" >58.549826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "      <td id=\"T_4b03d_row41_col0\" class=\"data row41 col0\" >Stereochemistry Alteration</td>\n",
              "      <td id=\"T_4b03d_row41_col1\" class=\"data row41 col1\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row41_col2\" class=\"data row41 col2\" >2.611683</td>\n",
              "      <td id=\"T_4b03d_row41_col3\" class=\"data row41 col3\" >CC(=O)O[C@H]1C(=O)[C@@]2(C)[C@H]([C@@H](OC(=O)c3ccccc3)[C@]3(O)C[C@H](OC(=O)[C@H](O)[C@@H](NC(=O)c4ccccc4)c4ccccc4)C(C)=C1C3(C)C)[C@]1(OC(C)=O)CO[C@@H]1C[C@@H]2O</td>\n",
              "      <td id=\"T_4b03d_row41_col4\" class=\"data row41 col4\" >853.918000</td>\n",
              "      <td id=\"T_4b03d_row41_col5\" class=\"data row41 col5\" >3.735700</td>\n",
              "      <td id=\"T_4b03d_row41_col6\" class=\"data row41 col6\" >14</td>\n",
              "      <td id=\"T_4b03d_row41_col7\" class=\"data row41 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row41_col8\" class=\"data row41 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row41_col9\" class=\"data row41 col9\" >0.129786</td>\n",
              "      <td id=\"T_4b03d_row41_col10\" class=\"data row41 col10\" >11</td>\n",
              "      <td id=\"T_4b03d_row41_col11\" class=\"data row41 col11\" >221.290000</td>\n",
              "      <td id=\"T_4b03d_row41_col12\" class=\"data row41 col12\" >0.367914</td>\n",
              "      <td id=\"T_4b03d_row41_col13\" class=\"data row41 col13\" >0.317943</td>\n",
              "      <td id=\"T_4b03d_row41_col14\" class=\"data row41 col14\" >0.705167</td>\n",
              "      <td id=\"T_4b03d_row41_col15\" class=\"data row41 col15\" >0.007666</td>\n",
              "      <td id=\"T_4b03d_row41_col16\" class=\"data row41 col16\" >0.057013</td>\n",
              "      <td id=\"T_4b03d_row41_col17\" class=\"data row41 col17\" >0.046140</td>\n",
              "      <td id=\"T_4b03d_row41_col18\" class=\"data row41 col18\" >0.111743</td>\n",
              "      <td id=\"T_4b03d_row41_col19\" class=\"data row41 col19\" >0.025539</td>\n",
              "      <td id=\"T_4b03d_row41_col20\" class=\"data row41 col20\" >0.058447</td>\n",
              "      <td id=\"T_4b03d_row41_col21\" class=\"data row41 col21\" >0.626651</td>\n",
              "      <td id=\"T_4b03d_row41_col22\" class=\"data row41 col22\" >0.586412</td>\n",
              "      <td id=\"T_4b03d_row41_col23\" class=\"data row41 col23\" >0.091948</td>\n",
              "      <td id=\"T_4b03d_row41_col24\" class=\"data row41 col24\" >0.339223</td>\n",
              "      <td id=\"T_4b03d_row41_col25\" class=\"data row41 col25\" >0.807525</td>\n",
              "      <td id=\"T_4b03d_row41_col26\" class=\"data row41 col26\" >0.977938</td>\n",
              "      <td id=\"T_4b03d_row41_col27\" class=\"data row41 col27\" >0.186525</td>\n",
              "      <td id=\"T_4b03d_row41_col28\" class=\"data row41 col28\" >0.393002</td>\n",
              "      <td id=\"T_4b03d_row41_col29\" class=\"data row41 col29\" >0.048310</td>\n",
              "      <td id=\"T_4b03d_row41_col30\" class=\"data row41 col30\" >0.232993</td>\n",
              "      <td id=\"T_4b03d_row41_col31\" class=\"data row41 col31\" >0.026417</td>\n",
              "      <td id=\"T_4b03d_row41_col32\" class=\"data row41 col32\" >0.182615</td>\n",
              "      <td id=\"T_4b03d_row41_col33\" class=\"data row41 col33\" >0.078467</td>\n",
              "      <td id=\"T_4b03d_row41_col34\" class=\"data row41 col34\" >0.496035</td>\n",
              "      <td id=\"T_4b03d_row41_col35\" class=\"data row41 col35\" >0.754155</td>\n",
              "      <td id=\"T_4b03d_row41_col36\" class=\"data row41 col36\" >0.291155</td>\n",
              "      <td id=\"T_4b03d_row41_col37\" class=\"data row41 col37\" >0.244668</td>\n",
              "      <td id=\"T_4b03d_row41_col38\" class=\"data row41 col38\" >0.036719</td>\n",
              "      <td id=\"T_4b03d_row41_col39\" class=\"data row41 col39\" >0.887558</td>\n",
              "      <td id=\"T_4b03d_row41_col40\" class=\"data row41 col40\" >0.271582</td>\n",
              "      <td id=\"T_4b03d_row41_col41\" class=\"data row41 col41\" >0.080110</td>\n",
              "      <td id=\"T_4b03d_row41_col42\" class=\"data row41 col42\" >0.600895</td>\n",
              "      <td id=\"T_4b03d_row41_col43\" class=\"data row41 col43\" >-5.633964</td>\n",
              "      <td id=\"T_4b03d_row41_col44\" class=\"data row41 col44\" >127.795565</td>\n",
              "      <td id=\"T_4b03d_row41_col45\" class=\"data row41 col45\" >98.179662</td>\n",
              "      <td id=\"T_4b03d_row41_col46\" class=\"data row41 col46\" >44.930155</td>\n",
              "      <td id=\"T_4b03d_row41_col47\" class=\"data row41 col47\" >-11.057616</td>\n",
              "      <td id=\"T_4b03d_row41_col48\" class=\"data row41 col48\" >4.342497</td>\n",
              "      <td id=\"T_4b03d_row41_col49\" class=\"data row41 col49\" >2.611683</td>\n",
              "      <td id=\"T_4b03d_row41_col50\" class=\"data row41 col50\" >99.785907</td>\n",
              "      <td id=\"T_4b03d_row41_col51\" class=\"data row41 col51\" >-5.998230</td>\n",
              "      <td id=\"T_4b03d_row41_col52\" class=\"data row41 col52\" >3.415752</td>\n",
              "      <td id=\"T_4b03d_row41_col53\" class=\"data row41 col53\" >95.153160</td>\n",
              "      <td id=\"T_4b03d_row41_col54\" class=\"data row41 col54\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row41_col55\" class=\"data row41 col55\" >94.590927</td>\n",
              "      <td id=\"T_4b03d_row41_col56\" class=\"data row41 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row41_col57\" class=\"data row41 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row41_col58\" class=\"data row41 col58\" >7.212098</td>\n",
              "      <td id=\"T_4b03d_row41_col59\" class=\"data row41 col59\" >96.471501</td>\n",
              "      <td id=\"T_4b03d_row41_col60\" class=\"data row41 col60\" >93.679721</td>\n",
              "      <td id=\"T_4b03d_row41_col61\" class=\"data row41 col61\" >73.284219</td>\n",
              "      <td id=\"T_4b03d_row41_col62\" class=\"data row41 col62\" >19.348585</td>\n",
              "      <td id=\"T_4b03d_row41_col63\" class=\"data row41 col63\" >37.184955</td>\n",
              "      <td id=\"T_4b03d_row41_col64\" class=\"data row41 col64\" >30.709577</td>\n",
              "      <td id=\"T_4b03d_row41_col65\" class=\"data row41 col65\" >40.480807</td>\n",
              "      <td id=\"T_4b03d_row41_col66\" class=\"data row41 col66\" >16.944552</td>\n",
              "      <td id=\"T_4b03d_row41_col67\" class=\"data row41 col67\" >67.157813</td>\n",
              "      <td id=\"T_4b03d_row41_col68\" class=\"data row41 col68\" >18.689415</td>\n",
              "      <td id=\"T_4b03d_row41_col69\" class=\"data row41 col69\" >53.043815</td>\n",
              "      <td id=\"T_4b03d_row41_col70\" class=\"data row41 col70\" >63.784413</td>\n",
              "      <td id=\"T_4b03d_row41_col71\" class=\"data row41 col71\" >83.481970</td>\n",
              "      <td id=\"T_4b03d_row41_col72\" class=\"data row41 col72\" >35.013571</td>\n",
              "      <td id=\"T_4b03d_row41_col73\" class=\"data row41 col73\" >78.945328</td>\n",
              "      <td id=\"T_4b03d_row41_col74\" class=\"data row41 col74\" >72.276076</td>\n",
              "      <td id=\"T_4b03d_row41_col75\" class=\"data row41 col75\" >35.517642</td>\n",
              "      <td id=\"T_4b03d_row41_col76\" class=\"data row41 col76\" >93.020551</td>\n",
              "      <td id=\"T_4b03d_row41_col77\" class=\"data row41 col77\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row41_col78\" class=\"data row41 col78\" >60.411012</td>\n",
              "      <td id=\"T_4b03d_row41_col79\" class=\"data row41 col79\" >87.514541</td>\n",
              "      <td id=\"T_4b03d_row41_col80\" class=\"data row41 col80\" >61.341605</td>\n",
              "      <td id=\"T_4b03d_row41_col81\" class=\"data row41 col81\" >77.316789</td>\n",
              "      <td id=\"T_4b03d_row41_col82\" class=\"data row41 col82\" >88.949205</td>\n",
              "      <td id=\"T_4b03d_row41_col83\" class=\"data row41 col83\" >34.199302</td>\n",
              "      <td id=\"T_4b03d_row41_col84\" class=\"data row41 col84\" >86.079876</td>\n",
              "      <td id=\"T_4b03d_row41_col85\" class=\"data row41 col85\" >70.763862</td>\n",
              "      <td id=\"T_4b03d_row41_col86\" class=\"data row41 col86\" >95.347034</td>\n",
              "      <td id=\"T_4b03d_row41_col87\" class=\"data row41 col87\" >69.057774</td>\n",
              "      <td id=\"T_4b03d_row41_col88\" class=\"data row41 col88\" >97.440869</td>\n",
              "      <td id=\"T_4b03d_row41_col89\" class=\"data row41 col89\" >89.143079</td>\n",
              "      <td id=\"T_4b03d_row41_col90\" class=\"data row41 col90\" >3.063203</td>\n",
              "      <td id=\"T_4b03d_row41_col91\" class=\"data row41 col91\" >68.359829</td>\n",
              "      <td id=\"T_4b03d_row41_col92\" class=\"data row41 col92\" >19.891431</td>\n",
              "      <td id=\"T_4b03d_row41_col93\" class=\"data row41 col93\" >96.587825</td>\n",
              "      <td id=\"T_4b03d_row41_col94\" class=\"data row41 col94\" >95.424583</td>\n",
              "      <td id=\"T_4b03d_row41_col95\" class=\"data row41 col95\" >85.032959</td>\n",
              "      <td id=\"T_4b03d_row41_col96\" class=\"data row41 col96\" >35.750291</td>\n",
              "      <td id=\"T_4b03d_row41_col97\" class=\"data row41 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row41_col98\" class=\"data row41 col98\" >72.392400</td>\n",
              "      <td id=\"T_4b03d_row41_col99\" class=\"data row41 col99\" >90.577743</td>\n",
              "      <td id=\"T_4b03d_row41_col100\" class=\"data row41 col100\" >5.506010</td>\n",
              "      <td id=\"T_4b03d_row41_col101\" class=\"data row41 col101\" >64.366033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "      <td id=\"T_4b03d_row42_col0\" class=\"data row42 col0\" >Atom Insertion</td>\n",
              "      <td id=\"T_4b03d_row42_col1\" class=\"data row42 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row42_col2\" class=\"data row42 col2\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row42_col3\" class=\"data row42 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row42_col4\" class=\"data row42 col4\" >845.983000</td>\n",
              "      <td id=\"T_4b03d_row42_col5\" class=\"data row42 col5\" >4.711920</td>\n",
              "      <td id=\"T_4b03d_row42_col6\" class=\"data row42 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row42_col7\" class=\"data row42 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row42_col8\" class=\"data row42 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row42_col9\" class=\"data row42 col9\" >0.095721</td>\n",
              "      <td id=\"T_4b03d_row42_col10\" class=\"data row42 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row42_col11\" class=\"data row42 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row42_col12\" class=\"data row42 col12\" >0.418777</td>\n",
              "      <td id=\"T_4b03d_row42_col13\" class=\"data row42 col13\" >0.313347</td>\n",
              "      <td id=\"T_4b03d_row42_col14\" class=\"data row42 col14\" >0.718345</td>\n",
              "      <td id=\"T_4b03d_row42_col15\" class=\"data row42 col15\" >0.008119</td>\n",
              "      <td id=\"T_4b03d_row42_col16\" class=\"data row42 col16\" >0.130581</td>\n",
              "      <td id=\"T_4b03d_row42_col17\" class=\"data row42 col17\" >0.053637</td>\n",
              "      <td id=\"T_4b03d_row42_col18\" class=\"data row42 col18\" >0.209448</td>\n",
              "      <td id=\"T_4b03d_row42_col19\" class=\"data row42 col19\" >0.037448</td>\n",
              "      <td id=\"T_4b03d_row42_col20\" class=\"data row42 col20\" >0.098746</td>\n",
              "      <td id=\"T_4b03d_row42_col21\" class=\"data row42 col21\" >0.741527</td>\n",
              "      <td id=\"T_4b03d_row42_col22\" class=\"data row42 col22\" >0.859824</td>\n",
              "      <td id=\"T_4b03d_row42_col23\" class=\"data row42 col23\" >0.156202</td>\n",
              "      <td id=\"T_4b03d_row42_col24\" class=\"data row42 col24\" >0.234659</td>\n",
              "      <td id=\"T_4b03d_row42_col25\" class=\"data row42 col25\" >0.723130</td>\n",
              "      <td id=\"T_4b03d_row42_col26\" class=\"data row42 col26\" >0.993656</td>\n",
              "      <td id=\"T_4b03d_row42_col27\" class=\"data row42 col27\" >0.116319</td>\n",
              "      <td id=\"T_4b03d_row42_col28\" class=\"data row42 col28\" >0.293043</td>\n",
              "      <td id=\"T_4b03d_row42_col29\" class=\"data row42 col29\" >0.073669</td>\n",
              "      <td id=\"T_4b03d_row42_col30\" class=\"data row42 col30\" >0.223948</td>\n",
              "      <td id=\"T_4b03d_row42_col31\" class=\"data row42 col31\" >0.024769</td>\n",
              "      <td id=\"T_4b03d_row42_col32\" class=\"data row42 col32\" >0.182238</td>\n",
              "      <td id=\"T_4b03d_row42_col33\" class=\"data row42 col33\" >0.068505</td>\n",
              "      <td id=\"T_4b03d_row42_col34\" class=\"data row42 col34\" >0.647033</td>\n",
              "      <td id=\"T_4b03d_row42_col35\" class=\"data row42 col35\" >0.920296</td>\n",
              "      <td id=\"T_4b03d_row42_col36\" class=\"data row42 col36\" >0.360788</td>\n",
              "      <td id=\"T_4b03d_row42_col37\" class=\"data row42 col37\" >0.176250</td>\n",
              "      <td id=\"T_4b03d_row42_col38\" class=\"data row42 col38\" >0.056515</td>\n",
              "      <td id=\"T_4b03d_row42_col39\" class=\"data row42 col39\" >0.933698</td>\n",
              "      <td id=\"T_4b03d_row42_col40\" class=\"data row42 col40\" >0.230766</td>\n",
              "      <td id=\"T_4b03d_row42_col41\" class=\"data row42 col41\" >0.094041</td>\n",
              "      <td id=\"T_4b03d_row42_col42\" class=\"data row42 col42\" >0.700260</td>\n",
              "      <td id=\"T_4b03d_row42_col43\" class=\"data row42 col43\" >-5.484095</td>\n",
              "      <td id=\"T_4b03d_row42_col44\" class=\"data row42 col44\" >130.905576</td>\n",
              "      <td id=\"T_4b03d_row42_col45\" class=\"data row42 col45\" >122.118249</td>\n",
              "      <td id=\"T_4b03d_row42_col46\" class=\"data row42 col46\" >43.168345</td>\n",
              "      <td id=\"T_4b03d_row42_col47\" class=\"data row42 col47\" >-9.874909</td>\n",
              "      <td id=\"T_4b03d_row42_col48\" class=\"data row42 col48\" >4.322272</td>\n",
              "      <td id=\"T_4b03d_row42_col49\" class=\"data row42 col49\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row42_col50\" class=\"data row42 col50\" >102.863042</td>\n",
              "      <td id=\"T_4b03d_row42_col51\" class=\"data row42 col51\" >-5.653315</td>\n",
              "      <td id=\"T_4b03d_row42_col52\" class=\"data row42 col52\" >3.011813</td>\n",
              "      <td id=\"T_4b03d_row42_col53\" class=\"data row42 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row42_col54\" class=\"data row42 col54\" >86.234975</td>\n",
              "      <td id=\"T_4b03d_row42_col55\" class=\"data row42 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row42_col56\" class=\"data row42 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row42_col57\" class=\"data row42 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row42_col58\" class=\"data row42 col58\" >5.312136</td>\n",
              "      <td id=\"T_4b03d_row42_col59\" class=\"data row42 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row42_col60\" class=\"data row42 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row42_col61\" class=\"data row42 col61\" >77.394339</td>\n",
              "      <td id=\"T_4b03d_row42_col62\" class=\"data row42 col62\" >19.077162</td>\n",
              "      <td id=\"T_4b03d_row42_col63\" class=\"data row42 col63\" >38.929818</td>\n",
              "      <td id=\"T_4b03d_row42_col64\" class=\"data row42 col64\" >31.252423</td>\n",
              "      <td id=\"T_4b03d_row42_col65\" class=\"data row42 col65\" >54.245832</td>\n",
              "      <td id=\"T_4b03d_row42_col66\" class=\"data row42 col66\" >19.503684</td>\n",
              "      <td id=\"T_4b03d_row42_col67\" class=\"data row42 col67\" >76.618845</td>\n",
              "      <td id=\"T_4b03d_row42_col68\" class=\"data row42 col68\" >26.870880</td>\n",
              "      <td id=\"T_4b03d_row42_col69\" class=\"data row42 col69\" >61.884451</td>\n",
              "      <td id=\"T_4b03d_row42_col70\" class=\"data row42 col70\" >79.100427</td>\n",
              "      <td id=\"T_4b03d_row42_col71\" class=\"data row42 col71\" >93.214424</td>\n",
              "      <td id=\"T_4b03d_row42_col72\" class=\"data row42 col72\" >50.213261</td>\n",
              "      <td id=\"T_4b03d_row42_col73\" class=\"data row42 col73\" >70.027142</td>\n",
              "      <td id=\"T_4b03d_row42_col74\" class=\"data row42 col74\" >65.917022</td>\n",
              "      <td id=\"T_4b03d_row42_col75\" class=\"data row42 col75\" >46.064366</td>\n",
              "      <td id=\"T_4b03d_row42_col76\" class=\"data row42 col76\" >91.508337</td>\n",
              "      <td id=\"T_4b03d_row42_col77\" class=\"data row42 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row42_col78\" class=\"data row42 col78\" >67.119038</td>\n",
              "      <td id=\"T_4b03d_row42_col79\" class=\"data row42 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row42_col80\" class=\"data row42 col80\" >59.596743</td>\n",
              "      <td id=\"T_4b03d_row42_col81\" class=\"data row42 col81\" >77.239240</td>\n",
              "      <td id=\"T_4b03d_row42_col82\" class=\"data row42 col82\" >87.708414</td>\n",
              "      <td id=\"T_4b03d_row42_col83\" class=\"data row42 col83\" >42.303218</td>\n",
              "      <td id=\"T_4b03d_row42_col84\" class=\"data row42 col84\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row42_col85\" class=\"data row42 col85\" >75.378054</td>\n",
              "      <td id=\"T_4b03d_row42_col86\" class=\"data row42 col86\" >93.020551</td>\n",
              "      <td id=\"T_4b03d_row42_col87\" class=\"data row42 col87\" >76.657619</td>\n",
              "      <td id=\"T_4b03d_row42_col88\" class=\"data row42 col88\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row42_col89\" class=\"data row42 col89\" >87.398216</td>\n",
              "      <td id=\"T_4b03d_row42_col90\" class=\"data row42 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row42_col91\" class=\"data row42 col91\" >72.314851</td>\n",
              "      <td id=\"T_4b03d_row42_col92\" class=\"data row42 col92\" >23.807677</td>\n",
              "      <td id=\"T_4b03d_row42_col93\" class=\"data row42 col93\" >97.130671</td>\n",
              "      <td id=\"T_4b03d_row42_col94\" class=\"data row42 col94\" >98.255138</td>\n",
              "      <td id=\"T_4b03d_row42_col95\" class=\"data row42 col95\" >84.102365</td>\n",
              "      <td id=\"T_4b03d_row42_col96\" class=\"data row42 col96\" >47.537805</td>\n",
              "      <td id=\"T_4b03d_row42_col97\" class=\"data row42 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row42_col98\" class=\"data row42 col98\" >84.412563</td>\n",
              "      <td id=\"T_4b03d_row42_col99\" class=\"data row42 col99\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row42_col100\" class=\"data row42 col100\" >7.599845</td>\n",
              "      <td id=\"T_4b03d_row42_col101\" class=\"data row42 col101\" >61.264056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "      <td id=\"T_4b03d_row43_col0\" class=\"data row43 col0\" >Atom Insertion</td>\n",
              "      <td id=\"T_4b03d_row43_col1\" class=\"data row43 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row43_col2\" class=\"data row43 col2\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row43_col3\" class=\"data row43 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row43_col4\" class=\"data row43 col4\" >845.983000</td>\n",
              "      <td id=\"T_4b03d_row43_col5\" class=\"data row43 col5\" >4.711920</td>\n",
              "      <td id=\"T_4b03d_row43_col6\" class=\"data row43 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row43_col7\" class=\"data row43 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row43_col8\" class=\"data row43 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row43_col9\" class=\"data row43 col9\" >0.095721</td>\n",
              "      <td id=\"T_4b03d_row43_col10\" class=\"data row43 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row43_col11\" class=\"data row43 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row43_col12\" class=\"data row43 col12\" >0.418777</td>\n",
              "      <td id=\"T_4b03d_row43_col13\" class=\"data row43 col13\" >0.313347</td>\n",
              "      <td id=\"T_4b03d_row43_col14\" class=\"data row43 col14\" >0.718345</td>\n",
              "      <td id=\"T_4b03d_row43_col15\" class=\"data row43 col15\" >0.008119</td>\n",
              "      <td id=\"T_4b03d_row43_col16\" class=\"data row43 col16\" >0.130581</td>\n",
              "      <td id=\"T_4b03d_row43_col17\" class=\"data row43 col17\" >0.053637</td>\n",
              "      <td id=\"T_4b03d_row43_col18\" class=\"data row43 col18\" >0.209448</td>\n",
              "      <td id=\"T_4b03d_row43_col19\" class=\"data row43 col19\" >0.037448</td>\n",
              "      <td id=\"T_4b03d_row43_col20\" class=\"data row43 col20\" >0.098746</td>\n",
              "      <td id=\"T_4b03d_row43_col21\" class=\"data row43 col21\" >0.741527</td>\n",
              "      <td id=\"T_4b03d_row43_col22\" class=\"data row43 col22\" >0.859824</td>\n",
              "      <td id=\"T_4b03d_row43_col23\" class=\"data row43 col23\" >0.156202</td>\n",
              "      <td id=\"T_4b03d_row43_col24\" class=\"data row43 col24\" >0.234659</td>\n",
              "      <td id=\"T_4b03d_row43_col25\" class=\"data row43 col25\" >0.723130</td>\n",
              "      <td id=\"T_4b03d_row43_col26\" class=\"data row43 col26\" >0.993656</td>\n",
              "      <td id=\"T_4b03d_row43_col27\" class=\"data row43 col27\" >0.116319</td>\n",
              "      <td id=\"T_4b03d_row43_col28\" class=\"data row43 col28\" >0.293043</td>\n",
              "      <td id=\"T_4b03d_row43_col29\" class=\"data row43 col29\" >0.073669</td>\n",
              "      <td id=\"T_4b03d_row43_col30\" class=\"data row43 col30\" >0.223948</td>\n",
              "      <td id=\"T_4b03d_row43_col31\" class=\"data row43 col31\" >0.024769</td>\n",
              "      <td id=\"T_4b03d_row43_col32\" class=\"data row43 col32\" >0.182238</td>\n",
              "      <td id=\"T_4b03d_row43_col33\" class=\"data row43 col33\" >0.068505</td>\n",
              "      <td id=\"T_4b03d_row43_col34\" class=\"data row43 col34\" >0.647033</td>\n",
              "      <td id=\"T_4b03d_row43_col35\" class=\"data row43 col35\" >0.920296</td>\n",
              "      <td id=\"T_4b03d_row43_col36\" class=\"data row43 col36\" >0.360788</td>\n",
              "      <td id=\"T_4b03d_row43_col37\" class=\"data row43 col37\" >0.176250</td>\n",
              "      <td id=\"T_4b03d_row43_col38\" class=\"data row43 col38\" >0.056515</td>\n",
              "      <td id=\"T_4b03d_row43_col39\" class=\"data row43 col39\" >0.933698</td>\n",
              "      <td id=\"T_4b03d_row43_col40\" class=\"data row43 col40\" >0.230766</td>\n",
              "      <td id=\"T_4b03d_row43_col41\" class=\"data row43 col41\" >0.094041</td>\n",
              "      <td id=\"T_4b03d_row43_col42\" class=\"data row43 col42\" >0.700260</td>\n",
              "      <td id=\"T_4b03d_row43_col43\" class=\"data row43 col43\" >-5.484095</td>\n",
              "      <td id=\"T_4b03d_row43_col44\" class=\"data row43 col44\" >130.905576</td>\n",
              "      <td id=\"T_4b03d_row43_col45\" class=\"data row43 col45\" >122.118249</td>\n",
              "      <td id=\"T_4b03d_row43_col46\" class=\"data row43 col46\" >43.168345</td>\n",
              "      <td id=\"T_4b03d_row43_col47\" class=\"data row43 col47\" >-9.874909</td>\n",
              "      <td id=\"T_4b03d_row43_col48\" class=\"data row43 col48\" >4.322272</td>\n",
              "      <td id=\"T_4b03d_row43_col49\" class=\"data row43 col49\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row43_col50\" class=\"data row43 col50\" >102.863042</td>\n",
              "      <td id=\"T_4b03d_row43_col51\" class=\"data row43 col51\" >-5.653315</td>\n",
              "      <td id=\"T_4b03d_row43_col52\" class=\"data row43 col52\" >3.011813</td>\n",
              "      <td id=\"T_4b03d_row43_col53\" class=\"data row43 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row43_col54\" class=\"data row43 col54\" >86.234975</td>\n",
              "      <td id=\"T_4b03d_row43_col55\" class=\"data row43 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row43_col56\" class=\"data row43 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row43_col57\" class=\"data row43 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row43_col58\" class=\"data row43 col58\" >5.312136</td>\n",
              "      <td id=\"T_4b03d_row43_col59\" class=\"data row43 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row43_col60\" class=\"data row43 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row43_col61\" class=\"data row43 col61\" >77.394339</td>\n",
              "      <td id=\"T_4b03d_row43_col62\" class=\"data row43 col62\" >19.077162</td>\n",
              "      <td id=\"T_4b03d_row43_col63\" class=\"data row43 col63\" >38.929818</td>\n",
              "      <td id=\"T_4b03d_row43_col64\" class=\"data row43 col64\" >31.252423</td>\n",
              "      <td id=\"T_4b03d_row43_col65\" class=\"data row43 col65\" >54.245832</td>\n",
              "      <td id=\"T_4b03d_row43_col66\" class=\"data row43 col66\" >19.503684</td>\n",
              "      <td id=\"T_4b03d_row43_col67\" class=\"data row43 col67\" >76.618845</td>\n",
              "      <td id=\"T_4b03d_row43_col68\" class=\"data row43 col68\" >26.870880</td>\n",
              "      <td id=\"T_4b03d_row43_col69\" class=\"data row43 col69\" >61.884451</td>\n",
              "      <td id=\"T_4b03d_row43_col70\" class=\"data row43 col70\" >79.100427</td>\n",
              "      <td id=\"T_4b03d_row43_col71\" class=\"data row43 col71\" >93.214424</td>\n",
              "      <td id=\"T_4b03d_row43_col72\" class=\"data row43 col72\" >50.213261</td>\n",
              "      <td id=\"T_4b03d_row43_col73\" class=\"data row43 col73\" >70.027142</td>\n",
              "      <td id=\"T_4b03d_row43_col74\" class=\"data row43 col74\" >65.917022</td>\n",
              "      <td id=\"T_4b03d_row43_col75\" class=\"data row43 col75\" >46.064366</td>\n",
              "      <td id=\"T_4b03d_row43_col76\" class=\"data row43 col76\" >91.508337</td>\n",
              "      <td id=\"T_4b03d_row43_col77\" class=\"data row43 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row43_col78\" class=\"data row43 col78\" >67.119038</td>\n",
              "      <td id=\"T_4b03d_row43_col79\" class=\"data row43 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row43_col80\" class=\"data row43 col80\" >59.596743</td>\n",
              "      <td id=\"T_4b03d_row43_col81\" class=\"data row43 col81\" >77.239240</td>\n",
              "      <td id=\"T_4b03d_row43_col82\" class=\"data row43 col82\" >87.708414</td>\n",
              "      <td id=\"T_4b03d_row43_col83\" class=\"data row43 col83\" >42.303218</td>\n",
              "      <td id=\"T_4b03d_row43_col84\" class=\"data row43 col84\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row43_col85\" class=\"data row43 col85\" >75.378054</td>\n",
              "      <td id=\"T_4b03d_row43_col86\" class=\"data row43 col86\" >93.020551</td>\n",
              "      <td id=\"T_4b03d_row43_col87\" class=\"data row43 col87\" >76.657619</td>\n",
              "      <td id=\"T_4b03d_row43_col88\" class=\"data row43 col88\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row43_col89\" class=\"data row43 col89\" >87.398216</td>\n",
              "      <td id=\"T_4b03d_row43_col90\" class=\"data row43 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row43_col91\" class=\"data row43 col91\" >72.314851</td>\n",
              "      <td id=\"T_4b03d_row43_col92\" class=\"data row43 col92\" >23.807677</td>\n",
              "      <td id=\"T_4b03d_row43_col93\" class=\"data row43 col93\" >97.130671</td>\n",
              "      <td id=\"T_4b03d_row43_col94\" class=\"data row43 col94\" >98.255138</td>\n",
              "      <td id=\"T_4b03d_row43_col95\" class=\"data row43 col95\" >84.102365</td>\n",
              "      <td id=\"T_4b03d_row43_col96\" class=\"data row43 col96\" >47.537805</td>\n",
              "      <td id=\"T_4b03d_row43_col97\" class=\"data row43 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row43_col98\" class=\"data row43 col98\" >84.412563</td>\n",
              "      <td id=\"T_4b03d_row43_col99\" class=\"data row43 col99\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row43_col100\" class=\"data row43 col100\" >7.599845</td>\n",
              "      <td id=\"T_4b03d_row43_col101\" class=\"data row43 col101\" >61.264056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "      <td id=\"T_4b03d_row44_col0\" class=\"data row44 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row44_col1\" class=\"data row44 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row44_col2\" class=\"data row44 col2\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row44_col3\" class=\"data row44 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row44_col4\" class=\"data row44 col4\" >845.983000</td>\n",
              "      <td id=\"T_4b03d_row44_col5\" class=\"data row44 col5\" >4.711920</td>\n",
              "      <td id=\"T_4b03d_row44_col6\" class=\"data row44 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row44_col7\" class=\"data row44 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row44_col8\" class=\"data row44 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row44_col9\" class=\"data row44 col9\" >0.095721</td>\n",
              "      <td id=\"T_4b03d_row44_col10\" class=\"data row44 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row44_col11\" class=\"data row44 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row44_col12\" class=\"data row44 col12\" >0.418777</td>\n",
              "      <td id=\"T_4b03d_row44_col13\" class=\"data row44 col13\" >0.313347</td>\n",
              "      <td id=\"T_4b03d_row44_col14\" class=\"data row44 col14\" >0.718345</td>\n",
              "      <td id=\"T_4b03d_row44_col15\" class=\"data row44 col15\" >0.008119</td>\n",
              "      <td id=\"T_4b03d_row44_col16\" class=\"data row44 col16\" >0.130581</td>\n",
              "      <td id=\"T_4b03d_row44_col17\" class=\"data row44 col17\" >0.053637</td>\n",
              "      <td id=\"T_4b03d_row44_col18\" class=\"data row44 col18\" >0.209448</td>\n",
              "      <td id=\"T_4b03d_row44_col19\" class=\"data row44 col19\" >0.037448</td>\n",
              "      <td id=\"T_4b03d_row44_col20\" class=\"data row44 col20\" >0.098746</td>\n",
              "      <td id=\"T_4b03d_row44_col21\" class=\"data row44 col21\" >0.741527</td>\n",
              "      <td id=\"T_4b03d_row44_col22\" class=\"data row44 col22\" >0.859824</td>\n",
              "      <td id=\"T_4b03d_row44_col23\" class=\"data row44 col23\" >0.156202</td>\n",
              "      <td id=\"T_4b03d_row44_col24\" class=\"data row44 col24\" >0.234659</td>\n",
              "      <td id=\"T_4b03d_row44_col25\" class=\"data row44 col25\" >0.723130</td>\n",
              "      <td id=\"T_4b03d_row44_col26\" class=\"data row44 col26\" >0.993656</td>\n",
              "      <td id=\"T_4b03d_row44_col27\" class=\"data row44 col27\" >0.116319</td>\n",
              "      <td id=\"T_4b03d_row44_col28\" class=\"data row44 col28\" >0.293043</td>\n",
              "      <td id=\"T_4b03d_row44_col29\" class=\"data row44 col29\" >0.073669</td>\n",
              "      <td id=\"T_4b03d_row44_col30\" class=\"data row44 col30\" >0.223948</td>\n",
              "      <td id=\"T_4b03d_row44_col31\" class=\"data row44 col31\" >0.024769</td>\n",
              "      <td id=\"T_4b03d_row44_col32\" class=\"data row44 col32\" >0.182238</td>\n",
              "      <td id=\"T_4b03d_row44_col33\" class=\"data row44 col33\" >0.068505</td>\n",
              "      <td id=\"T_4b03d_row44_col34\" class=\"data row44 col34\" >0.647033</td>\n",
              "      <td id=\"T_4b03d_row44_col35\" class=\"data row44 col35\" >0.920296</td>\n",
              "      <td id=\"T_4b03d_row44_col36\" class=\"data row44 col36\" >0.360788</td>\n",
              "      <td id=\"T_4b03d_row44_col37\" class=\"data row44 col37\" >0.176250</td>\n",
              "      <td id=\"T_4b03d_row44_col38\" class=\"data row44 col38\" >0.056515</td>\n",
              "      <td id=\"T_4b03d_row44_col39\" class=\"data row44 col39\" >0.933698</td>\n",
              "      <td id=\"T_4b03d_row44_col40\" class=\"data row44 col40\" >0.230766</td>\n",
              "      <td id=\"T_4b03d_row44_col41\" class=\"data row44 col41\" >0.094041</td>\n",
              "      <td id=\"T_4b03d_row44_col42\" class=\"data row44 col42\" >0.700260</td>\n",
              "      <td id=\"T_4b03d_row44_col43\" class=\"data row44 col43\" >-5.484095</td>\n",
              "      <td id=\"T_4b03d_row44_col44\" class=\"data row44 col44\" >130.905576</td>\n",
              "      <td id=\"T_4b03d_row44_col45\" class=\"data row44 col45\" >122.118249</td>\n",
              "      <td id=\"T_4b03d_row44_col46\" class=\"data row44 col46\" >43.168345</td>\n",
              "      <td id=\"T_4b03d_row44_col47\" class=\"data row44 col47\" >-9.874909</td>\n",
              "      <td id=\"T_4b03d_row44_col48\" class=\"data row44 col48\" >4.322272</td>\n",
              "      <td id=\"T_4b03d_row44_col49\" class=\"data row44 col49\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row44_col50\" class=\"data row44 col50\" >102.863042</td>\n",
              "      <td id=\"T_4b03d_row44_col51\" class=\"data row44 col51\" >-5.653315</td>\n",
              "      <td id=\"T_4b03d_row44_col52\" class=\"data row44 col52\" >3.011813</td>\n",
              "      <td id=\"T_4b03d_row44_col53\" class=\"data row44 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row44_col54\" class=\"data row44 col54\" >86.234975</td>\n",
              "      <td id=\"T_4b03d_row44_col55\" class=\"data row44 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row44_col56\" class=\"data row44 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row44_col57\" class=\"data row44 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row44_col58\" class=\"data row44 col58\" >5.312136</td>\n",
              "      <td id=\"T_4b03d_row44_col59\" class=\"data row44 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row44_col60\" class=\"data row44 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row44_col61\" class=\"data row44 col61\" >77.394339</td>\n",
              "      <td id=\"T_4b03d_row44_col62\" class=\"data row44 col62\" >19.077162</td>\n",
              "      <td id=\"T_4b03d_row44_col63\" class=\"data row44 col63\" >38.929818</td>\n",
              "      <td id=\"T_4b03d_row44_col64\" class=\"data row44 col64\" >31.252423</td>\n",
              "      <td id=\"T_4b03d_row44_col65\" class=\"data row44 col65\" >54.245832</td>\n",
              "      <td id=\"T_4b03d_row44_col66\" class=\"data row44 col66\" >19.503684</td>\n",
              "      <td id=\"T_4b03d_row44_col67\" class=\"data row44 col67\" >76.618845</td>\n",
              "      <td id=\"T_4b03d_row44_col68\" class=\"data row44 col68\" >26.870880</td>\n",
              "      <td id=\"T_4b03d_row44_col69\" class=\"data row44 col69\" >61.884451</td>\n",
              "      <td id=\"T_4b03d_row44_col70\" class=\"data row44 col70\" >79.100427</td>\n",
              "      <td id=\"T_4b03d_row44_col71\" class=\"data row44 col71\" >93.214424</td>\n",
              "      <td id=\"T_4b03d_row44_col72\" class=\"data row44 col72\" >50.213261</td>\n",
              "      <td id=\"T_4b03d_row44_col73\" class=\"data row44 col73\" >70.027142</td>\n",
              "      <td id=\"T_4b03d_row44_col74\" class=\"data row44 col74\" >65.917022</td>\n",
              "      <td id=\"T_4b03d_row44_col75\" class=\"data row44 col75\" >46.064366</td>\n",
              "      <td id=\"T_4b03d_row44_col76\" class=\"data row44 col76\" >91.508337</td>\n",
              "      <td id=\"T_4b03d_row44_col77\" class=\"data row44 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row44_col78\" class=\"data row44 col78\" >67.119038</td>\n",
              "      <td id=\"T_4b03d_row44_col79\" class=\"data row44 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row44_col80\" class=\"data row44 col80\" >59.596743</td>\n",
              "      <td id=\"T_4b03d_row44_col81\" class=\"data row44 col81\" >77.239240</td>\n",
              "      <td id=\"T_4b03d_row44_col82\" class=\"data row44 col82\" >87.708414</td>\n",
              "      <td id=\"T_4b03d_row44_col83\" class=\"data row44 col83\" >42.303218</td>\n",
              "      <td id=\"T_4b03d_row44_col84\" class=\"data row44 col84\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row44_col85\" class=\"data row44 col85\" >75.378054</td>\n",
              "      <td id=\"T_4b03d_row44_col86\" class=\"data row44 col86\" >93.020551</td>\n",
              "      <td id=\"T_4b03d_row44_col87\" class=\"data row44 col87\" >76.657619</td>\n",
              "      <td id=\"T_4b03d_row44_col88\" class=\"data row44 col88\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row44_col89\" class=\"data row44 col89\" >87.398216</td>\n",
              "      <td id=\"T_4b03d_row44_col90\" class=\"data row44 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row44_col91\" class=\"data row44 col91\" >72.314851</td>\n",
              "      <td id=\"T_4b03d_row44_col92\" class=\"data row44 col92\" >23.807677</td>\n",
              "      <td id=\"T_4b03d_row44_col93\" class=\"data row44 col93\" >97.130671</td>\n",
              "      <td id=\"T_4b03d_row44_col94\" class=\"data row44 col94\" >98.255138</td>\n",
              "      <td id=\"T_4b03d_row44_col95\" class=\"data row44 col95\" >84.102365</td>\n",
              "      <td id=\"T_4b03d_row44_col96\" class=\"data row44 col96\" >47.537805</td>\n",
              "      <td id=\"T_4b03d_row44_col97\" class=\"data row44 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row44_col98\" class=\"data row44 col98\" >84.412563</td>\n",
              "      <td id=\"T_4b03d_row44_col99\" class=\"data row44 col99\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row44_col100\" class=\"data row44 col100\" >7.599845</td>\n",
              "      <td id=\"T_4b03d_row44_col101\" class=\"data row44 col101\" >61.264056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "      <td id=\"T_4b03d_row45_col0\" class=\"data row45 col0\" >Atom Swapping</td>\n",
              "      <td id=\"T_4b03d_row45_col1\" class=\"data row45 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row45_col2\" class=\"data row45 col2\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row45_col3\" class=\"data row45 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row45_col4\" class=\"data row45 col4\" >845.983000</td>\n",
              "      <td id=\"T_4b03d_row45_col5\" class=\"data row45 col5\" >4.711920</td>\n",
              "      <td id=\"T_4b03d_row45_col6\" class=\"data row45 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row45_col7\" class=\"data row45 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row45_col8\" class=\"data row45 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row45_col9\" class=\"data row45 col9\" >0.095721</td>\n",
              "      <td id=\"T_4b03d_row45_col10\" class=\"data row45 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row45_col11\" class=\"data row45 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row45_col12\" class=\"data row45 col12\" >0.418777</td>\n",
              "      <td id=\"T_4b03d_row45_col13\" class=\"data row45 col13\" >0.313347</td>\n",
              "      <td id=\"T_4b03d_row45_col14\" class=\"data row45 col14\" >0.718345</td>\n",
              "      <td id=\"T_4b03d_row45_col15\" class=\"data row45 col15\" >0.008119</td>\n",
              "      <td id=\"T_4b03d_row45_col16\" class=\"data row45 col16\" >0.130581</td>\n",
              "      <td id=\"T_4b03d_row45_col17\" class=\"data row45 col17\" >0.053637</td>\n",
              "      <td id=\"T_4b03d_row45_col18\" class=\"data row45 col18\" >0.209448</td>\n",
              "      <td id=\"T_4b03d_row45_col19\" class=\"data row45 col19\" >0.037448</td>\n",
              "      <td id=\"T_4b03d_row45_col20\" class=\"data row45 col20\" >0.098746</td>\n",
              "      <td id=\"T_4b03d_row45_col21\" class=\"data row45 col21\" >0.741527</td>\n",
              "      <td id=\"T_4b03d_row45_col22\" class=\"data row45 col22\" >0.859824</td>\n",
              "      <td id=\"T_4b03d_row45_col23\" class=\"data row45 col23\" >0.156202</td>\n",
              "      <td id=\"T_4b03d_row45_col24\" class=\"data row45 col24\" >0.234659</td>\n",
              "      <td id=\"T_4b03d_row45_col25\" class=\"data row45 col25\" >0.723130</td>\n",
              "      <td id=\"T_4b03d_row45_col26\" class=\"data row45 col26\" >0.993656</td>\n",
              "      <td id=\"T_4b03d_row45_col27\" class=\"data row45 col27\" >0.116319</td>\n",
              "      <td id=\"T_4b03d_row45_col28\" class=\"data row45 col28\" >0.293043</td>\n",
              "      <td id=\"T_4b03d_row45_col29\" class=\"data row45 col29\" >0.073669</td>\n",
              "      <td id=\"T_4b03d_row45_col30\" class=\"data row45 col30\" >0.223948</td>\n",
              "      <td id=\"T_4b03d_row45_col31\" class=\"data row45 col31\" >0.024769</td>\n",
              "      <td id=\"T_4b03d_row45_col32\" class=\"data row45 col32\" >0.182238</td>\n",
              "      <td id=\"T_4b03d_row45_col33\" class=\"data row45 col33\" >0.068505</td>\n",
              "      <td id=\"T_4b03d_row45_col34\" class=\"data row45 col34\" >0.647033</td>\n",
              "      <td id=\"T_4b03d_row45_col35\" class=\"data row45 col35\" >0.920296</td>\n",
              "      <td id=\"T_4b03d_row45_col36\" class=\"data row45 col36\" >0.360788</td>\n",
              "      <td id=\"T_4b03d_row45_col37\" class=\"data row45 col37\" >0.176250</td>\n",
              "      <td id=\"T_4b03d_row45_col38\" class=\"data row45 col38\" >0.056515</td>\n",
              "      <td id=\"T_4b03d_row45_col39\" class=\"data row45 col39\" >0.933698</td>\n",
              "      <td id=\"T_4b03d_row45_col40\" class=\"data row45 col40\" >0.230766</td>\n",
              "      <td id=\"T_4b03d_row45_col41\" class=\"data row45 col41\" >0.094041</td>\n",
              "      <td id=\"T_4b03d_row45_col42\" class=\"data row45 col42\" >0.700260</td>\n",
              "      <td id=\"T_4b03d_row45_col43\" class=\"data row45 col43\" >-5.484095</td>\n",
              "      <td id=\"T_4b03d_row45_col44\" class=\"data row45 col44\" >130.905576</td>\n",
              "      <td id=\"T_4b03d_row45_col45\" class=\"data row45 col45\" >122.118249</td>\n",
              "      <td id=\"T_4b03d_row45_col46\" class=\"data row45 col46\" >43.168345</td>\n",
              "      <td id=\"T_4b03d_row45_col47\" class=\"data row45 col47\" >-9.874909</td>\n",
              "      <td id=\"T_4b03d_row45_col48\" class=\"data row45 col48\" >4.322272</td>\n",
              "      <td id=\"T_4b03d_row45_col49\" class=\"data row45 col49\" >3.172219</td>\n",
              "      <td id=\"T_4b03d_row45_col50\" class=\"data row45 col50\" >102.863042</td>\n",
              "      <td id=\"T_4b03d_row45_col51\" class=\"data row45 col51\" >-5.653315</td>\n",
              "      <td id=\"T_4b03d_row45_col52\" class=\"data row45 col52\" >3.011813</td>\n",
              "      <td id=\"T_4b03d_row45_col53\" class=\"data row45 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row45_col54\" class=\"data row45 col54\" >86.234975</td>\n",
              "      <td id=\"T_4b03d_row45_col55\" class=\"data row45 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row45_col56\" class=\"data row45 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row45_col57\" class=\"data row45 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row45_col58\" class=\"data row45 col58\" >5.312136</td>\n",
              "      <td id=\"T_4b03d_row45_col59\" class=\"data row45 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row45_col60\" class=\"data row45 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row45_col61\" class=\"data row45 col61\" >77.394339</td>\n",
              "      <td id=\"T_4b03d_row45_col62\" class=\"data row45 col62\" >19.077162</td>\n",
              "      <td id=\"T_4b03d_row45_col63\" class=\"data row45 col63\" >38.929818</td>\n",
              "      <td id=\"T_4b03d_row45_col64\" class=\"data row45 col64\" >31.252423</td>\n",
              "      <td id=\"T_4b03d_row45_col65\" class=\"data row45 col65\" >54.245832</td>\n",
              "      <td id=\"T_4b03d_row45_col66\" class=\"data row45 col66\" >19.503684</td>\n",
              "      <td id=\"T_4b03d_row45_col67\" class=\"data row45 col67\" >76.618845</td>\n",
              "      <td id=\"T_4b03d_row45_col68\" class=\"data row45 col68\" >26.870880</td>\n",
              "      <td id=\"T_4b03d_row45_col69\" class=\"data row45 col69\" >61.884451</td>\n",
              "      <td id=\"T_4b03d_row45_col70\" class=\"data row45 col70\" >79.100427</td>\n",
              "      <td id=\"T_4b03d_row45_col71\" class=\"data row45 col71\" >93.214424</td>\n",
              "      <td id=\"T_4b03d_row45_col72\" class=\"data row45 col72\" >50.213261</td>\n",
              "      <td id=\"T_4b03d_row45_col73\" class=\"data row45 col73\" >70.027142</td>\n",
              "      <td id=\"T_4b03d_row45_col74\" class=\"data row45 col74\" >65.917022</td>\n",
              "      <td id=\"T_4b03d_row45_col75\" class=\"data row45 col75\" >46.064366</td>\n",
              "      <td id=\"T_4b03d_row45_col76\" class=\"data row45 col76\" >91.508337</td>\n",
              "      <td id=\"T_4b03d_row45_col77\" class=\"data row45 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row45_col78\" class=\"data row45 col78\" >67.119038</td>\n",
              "      <td id=\"T_4b03d_row45_col79\" class=\"data row45 col79\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row45_col80\" class=\"data row45 col80\" >59.596743</td>\n",
              "      <td id=\"T_4b03d_row45_col81\" class=\"data row45 col81\" >77.239240</td>\n",
              "      <td id=\"T_4b03d_row45_col82\" class=\"data row45 col82\" >87.708414</td>\n",
              "      <td id=\"T_4b03d_row45_col83\" class=\"data row45 col83\" >42.303218</td>\n",
              "      <td id=\"T_4b03d_row45_col84\" class=\"data row45 col84\" >95.230710</td>\n",
              "      <td id=\"T_4b03d_row45_col85\" class=\"data row45 col85\" >75.378054</td>\n",
              "      <td id=\"T_4b03d_row45_col86\" class=\"data row45 col86\" >93.020551</td>\n",
              "      <td id=\"T_4b03d_row45_col87\" class=\"data row45 col87\" >76.657619</td>\n",
              "      <td id=\"T_4b03d_row45_col88\" class=\"data row45 col88\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row45_col89\" class=\"data row45 col89\" >87.398216</td>\n",
              "      <td id=\"T_4b03d_row45_col90\" class=\"data row45 col90\" >4.769290</td>\n",
              "      <td id=\"T_4b03d_row45_col91\" class=\"data row45 col91\" >72.314851</td>\n",
              "      <td id=\"T_4b03d_row45_col92\" class=\"data row45 col92\" >23.807677</td>\n",
              "      <td id=\"T_4b03d_row45_col93\" class=\"data row45 col93\" >97.130671</td>\n",
              "      <td id=\"T_4b03d_row45_col94\" class=\"data row45 col94\" >98.255138</td>\n",
              "      <td id=\"T_4b03d_row45_col95\" class=\"data row45 col95\" >84.102365</td>\n",
              "      <td id=\"T_4b03d_row45_col96\" class=\"data row45 col96\" >47.537805</td>\n",
              "      <td id=\"T_4b03d_row45_col97\" class=\"data row45 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row45_col98\" class=\"data row45 col98\" >84.412563</td>\n",
              "      <td id=\"T_4b03d_row45_col99\" class=\"data row45 col99\" >94.532765</td>\n",
              "      <td id=\"T_4b03d_row45_col100\" class=\"data row45 col100\" >7.599845</td>\n",
              "      <td id=\"T_4b03d_row45_col101\" class=\"data row45 col101\" >61.264056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "      <td id=\"T_4b03d_row46_col0\" class=\"data row46 col0\" >Substructure Replacement</td>\n",
              "      <td id=\"T_4b03d_row46_col1\" class=\"data row46 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COC(=O)c1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row46_col2\" class=\"data row46 col2\" >2.282454</td>\n",
              "      <td id=\"T_4b03d_row46_col3\" class=\"data row46 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COC(=O)c1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row46_col4\" class=\"data row46 col4\" >843.971000</td>\n",
              "      <td id=\"T_4b03d_row46_col5\" class=\"data row46 col5\" >3.976500</td>\n",
              "      <td id=\"T_4b03d_row46_col6\" class=\"data row46 col6\" >13</td>\n",
              "      <td id=\"T_4b03d_row46_col7\" class=\"data row46 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row46_col8\" class=\"data row46 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row46_col9\" class=\"data row46 col9\" >0.105139</td>\n",
              "      <td id=\"T_4b03d_row46_col10\" class=\"data row46 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row46_col11\" class=\"data row46 col11\" >244.140000</td>\n",
              "      <td id=\"T_4b03d_row46_col12\" class=\"data row46 col12\" >0.545653</td>\n",
              "      <td id=\"T_4b03d_row46_col13\" class=\"data row46 col13\" >0.501099</td>\n",
              "      <td id=\"T_4b03d_row46_col14\" class=\"data row46 col14\" >0.826695</td>\n",
              "      <td id=\"T_4b03d_row46_col15\" class=\"data row46 col15\" >0.017611</td>\n",
              "      <td id=\"T_4b03d_row46_col16\" class=\"data row46 col16\" >0.155758</td>\n",
              "      <td id=\"T_4b03d_row46_col17\" class=\"data row46 col17\" >0.055868</td>\n",
              "      <td id=\"T_4b03d_row46_col18\" class=\"data row46 col18\" >0.239527</td>\n",
              "      <td id=\"T_4b03d_row46_col19\" class=\"data row46 col19\" >0.070845</td>\n",
              "      <td id=\"T_4b03d_row46_col20\" class=\"data row46 col20\" >0.214563</td>\n",
              "      <td id=\"T_4b03d_row46_col21\" class=\"data row46 col21\" >0.664489</td>\n",
              "      <td id=\"T_4b03d_row46_col22\" class=\"data row46 col22\" >0.944629</td>\n",
              "      <td id=\"T_4b03d_row46_col23\" class=\"data row46 col23\" >0.182028</td>\n",
              "      <td id=\"T_4b03d_row46_col24\" class=\"data row46 col24\" >0.542302</td>\n",
              "      <td id=\"T_4b03d_row46_col25\" class=\"data row46 col25\" >0.754143</td>\n",
              "      <td id=\"T_4b03d_row46_col26\" class=\"data row46 col26\" >0.990564</td>\n",
              "      <td id=\"T_4b03d_row46_col27\" class=\"data row46 col27\" >0.096233</td>\n",
              "      <td id=\"T_4b03d_row46_col28\" class=\"data row46 col28\" >0.176162</td>\n",
              "      <td id=\"T_4b03d_row46_col29\" class=\"data row46 col29\" >0.117037</td>\n",
              "      <td id=\"T_4b03d_row46_col30\" class=\"data row46 col30\" >0.193063</td>\n",
              "      <td id=\"T_4b03d_row46_col31\" class=\"data row46 col31\" >0.035420</td>\n",
              "      <td id=\"T_4b03d_row46_col32\" class=\"data row46 col32\" >0.135038</td>\n",
              "      <td id=\"T_4b03d_row46_col33\" class=\"data row46 col33\" >0.108704</td>\n",
              "      <td id=\"T_4b03d_row46_col34\" class=\"data row46 col34\" >0.644883</td>\n",
              "      <td id=\"T_4b03d_row46_col35\" class=\"data row46 col35\" >0.777939</td>\n",
              "      <td id=\"T_4b03d_row46_col36\" class=\"data row46 col36\" >0.333323</td>\n",
              "      <td id=\"T_4b03d_row46_col37\" class=\"data row46 col37\" >0.274412</td>\n",
              "      <td id=\"T_4b03d_row46_col38\" class=\"data row46 col38\" >0.049555</td>\n",
              "      <td id=\"T_4b03d_row46_col39\" class=\"data row46 col39\" >0.841068</td>\n",
              "      <td id=\"T_4b03d_row46_col40\" class=\"data row46 col40\" >0.214116</td>\n",
              "      <td id=\"T_4b03d_row46_col41\" class=\"data row46 col41\" >0.147014</td>\n",
              "      <td id=\"T_4b03d_row46_col42\" class=\"data row46 col42\" >0.753237</td>\n",
              "      <td id=\"T_4b03d_row46_col43\" class=\"data row46 col43\" >-5.809363</td>\n",
              "      <td id=\"T_4b03d_row46_col44\" class=\"data row46 col44\" >127.555468</td>\n",
              "      <td id=\"T_4b03d_row46_col45\" class=\"data row46 col45\" >102.375238</td>\n",
              "      <td id=\"T_4b03d_row46_col46\" class=\"data row46 col46\" >41.669499</td>\n",
              "      <td id=\"T_4b03d_row46_col47\" class=\"data row46 col47\" >-9.445219</td>\n",
              "      <td id=\"T_4b03d_row46_col48\" class=\"data row46 col48\" >4.003892</td>\n",
              "      <td id=\"T_4b03d_row46_col49\" class=\"data row46 col49\" >2.282455</td>\n",
              "      <td id=\"T_4b03d_row46_col50\" class=\"data row46 col50\" >96.454401</td>\n",
              "      <td id=\"T_4b03d_row46_col51\" class=\"data row46 col51\" >-5.584215</td>\n",
              "      <td id=\"T_4b03d_row46_col52\" class=\"data row46 col52\" >4.730081</td>\n",
              "      <td id=\"T_4b03d_row46_col53\" class=\"data row46 col53\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row46_col54\" class=\"data row46 col54\" >77.239240</td>\n",
              "      <td id=\"T_4b03d_row46_col55\" class=\"data row46 col55\" >93.544009</td>\n",
              "      <td id=\"T_4b03d_row46_col56\" class=\"data row46 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row46_col57\" class=\"data row46 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row46_col58\" class=\"data row46 col58\" >5.699884</td>\n",
              "      <td id=\"T_4b03d_row46_col59\" class=\"data row46 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row46_col60\" class=\"data row46 col60\" >94.455215</td>\n",
              "      <td id=\"T_4b03d_row46_col61\" class=\"data row46 col61\" >85.459480</td>\n",
              "      <td id=\"T_4b03d_row46_col62\" class=\"data row46 col62\" >30.709577</td>\n",
              "      <td id=\"T_4b03d_row46_col63\" class=\"data row46 col63\" >55.486623</td>\n",
              "      <td id=\"T_4b03d_row46_col64\" class=\"data row46 col64\" >39.588988</td>\n",
              "      <td id=\"T_4b03d_row46_col65\" class=\"data row46 col65\" >57.658007</td>\n",
              "      <td id=\"T_4b03d_row46_col66\" class=\"data row46 col66\" >20.279178</td>\n",
              "      <td id=\"T_4b03d_row46_col67\" class=\"data row46 col67\" >78.945328</td>\n",
              "      <td id=\"T_4b03d_row46_col68\" class=\"data row46 col68\" >40.829779</td>\n",
              "      <td id=\"T_4b03d_row46_col69\" class=\"data row46 col69\" >74.525010</td>\n",
              "      <td id=\"T_4b03d_row46_col70\" class=\"data row46 col70\" >68.631252</td>\n",
              "      <td id=\"T_4b03d_row46_col71\" class=\"data row46 col71\" >97.169446</td>\n",
              "      <td id=\"T_4b03d_row46_col72\" class=\"data row46 col72\" >55.370299</td>\n",
              "      <td id=\"T_4b03d_row46_col73\" class=\"data row46 col73\" >91.275688</td>\n",
              "      <td id=\"T_4b03d_row46_col74\" class=\"data row46 col74\" >68.165956</td>\n",
              "      <td id=\"T_4b03d_row46_col75\" class=\"data row46 col75\" >42.148119</td>\n",
              "      <td id=\"T_4b03d_row46_col76\" class=\"data row46 col76\" >90.500194</td>\n",
              "      <td id=\"T_4b03d_row46_col77\" class=\"data row46 col77\" >91.159364</td>\n",
              "      <td id=\"T_4b03d_row46_col78\" class=\"data row46 col78\" >73.943389</td>\n",
              "      <td id=\"T_4b03d_row46_col79\" class=\"data row46 col79\" >84.645211</td>\n",
              "      <td id=\"T_4b03d_row46_col80\" class=\"data row46 col80\" >68.902675</td>\n",
              "      <td id=\"T_4b03d_row46_col81\" class=\"data row46 col81\" >66.847615</td>\n",
              "      <td id=\"T_4b03d_row46_col82\" class=\"data row46 col82\" >91.896084</td>\n",
              "      <td id=\"T_4b03d_row46_col83\" class=\"data row46 col83\" >42.031795</td>\n",
              "      <td id=\"T_4b03d_row46_col84\" class=\"data row46 col84\" >87.243117</td>\n",
              "      <td id=\"T_4b03d_row46_col85\" class=\"data row46 col85\" >73.555642</td>\n",
              "      <td id=\"T_4b03d_row46_col86\" class=\"data row46 col86\" >96.238852</td>\n",
              "      <td id=\"T_4b03d_row46_col87\" class=\"data row46 col87\" >74.757658</td>\n",
              "      <td id=\"T_4b03d_row46_col88\" class=\"data row46 col88\" >96.355176</td>\n",
              "      <td id=\"T_4b03d_row46_col89\" class=\"data row46 col89\" >86.467623</td>\n",
              "      <td id=\"T_4b03d_row46_col90\" class=\"data row46 col90\" >13.028306</td>\n",
              "      <td id=\"T_4b03d_row46_col91\" class=\"data row46 col91\" >75.184180</td>\n",
              "      <td id=\"T_4b03d_row46_col92\" class=\"data row46 col92\" >15.548662</td>\n",
              "      <td id=\"T_4b03d_row46_col93\" class=\"data row46 col93\" >96.549050</td>\n",
              "      <td id=\"T_4b03d_row46_col94\" class=\"data row46 col94\" >96.044979</td>\n",
              "      <td id=\"T_4b03d_row46_col95\" class=\"data row46 col95\" >83.249321</td>\n",
              "      <td id=\"T_4b03d_row46_col96\" class=\"data row46 col96\" >52.190772</td>\n",
              "      <td id=\"T_4b03d_row46_col97\" class=\"data row46 col97\" >97.557193</td>\n",
              "      <td id=\"T_4b03d_row46_col98\" class=\"data row46 col98\" >65.994572</td>\n",
              "      <td id=\"T_4b03d_row46_col99\" class=\"data row46 col99\" >85.071733</td>\n",
              "      <td id=\"T_4b03d_row46_col100\" class=\"data row46 col100\" >7.987592</td>\n",
              "      <td id=\"T_4b03d_row46_col101\" class=\"data row46 col101\" >73.012796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "      <td id=\"T_4b03d_row47_col0\" class=\"data row47 col0\" >Substructure Replacement</td>\n",
              "      <td id=\"T_4b03d_row47_col1\" class=\"data row47 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row47_col2\" class=\"data row47 col2\" >2.449058</td>\n",
              "      <td id=\"T_4b03d_row47_col3\" class=\"data row47 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row47_col4\" class=\"data row47 col4\" >829.988000</td>\n",
              "      <td id=\"T_4b03d_row47_col5\" class=\"data row47 col5\" >4.336300</td>\n",
              "      <td id=\"T_4b03d_row47_col6\" class=\"data row47 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row47_col7\" class=\"data row47 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row47_col8\" class=\"data row47 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row47_col9\" class=\"data row47 col9\" >0.098167</td>\n",
              "      <td id=\"T_4b03d_row47_col10\" class=\"data row47 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row47_col11\" class=\"data row47 col11\" >227.070000</td>\n",
              "      <td id=\"T_4b03d_row47_col12\" class=\"data row47 col12\" >0.555910</td>\n",
              "      <td id=\"T_4b03d_row47_col13\" class=\"data row47 col13\" >0.473468</td>\n",
              "      <td id=\"T_4b03d_row47_col14\" class=\"data row47 col14\" >0.828686</td>\n",
              "      <td id=\"T_4b03d_row47_col15\" class=\"data row47 col15\" >0.014313</td>\n",
              "      <td id=\"T_4b03d_row47_col16\" class=\"data row47 col16\" >0.169347</td>\n",
              "      <td id=\"T_4b03d_row47_col17\" class=\"data row47 col17\" >0.044692</td>\n",
              "      <td id=\"T_4b03d_row47_col18\" class=\"data row47 col18\" >0.190943</td>\n",
              "      <td id=\"T_4b03d_row47_col19\" class=\"data row47 col19\" >0.083978</td>\n",
              "      <td id=\"T_4b03d_row47_col20\" class=\"data row47 col20\" >0.176319</td>\n",
              "      <td id=\"T_4b03d_row47_col21\" class=\"data row47 col21\" >0.674975</td>\n",
              "      <td id=\"T_4b03d_row47_col22\" class=\"data row47 col22\" >0.960365</td>\n",
              "      <td id=\"T_4b03d_row47_col23\" class=\"data row47 col23\" >0.164205</td>\n",
              "      <td id=\"T_4b03d_row47_col24\" class=\"data row47 col24\" >0.527609</td>\n",
              "      <td id=\"T_4b03d_row47_col25\" class=\"data row47 col25\" >0.660738</td>\n",
              "      <td id=\"T_4b03d_row47_col26\" class=\"data row47 col26\" >0.992493</td>\n",
              "      <td id=\"T_4b03d_row47_col27\" class=\"data row47 col27\" >0.110393</td>\n",
              "      <td id=\"T_4b03d_row47_col28\" class=\"data row47 col28\" >0.198134</td>\n",
              "      <td id=\"T_4b03d_row47_col29\" class=\"data row47 col29\" >0.123265</td>\n",
              "      <td id=\"T_4b03d_row47_col30\" class=\"data row47 col30\" >0.178238</td>\n",
              "      <td id=\"T_4b03d_row47_col31\" class=\"data row47 col31\" >0.042955</td>\n",
              "      <td id=\"T_4b03d_row47_col32\" class=\"data row47 col32\" >0.157485</td>\n",
              "      <td id=\"T_4b03d_row47_col33\" class=\"data row47 col33\" >0.085282</td>\n",
              "      <td id=\"T_4b03d_row47_col34\" class=\"data row47 col34\" >0.652735</td>\n",
              "      <td id=\"T_4b03d_row47_col35\" class=\"data row47 col35\" >0.822476</td>\n",
              "      <td id=\"T_4b03d_row47_col36\" class=\"data row47 col36\" >0.365200</td>\n",
              "      <td id=\"T_4b03d_row47_col37\" class=\"data row47 col37\" >0.259701</td>\n",
              "      <td id=\"T_4b03d_row47_col38\" class=\"data row47 col38\" >0.052113</td>\n",
              "      <td id=\"T_4b03d_row47_col39\" class=\"data row47 col39\" >0.852567</td>\n",
              "      <td id=\"T_4b03d_row47_col40\" class=\"data row47 col40\" >0.206238</td>\n",
              "      <td id=\"T_4b03d_row47_col41\" class=\"data row47 col41\" >0.160131</td>\n",
              "      <td id=\"T_4b03d_row47_col42\" class=\"data row47 col42\" >0.708860</td>\n",
              "      <td id=\"T_4b03d_row47_col43\" class=\"data row47 col43\" >-5.731778</td>\n",
              "      <td id=\"T_4b03d_row47_col44\" class=\"data row47 col44\" >129.942582</td>\n",
              "      <td id=\"T_4b03d_row47_col45\" class=\"data row47 col45\" >94.923466</td>\n",
              "      <td id=\"T_4b03d_row47_col46\" class=\"data row47 col46\" >35.159172</td>\n",
              "      <td id=\"T_4b03d_row47_col47\" class=\"data row47 col47\" >-9.219391</td>\n",
              "      <td id=\"T_4b03d_row47_col48\" class=\"data row47 col48\" >4.237781</td>\n",
              "      <td id=\"T_4b03d_row47_col49\" class=\"data row47 col49\" >2.449057</td>\n",
              "      <td id=\"T_4b03d_row47_col50\" class=\"data row47 col50\" >96.448195</td>\n",
              "      <td id=\"T_4b03d_row47_col51\" class=\"data row47 col51\" >-5.613785</td>\n",
              "      <td id=\"T_4b03d_row47_col52\" class=\"data row47 col52\" >3.323756</td>\n",
              "      <td id=\"T_4b03d_row47_col53\" class=\"data row47 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row47_col54\" class=\"data row47 col54\" >82.357503</td>\n",
              "      <td id=\"T_4b03d_row47_col55\" class=\"data row47 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row47_col56\" class=\"data row47 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row47_col57\" class=\"data row47 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row47_col58\" class=\"data row47 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row47_col59\" class=\"data row47 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row47_col60\" class=\"data row47 col60\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row47_col61\" class=\"data row47 col61\" >86.002326</td>\n",
              "      <td id=\"T_4b03d_row47_col62\" class=\"data row47 col62\" >28.887166</td>\n",
              "      <td id=\"T_4b03d_row47_col63\" class=\"data row47 col63\" >55.758046</td>\n",
              "      <td id=\"T_4b03d_row47_col64\" class=\"data row47 col64\" >37.301280</td>\n",
              "      <td id=\"T_4b03d_row47_col65\" class=\"data row47 col65\" >59.247770</td>\n",
              "      <td id=\"T_4b03d_row47_col66\" class=\"data row47 col66\" >16.556805</td>\n",
              "      <td id=\"T_4b03d_row47_col67\" class=\"data row47 col67\" >75.300504</td>\n",
              "      <td id=\"T_4b03d_row47_col68\" class=\"data row47 col68\" >44.358278</td>\n",
              "      <td id=\"T_4b03d_row47_col69\" class=\"data row47 col69\" >71.539356</td>\n",
              "      <td id=\"T_4b03d_row47_col70\" class=\"data row47 col70\" >69.872043</td>\n",
              "      <td id=\"T_4b03d_row47_col71\" class=\"data row47 col71\" >97.944940</td>\n",
              "      <td id=\"T_4b03d_row47_col72\" class=\"data row47 col72\" >52.151997</td>\n",
              "      <td id=\"T_4b03d_row47_col73\" class=\"data row47 col73\" >90.577743</td>\n",
              "      <td id=\"T_4b03d_row47_col74\" class=\"data row47 col74\" >62.892594</td>\n",
              "      <td id=\"T_4b03d_row47_col75\" class=\"data row47 col75\" >44.397053</td>\n",
              "      <td id=\"T_4b03d_row47_col76\" class=\"data row47 col76\" >91.159364</td>\n",
              "      <td id=\"T_4b03d_row47_col77\" class=\"data row47 col77\" >92.012408</td>\n",
              "      <td id=\"T_4b03d_row47_col78\" class=\"data row47 col78\" >74.602559</td>\n",
              "      <td id=\"T_4b03d_row47_col79\" class=\"data row47 col79\" >83.404420</td>\n",
              "      <td id=\"T_4b03d_row47_col80\" class=\"data row47 col80\" >73.129120</td>\n",
              "      <td id=\"T_4b03d_row47_col81\" class=\"data row47 col81\" >72.314851</td>\n",
              "      <td id=\"T_4b03d_row47_col82\" class=\"data row47 col82\" >89.763474</td>\n",
              "      <td id=\"T_4b03d_row47_col83\" class=\"data row47 col83\" >42.535867</td>\n",
              "      <td id=\"T_4b03d_row47_col84\" class=\"data row47 col84\" >88.832881</td>\n",
              "      <td id=\"T_4b03d_row47_col85\" class=\"data row47 col85\" >75.765801</td>\n",
              "      <td id=\"T_4b03d_row47_col86\" class=\"data row47 col86\" >95.734781</td>\n",
              "      <td id=\"T_4b03d_row47_col87\" class=\"data row47 col87\" >75.339279</td>\n",
              "      <td id=\"T_4b03d_row47_col88\" class=\"data row47 col88\" >96.587825</td>\n",
              "      <td id=\"T_4b03d_row47_col89\" class=\"data row47 col89\" >85.653354</td>\n",
              "      <td id=\"T_4b03d_row47_col90\" class=\"data row47 col90\" >14.928267</td>\n",
              "      <td id=\"T_4b03d_row47_col91\" class=\"data row47 col91\" >72.896472</td>\n",
              "      <td id=\"T_4b03d_row47_col92\" class=\"data row47 col92\" >17.409849</td>\n",
              "      <td id=\"T_4b03d_row47_col93\" class=\"data row47 col93\" >96.975572</td>\n",
              "      <td id=\"T_4b03d_row47_col94\" class=\"data row47 col94\" >94.881737</td>\n",
              "      <td id=\"T_4b03d_row47_col95\" class=\"data row47 col95\" >79.449399</td>\n",
              "      <td id=\"T_4b03d_row47_col96\" class=\"data row47 col96\" >54.594804</td>\n",
              "      <td id=\"T_4b03d_row47_col97\" class=\"data row47 col97\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row47_col98\" class=\"data row47 col98\" >68.902675</td>\n",
              "      <td id=\"T_4b03d_row47_col99\" class=\"data row47 col99\" >85.071733</td>\n",
              "      <td id=\"T_4b03d_row47_col100\" class=\"data row47 col100\" >7.754944</td>\n",
              "      <td id=\"T_4b03d_row47_col101\" class=\"data row47 col101\" >63.629314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "      <td id=\"T_4b03d_row48_col0\" class=\"data row48 col0\" >Stereochemistry Alteration</td>\n",
              "      <td id=\"T_4b03d_row48_col1\" class=\"data row48 col1\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row48_col2\" class=\"data row48 col2\" >3.050107</td>\n",
              "      <td id=\"T_4b03d_row48_col3\" class=\"data row48 col3\" >CC(=O)O[C@@H](C(=O)[C@@]1(C)[C@H](COCc2ccccc2)[C@]2(OC(C)=O)CO[C@@H]2C[C@@H]1O)C(=C(C)[C@@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row48_col4\" class=\"data row48 col4\" >831.956000</td>\n",
              "      <td id=\"T_4b03d_row48_col5\" class=\"data row48 col5\" >4.403500</td>\n",
              "      <td id=\"T_4b03d_row48_col6\" class=\"data row48 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row48_col7\" class=\"data row48 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row48_col8\" class=\"data row48 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row48_col9\" class=\"data row48 col9\" >0.098075</td>\n",
              "      <td id=\"T_4b03d_row48_col10\" class=\"data row48 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row48_col11\" class=\"data row48 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row48_col12\" class=\"data row48 col12\" >0.392741</td>\n",
              "      <td id=\"T_4b03d_row48_col13\" class=\"data row48 col13\" >0.311170</td>\n",
              "      <td id=\"T_4b03d_row48_col14\" class=\"data row48 col14\" >0.716180</td>\n",
              "      <td id=\"T_4b03d_row48_col15\" class=\"data row48 col15\" >0.009838</td>\n",
              "      <td id=\"T_4b03d_row48_col16\" class=\"data row48 col16\" >0.113925</td>\n",
              "      <td id=\"T_4b03d_row48_col17\" class=\"data row48 col17\" >0.055124</td>\n",
              "      <td id=\"T_4b03d_row48_col18\" class=\"data row48 col18\" >0.204073</td>\n",
              "      <td id=\"T_4b03d_row48_col19\" class=\"data row48 col19\" >0.038222</td>\n",
              "      <td id=\"T_4b03d_row48_col20\" class=\"data row48 col20\" >0.091014</td>\n",
              "      <td id=\"T_4b03d_row48_col21\" class=\"data row48 col21\" >0.690489</td>\n",
              "      <td id=\"T_4b03d_row48_col22\" class=\"data row48 col22\" >0.822379</td>\n",
              "      <td id=\"T_4b03d_row48_col23\" class=\"data row48 col23\" >0.125814</td>\n",
              "      <td id=\"T_4b03d_row48_col24\" class=\"data row48 col24\" >0.255584</td>\n",
              "      <td id=\"T_4b03d_row48_col25\" class=\"data row48 col25\" >0.738721</td>\n",
              "      <td id=\"T_4b03d_row48_col26\" class=\"data row48 col26\" >0.991385</td>\n",
              "      <td id=\"T_4b03d_row48_col27\" class=\"data row48 col27\" >0.131221</td>\n",
              "      <td id=\"T_4b03d_row48_col28\" class=\"data row48 col28\" >0.293721</td>\n",
              "      <td id=\"T_4b03d_row48_col29\" class=\"data row48 col29\" >0.085508</td>\n",
              "      <td id=\"T_4b03d_row48_col30\" class=\"data row48 col30\" >0.218241</td>\n",
              "      <td id=\"T_4b03d_row48_col31\" class=\"data row48 col31\" >0.022697</td>\n",
              "      <td id=\"T_4b03d_row48_col32\" class=\"data row48 col32\" >0.161245</td>\n",
              "      <td id=\"T_4b03d_row48_col33\" class=\"data row48 col33\" >0.074677</td>\n",
              "      <td id=\"T_4b03d_row48_col34\" class=\"data row48 col34\" >0.630097</td>\n",
              "      <td id=\"T_4b03d_row48_col35\" class=\"data row48 col35\" >0.832328</td>\n",
              "      <td id=\"T_4b03d_row48_col36\" class=\"data row48 col36\" >0.346140</td>\n",
              "      <td id=\"T_4b03d_row48_col37\" class=\"data row48 col37\" >0.184417</td>\n",
              "      <td id=\"T_4b03d_row48_col38\" class=\"data row48 col38\" >0.049758</td>\n",
              "      <td id=\"T_4b03d_row48_col39\" class=\"data row48 col39\" >0.916799</td>\n",
              "      <td id=\"T_4b03d_row48_col40\" class=\"data row48 col40\" >0.241353</td>\n",
              "      <td id=\"T_4b03d_row48_col41\" class=\"data row48 col41\" >0.100508</td>\n",
              "      <td id=\"T_4b03d_row48_col42\" class=\"data row48 col42\" >0.664185</td>\n",
              "      <td id=\"T_4b03d_row48_col43\" class=\"data row48 col43\" >-5.518311</td>\n",
              "      <td id=\"T_4b03d_row48_col44\" class=\"data row48 col44\" >131.734711</td>\n",
              "      <td id=\"T_4b03d_row48_col45\" class=\"data row48 col45\" >112.423690</td>\n",
              "      <td id=\"T_4b03d_row48_col46\" class=\"data row48 col46\" >34.179409</td>\n",
              "      <td id=\"T_4b03d_row48_col47\" class=\"data row48 col47\" >-10.422555</td>\n",
              "      <td id=\"T_4b03d_row48_col48\" class=\"data row48 col48\" >4.379547</td>\n",
              "      <td id=\"T_4b03d_row48_col49\" class=\"data row48 col49\" >3.050107</td>\n",
              "      <td id=\"T_4b03d_row48_col50\" class=\"data row48 col50\" >103.017143</td>\n",
              "      <td id=\"T_4b03d_row48_col51\" class=\"data row48 col51\" >-5.820459</td>\n",
              "      <td id=\"T_4b03d_row48_col52\" class=\"data row48 col52\" >3.771918</td>\n",
              "      <td id=\"T_4b03d_row48_col53\" class=\"data row48 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row48_col54\" class=\"data row48 col54\" >82.900349</td>\n",
              "      <td id=\"T_4b03d_row48_col55\" class=\"data row48 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row48_col56\" class=\"data row48 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row48_col57\" class=\"data row48 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row48_col58\" class=\"data row48 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row48_col59\" class=\"data row48 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row48_col60\" class=\"data row48 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row48_col61\" class=\"data row48 col61\" >75.494378</td>\n",
              "      <td id=\"T_4b03d_row48_col62\" class=\"data row48 col62\" >19.038387</td>\n",
              "      <td id=\"T_4b03d_row48_col63\" class=\"data row48 col63\" >38.697169</td>\n",
              "      <td id=\"T_4b03d_row48_col64\" class=\"data row48 col64\" >33.695231</td>\n",
              "      <td id=\"T_4b03d_row48_col65\" class=\"data row48 col65\" >52.035673</td>\n",
              "      <td id=\"T_4b03d_row48_col66\" class=\"data row48 col66\" >20.124079</td>\n",
              "      <td id=\"T_4b03d_row48_col67\" class=\"data row48 col67\" >76.308647</td>\n",
              "      <td id=\"T_4b03d_row48_col68\" class=\"data row48 col68\" >27.413726</td>\n",
              "      <td id=\"T_4b03d_row48_col69\" class=\"data row48 col69\" >60.449787</td>\n",
              "      <td id=\"T_4b03d_row48_col70\" class=\"data row48 col70\" >71.655680</td>\n",
              "      <td id=\"T_4b03d_row48_col71\" class=\"data row48 col71\" >91.469562</td>\n",
              "      <td id=\"T_4b03d_row48_col72\" class=\"data row48 col72\" >44.009306</td>\n",
              "      <td id=\"T_4b03d_row48_col73\" class=\"data row48 col73\" >72.586274</td>\n",
              "      <td id=\"T_4b03d_row48_col74\" class=\"data row48 col74\" >67.274137</td>\n",
              "      <td id=\"T_4b03d_row48_col75\" class=\"data row48 col75\" >43.001163</td>\n",
              "      <td id=\"T_4b03d_row48_col76\" class=\"data row48 col76\" >92.128732</td>\n",
              "      <td id=\"T_4b03d_row48_col77\" class=\"data row48 col77\" >93.834820</td>\n",
              "      <td id=\"T_4b03d_row48_col78\" class=\"data row48 col78\" >69.096549</td>\n",
              "      <td id=\"T_4b03d_row48_col79\" class=\"data row48 col79\" >87.010469</td>\n",
              "      <td id=\"T_4b03d_row48_col80\" class=\"data row48 col80\" >56.921287</td>\n",
              "      <td id=\"T_4b03d_row48_col81\" class=\"data row48 col81\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row48_col82\" class=\"data row48 col82\" >88.445134</td>\n",
              "      <td id=\"T_4b03d_row48_col83\" class=\"data row48 col83\" >41.527724</td>\n",
              "      <td id=\"T_4b03d_row48_col84\" class=\"data row48 col84\" >89.375727</td>\n",
              "      <td id=\"T_4b03d_row48_col85\" class=\"data row48 col85\" >74.408686</td>\n",
              "      <td id=\"T_4b03d_row48_col86\" class=\"data row48 col86\" >93.330748</td>\n",
              "      <td id=\"T_4b03d_row48_col87\" class=\"data row48 col87\" >74.757658</td>\n",
              "      <td id=\"T_4b03d_row48_col88\" class=\"data row48 col88\" >98.255138</td>\n",
              "      <td id=\"T_4b03d_row48_col89\" class=\"data row48 col89\" >88.018612</td>\n",
              "      <td id=\"T_4b03d_row48_col90\" class=\"data row48 col90\" >5.583560</td>\n",
              "      <td id=\"T_4b03d_row48_col91\" class=\"data row48 col91\" >70.531214</td>\n",
              "      <td id=\"T_4b03d_row48_col92\" class=\"data row48 col92\" >22.644436</td>\n",
              "      <td id=\"T_4b03d_row48_col93\" class=\"data row48 col93\" >97.363319</td>\n",
              "      <td id=\"T_4b03d_row48_col94\" class=\"data row48 col94\" >97.208220</td>\n",
              "      <td id=\"T_4b03d_row48_col95\" class=\"data row48 col95\" >79.022877</td>\n",
              "      <td id=\"T_4b03d_row48_col96\" class=\"data row48 col96\" >42.225669</td>\n",
              "      <td id=\"T_4b03d_row48_col97\" class=\"data row48 col97\" >99.263280</td>\n",
              "      <td id=\"T_4b03d_row48_col98\" class=\"data row48 col98\" >81.892206</td>\n",
              "      <td id=\"T_4b03d_row48_col99\" class=\"data row48 col99\" >94.610314</td>\n",
              "      <td id=\"T_4b03d_row48_col100\" class=\"data row48 col100\" >6.436603</td>\n",
              "      <td id=\"T_4b03d_row48_col101\" class=\"data row48 col101\" >66.692516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "      <td id=\"T_4b03d_row49_col0\" class=\"data row49 col0\" >Atom Deletion</td>\n",
              "      <td id=\"T_4b03d_row49_col1\" class=\"data row49 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row49_col2\" class=\"data row49 col2\" >3.337291</td>\n",
              "      <td id=\"T_4b03d_row49_col3\" class=\"data row49 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row49_col4\" class=\"data row49 col4\" >832.000000</td>\n",
              "      <td id=\"T_4b03d_row49_col5\" class=\"data row49 col5\" >5.723120</td>\n",
              "      <td id=\"T_4b03d_row49_col6\" class=\"data row49 col6\" >11</td>\n",
              "      <td id=\"T_4b03d_row49_col7\" class=\"data row49 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row49_col8\" class=\"data row49 col8\" >1.000000</td>\n",
              "      <td id=\"T_4b03d_row49_col9\" class=\"data row49 col9\" >0.085185</td>\n",
              "      <td id=\"T_4b03d_row49_col10\" class=\"data row49 col10\" >8</td>\n",
              "      <td id=\"T_4b03d_row49_col11\" class=\"data row49 col11\" >206.260000</td>\n",
              "      <td id=\"T_4b03d_row49_col12\" class=\"data row49 col12\" >0.209764</td>\n",
              "      <td id=\"T_4b03d_row49_col13\" class=\"data row49 col13\" >0.300339</td>\n",
              "      <td id=\"T_4b03d_row49_col14\" class=\"data row49 col14\" >0.720810</td>\n",
              "      <td id=\"T_4b03d_row49_col15\" class=\"data row49 col15\" >0.009628</td>\n",
              "      <td id=\"T_4b03d_row49_col16\" class=\"data row49 col16\" >0.178035</td>\n",
              "      <td id=\"T_4b03d_row49_col17\" class=\"data row49 col17\" >0.078723</td>\n",
              "      <td id=\"T_4b03d_row49_col18\" class=\"data row49 col18\" >0.269128</td>\n",
              "      <td id=\"T_4b03d_row49_col19\" class=\"data row49 col19\" >0.029766</td>\n",
              "      <td id=\"T_4b03d_row49_col20\" class=\"data row49 col20\" >0.126835</td>\n",
              "      <td id=\"T_4b03d_row49_col21\" class=\"data row49 col21\" >0.760303</td>\n",
              "      <td id=\"T_4b03d_row49_col22\" class=\"data row49 col22\" >0.875372</td>\n",
              "      <td id=\"T_4b03d_row49_col23\" class=\"data row49 col23\" >0.234195</td>\n",
              "      <td id=\"T_4b03d_row49_col24\" class=\"data row49 col24\" >0.129898</td>\n",
              "      <td id=\"T_4b03d_row49_col25\" class=\"data row49 col25\" >0.587030</td>\n",
              "      <td id=\"T_4b03d_row49_col26\" class=\"data row49 col26\" >0.994755</td>\n",
              "      <td id=\"T_4b03d_row49_col27\" class=\"data row49 col27\" >0.102999</td>\n",
              "      <td id=\"T_4b03d_row49_col28\" class=\"data row49 col28\" >0.223594</td>\n",
              "      <td id=\"T_4b03d_row49_col29\" class=\"data row49 col29\" >0.047605</td>\n",
              "      <td id=\"T_4b03d_row49_col30\" class=\"data row49 col30\" >0.156939</td>\n",
              "      <td id=\"T_4b03d_row49_col31\" class=\"data row49 col31\" >0.018173</td>\n",
              "      <td id=\"T_4b03d_row49_col32\" class=\"data row49 col32\" >0.138869</td>\n",
              "      <td id=\"T_4b03d_row49_col33\" class=\"data row49 col33\" >0.048941</td>\n",
              "      <td id=\"T_4b03d_row49_col34\" class=\"data row49 col34\" >0.680067</td>\n",
              "      <td id=\"T_4b03d_row49_col35\" class=\"data row49 col35\" >0.934618</td>\n",
              "      <td id=\"T_4b03d_row49_col36\" class=\"data row49 col36\" >0.264518</td>\n",
              "      <td id=\"T_4b03d_row49_col37\" class=\"data row49 col37\" >0.056621</td>\n",
              "      <td id=\"T_4b03d_row49_col38\" class=\"data row49 col38\" >0.063418</td>\n",
              "      <td id=\"T_4b03d_row49_col39\" class=\"data row49 col39\" >0.913476</td>\n",
              "      <td id=\"T_4b03d_row49_col40\" class=\"data row49 col40\" >0.075015</td>\n",
              "      <td id=\"T_4b03d_row49_col41\" class=\"data row49 col41\" >0.087905</td>\n",
              "      <td id=\"T_4b03d_row49_col42\" class=\"data row49 col42\" >0.626150</td>\n",
              "      <td id=\"T_4b03d_row49_col43\" class=\"data row49 col43\" >-5.139126</td>\n",
              "      <td id=\"T_4b03d_row49_col44\" class=\"data row49 col44\" >140.406730</td>\n",
              "      <td id=\"T_4b03d_row49_col45\" class=\"data row49 col45\" >128.993391</td>\n",
              "      <td id=\"T_4b03d_row49_col46\" class=\"data row49 col46\" >37.169080</td>\n",
              "      <td id=\"T_4b03d_row49_col47\" class=\"data row49 col47\" >-9.145741</td>\n",
              "      <td id=\"T_4b03d_row49_col48\" class=\"data row49 col48\" >3.711954</td>\n",
              "      <td id=\"T_4b03d_row49_col49\" class=\"data row49 col49\" >3.337292</td>\n",
              "      <td id=\"T_4b03d_row49_col50\" class=\"data row49 col50\" >106.575604</td>\n",
              "      <td id=\"T_4b03d_row49_col51\" class=\"data row49 col51\" >-5.766011</td>\n",
              "      <td id=\"T_4b03d_row49_col52\" class=\"data row49 col52\" >2.485560</td>\n",
              "      <td id=\"T_4b03d_row49_col53\" class=\"data row49 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row49_col54\" class=\"data row49 col54\" >92.865452</td>\n",
              "      <td id=\"T_4b03d_row49_col55\" class=\"data row49 col55\" >90.732842</td>\n",
              "      <td id=\"T_4b03d_row49_col56\" class=\"data row49 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row49_col57\" class=\"data row49 col57\" >2.908104</td>\n",
              "      <td id=\"T_4b03d_row49_col58\" class=\"data row49 col58\" >4.730516</td>\n",
              "      <td id=\"T_4b03d_row49_col59\" class=\"data row49 col59\" >91.973633</td>\n",
              "      <td id=\"T_4b03d_row49_col60\" class=\"data row49 col60\" >92.438930</td>\n",
              "      <td id=\"T_4b03d_row49_col61\" class=\"data row49 col61\" >53.392788</td>\n",
              "      <td id=\"T_4b03d_row49_col62\" class=\"data row49 col62\" >18.340442</td>\n",
              "      <td id=\"T_4b03d_row49_col63\" class=\"data row49 col63\" >39.278790</td>\n",
              "      <td id=\"T_4b03d_row49_col64\" class=\"data row49 col64\" >33.307484</td>\n",
              "      <td id=\"T_4b03d_row49_col65\" class=\"data row49 col65\" >60.566111</td>\n",
              "      <td id=\"T_4b03d_row49_col66\" class=\"data row49 col66\" >28.499418</td>\n",
              "      <td id=\"T_4b03d_row49_col67\" class=\"data row49 col67\" >80.728965</td>\n",
              "      <td id=\"T_4b03d_row49_col68\" class=\"data row49 col68\" >22.179139</td>\n",
              "      <td id=\"T_4b03d_row49_col69\" class=\"data row49 col69\" >66.808841</td>\n",
              "      <td id=\"T_4b03d_row49_col70\" class=\"data row49 col70\" >82.357503</td>\n",
              "      <td id=\"T_4b03d_row49_col71\" class=\"data row49 col71\" >93.912369</td>\n",
              "      <td id=\"T_4b03d_row49_col72\" class=\"data row49 col72\" >63.939511</td>\n",
              "      <td id=\"T_4b03d_row49_col73\" class=\"data row49 col73\" >56.960062</td>\n",
              "      <td id=\"T_4b03d_row49_col74\" class=\"data row49 col74\" >58.937573</td>\n",
              "      <td id=\"T_4b03d_row49_col75\" class=\"data row49 col75\" >48.041877</td>\n",
              "      <td id=\"T_4b03d_row49_col76\" class=\"data row49 col76\" >90.926716</td>\n",
              "      <td id=\"T_4b03d_row49_col77\" class=\"data row49 col77\" >92.749128</td>\n",
              "      <td id=\"T_4b03d_row49_col78\" class=\"data row49 col78\" >60.294688</td>\n",
              "      <td id=\"T_4b03d_row49_col79\" class=\"data row49 col79\" >80.961613</td>\n",
              "      <td id=\"T_4b03d_row49_col80\" class=\"data row49 col80\" >50.678558</td>\n",
              "      <td id=\"T_4b03d_row49_col81\" class=\"data row49 col81\" >67.894533</td>\n",
              "      <td id=\"T_4b03d_row49_col82\" class=\"data row49 col82\" >83.443195</td>\n",
              "      <td id=\"T_4b03d_row49_col83\" class=\"data row49 col83\" >44.668476</td>\n",
              "      <td id=\"T_4b03d_row49_col84\" class=\"data row49 col84\" >96.316402</td>\n",
              "      <td id=\"T_4b03d_row49_col85\" class=\"data row49 col85\" >67.855758</td>\n",
              "      <td id=\"T_4b03d_row49_col86\" class=\"data row49 col86\" >81.504459</td>\n",
              "      <td id=\"T_4b03d_row49_col87\" class=\"data row49 col87\" >78.518806</td>\n",
              "      <td id=\"T_4b03d_row49_col88\" class=\"data row49 col88\" >98.216363</td>\n",
              "      <td id=\"T_4b03d_row49_col89\" class=\"data row49 col89\" >71.616906</td>\n",
              "      <td id=\"T_4b03d_row49_col90\" class=\"data row49 col90\" >4.071345</td>\n",
              "      <td id=\"T_4b03d_row49_col91\" class=\"data row49 col91\" >69.329197</td>\n",
              "      <td id=\"T_4b03d_row49_col92\" class=\"data row49 col92\" >40.286933</td>\n",
              "      <td id=\"T_4b03d_row49_col93\" class=\"data row49 col93\" >98.410237</td>\n",
              "      <td id=\"T_4b03d_row49_col94\" class=\"data row49 col94\" >98.914308</td>\n",
              "      <td id=\"T_4b03d_row49_col95\" class=\"data row49 col95\" >80.651415</td>\n",
              "      <td id=\"T_4b03d_row49_col96\" class=\"data row49 col96\" >55.409073</td>\n",
              "      <td id=\"T_4b03d_row49_col97\" class=\"data row49 col97\" >95.540907</td>\n",
              "      <td id=\"T_4b03d_row49_col98\" class=\"data row49 col98\" >87.281892</td>\n",
              "      <td id=\"T_4b03d_row49_col99\" class=\"data row49 col99\" >97.789841</td>\n",
              "      <td id=\"T_4b03d_row49_col100\" class=\"data row49 col100\" >6.863125</td>\n",
              "      <td id=\"T_4b03d_row49_col101\" class=\"data row49 col101\" >57.619232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "      <td id=\"T_4b03d_row50_col0\" class=\"data row50 col0\" >Substructure Replacement</td>\n",
              "      <td id=\"T_4b03d_row50_col1\" class=\"data row50 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row50_col2\" class=\"data row50 col2\" >2.454502</td>\n",
              "      <td id=\"T_4b03d_row50_col3\" class=\"data row50 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row50_col4\" class=\"data row50 col4\" >829.988000</td>\n",
              "      <td id=\"T_4b03d_row50_col5\" class=\"data row50 col5\" >4.336300</td>\n",
              "      <td id=\"T_4b03d_row50_col6\" class=\"data row50 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row50_col7\" class=\"data row50 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row50_col8\" class=\"data row50 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row50_col9\" class=\"data row50 col9\" >0.098167</td>\n",
              "      <td id=\"T_4b03d_row50_col10\" class=\"data row50 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row50_col11\" class=\"data row50 col11\" >227.070000</td>\n",
              "      <td id=\"T_4b03d_row50_col12\" class=\"data row50 col12\" >0.559601</td>\n",
              "      <td id=\"T_4b03d_row50_col13\" class=\"data row50 col13\" >0.475131</td>\n",
              "      <td id=\"T_4b03d_row50_col14\" class=\"data row50 col14\" >0.838081</td>\n",
              "      <td id=\"T_4b03d_row50_col15\" class=\"data row50 col15\" >0.014178</td>\n",
              "      <td id=\"T_4b03d_row50_col16\" class=\"data row50 col16\" >0.159961</td>\n",
              "      <td id=\"T_4b03d_row50_col17\" class=\"data row50 col17\" >0.045292</td>\n",
              "      <td id=\"T_4b03d_row50_col18\" class=\"data row50 col18\" >0.176849</td>\n",
              "      <td id=\"T_4b03d_row50_col19\" class=\"data row50 col19\" >0.087129</td>\n",
              "      <td id=\"T_4b03d_row50_col20\" class=\"data row50 col20\" >0.166058</td>\n",
              "      <td id=\"T_4b03d_row50_col21\" class=\"data row50 col21\" >0.674928</td>\n",
              "      <td id=\"T_4b03d_row50_col22\" class=\"data row50 col22\" >0.958551</td>\n",
              "      <td id=\"T_4b03d_row50_col23\" class=\"data row50 col23\" >0.165411</td>\n",
              "      <td id=\"T_4b03d_row50_col24\" class=\"data row50 col24\" >0.534557</td>\n",
              "      <td id=\"T_4b03d_row50_col25\" class=\"data row50 col25\" >0.662349</td>\n",
              "      <td id=\"T_4b03d_row50_col26\" class=\"data row50 col26\" >0.992352</td>\n",
              "      <td id=\"T_4b03d_row50_col27\" class=\"data row50 col27\" >0.110231</td>\n",
              "      <td id=\"T_4b03d_row50_col28\" class=\"data row50 col28\" >0.195338</td>\n",
              "      <td id=\"T_4b03d_row50_col29\" class=\"data row50 col29\" >0.125184</td>\n",
              "      <td id=\"T_4b03d_row50_col30\" class=\"data row50 col30\" >0.173265</td>\n",
              "      <td id=\"T_4b03d_row50_col31\" class=\"data row50 col31\" >0.042236</td>\n",
              "      <td id=\"T_4b03d_row50_col32\" class=\"data row50 col32\" >0.155087</td>\n",
              "      <td id=\"T_4b03d_row50_col33\" class=\"data row50 col33\" >0.084772</td>\n",
              "      <td id=\"T_4b03d_row50_col34\" class=\"data row50 col34\" >0.649730</td>\n",
              "      <td id=\"T_4b03d_row50_col35\" class=\"data row50 col35\" >0.824108</td>\n",
              "      <td id=\"T_4b03d_row50_col36\" class=\"data row50 col36\" >0.362540</td>\n",
              "      <td id=\"T_4b03d_row50_col37\" class=\"data row50 col37\" >0.257692</td>\n",
              "      <td id=\"T_4b03d_row50_col38\" class=\"data row50 col38\" >0.051398</td>\n",
              "      <td id=\"T_4b03d_row50_col39\" class=\"data row50 col39\" >0.848864</td>\n",
              "      <td id=\"T_4b03d_row50_col40\" class=\"data row50 col40\" >0.200772</td>\n",
              "      <td id=\"T_4b03d_row50_col41\" class=\"data row50 col41\" >0.159410</td>\n",
              "      <td id=\"T_4b03d_row50_col42\" class=\"data row50 col42\" >0.709695</td>\n",
              "      <td id=\"T_4b03d_row50_col43\" class=\"data row50 col43\" >-5.726415</td>\n",
              "      <td id=\"T_4b03d_row50_col44\" class=\"data row50 col44\" >130.072084</td>\n",
              "      <td id=\"T_4b03d_row50_col45\" class=\"data row50 col45\" >95.076969</td>\n",
              "      <td id=\"T_4b03d_row50_col46\" class=\"data row50 col46\" >34.898124</td>\n",
              "      <td id=\"T_4b03d_row50_col47\" class=\"data row50 col47\" >-9.203876</td>\n",
              "      <td id=\"T_4b03d_row50_col48\" class=\"data row50 col48\" >4.230093</td>\n",
              "      <td id=\"T_4b03d_row50_col49\" class=\"data row50 col49\" >2.454502</td>\n",
              "      <td id=\"T_4b03d_row50_col50\" class=\"data row50 col50\" >96.504863</td>\n",
              "      <td id=\"T_4b03d_row50_col51\" class=\"data row50 col51\" >-5.618443</td>\n",
              "      <td id=\"T_4b03d_row50_col52\" class=\"data row50 col52\" >3.312679</td>\n",
              "      <td id=\"T_4b03d_row50_col53\" class=\"data row50 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row50_col54\" class=\"data row50 col54\" >82.357503</td>\n",
              "      <td id=\"T_4b03d_row50_col55\" class=\"data row50 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row50_col56\" class=\"data row50 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row50_col57\" class=\"data row50 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row50_col58\" class=\"data row50 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row50_col59\" class=\"data row50 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row50_col60\" class=\"data row50 col60\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row50_col61\" class=\"data row50 col61\" >86.157425</td>\n",
              "      <td id=\"T_4b03d_row50_col62\" class=\"data row50 col62\" >29.042264</td>\n",
              "      <td id=\"T_4b03d_row50_col63\" class=\"data row50 col63\" >58.433501</td>\n",
              "      <td id=\"T_4b03d_row50_col64\" class=\"data row50 col64\" >37.301280</td>\n",
              "      <td id=\"T_4b03d_row50_col65\" class=\"data row50 col65\" >58.045754</td>\n",
              "      <td id=\"T_4b03d_row50_col66\" class=\"data row50 col66\" >16.634354</td>\n",
              "      <td id=\"T_4b03d_row50_col67\" class=\"data row50 col67\" >74.176037</td>\n",
              "      <td id=\"T_4b03d_row50_col68\" class=\"data row50 col68\" >45.405196</td>\n",
              "      <td id=\"T_4b03d_row50_col69\" class=\"data row50 col69\" >70.569988</td>\n",
              "      <td id=\"T_4b03d_row50_col70\" class=\"data row50 col70\" >69.833269</td>\n",
              "      <td id=\"T_4b03d_row50_col71\" class=\"data row50 col71\" >97.828616</td>\n",
              "      <td id=\"T_4b03d_row50_col72\" class=\"data row50 col72\" >52.229546</td>\n",
              "      <td id=\"T_4b03d_row50_col73\" class=\"data row50 col73\" >90.926716</td>\n",
              "      <td id=\"T_4b03d_row50_col74\" class=\"data row50 col74\" >63.008918</td>\n",
              "      <td id=\"T_4b03d_row50_col75\" class=\"data row50 col75\" >44.086855</td>\n",
              "      <td id=\"T_4b03d_row50_col76\" class=\"data row50 col76\" >91.159364</td>\n",
              "      <td id=\"T_4b03d_row50_col77\" class=\"data row50 col77\" >91.973633</td>\n",
              "      <td id=\"T_4b03d_row50_col78\" class=\"data row50 col78\" >74.680109</td>\n",
              "      <td id=\"T_4b03d_row50_col79\" class=\"data row50 col79\" >82.939124</td>\n",
              "      <td id=\"T_4b03d_row50_col80\" class=\"data row50 col80\" >72.663823</td>\n",
              "      <td id=\"T_4b03d_row50_col81\" class=\"data row50 col81\" >71.772005</td>\n",
              "      <td id=\"T_4b03d_row50_col82\" class=\"data row50 col82\" >89.647150</td>\n",
              "      <td id=\"T_4b03d_row50_col83\" class=\"data row50 col83\" >42.458317</td>\n",
              "      <td id=\"T_4b03d_row50_col84\" class=\"data row50 col84\" >88.987980</td>\n",
              "      <td id=\"T_4b03d_row50_col85\" class=\"data row50 col85\" >75.378054</td>\n",
              "      <td id=\"T_4b03d_row50_col86\" class=\"data row50 col86\" >95.657231</td>\n",
              "      <td id=\"T_4b03d_row50_col87\" class=\"data row50 col87\" >75.222955</td>\n",
              "      <td id=\"T_4b03d_row50_col88\" class=\"data row50 col88\" >96.432726</td>\n",
              "      <td id=\"T_4b03d_row50_col89\" class=\"data row50 col89\" >85.304382</td>\n",
              "      <td id=\"T_4b03d_row50_col90\" class=\"data row50 col90\" >14.850717</td>\n",
              "      <td id=\"T_4b03d_row50_col91\" class=\"data row50 col91\" >73.012796</td>\n",
              "      <td id=\"T_4b03d_row50_col92\" class=\"data row50 col92\" >17.603722</td>\n",
              "      <td id=\"T_4b03d_row50_col93\" class=\"data row50 col93\" >96.975572</td>\n",
              "      <td id=\"T_4b03d_row50_col94\" class=\"data row50 col94\" >94.881737</td>\n",
              "      <td id=\"T_4b03d_row50_col95\" class=\"data row50 col95\" >79.294300</td>\n",
              "      <td id=\"T_4b03d_row50_col96\" class=\"data row50 col96\" >54.866227</td>\n",
              "      <td id=\"T_4b03d_row50_col97\" class=\"data row50 col97\" >98.759209</td>\n",
              "      <td id=\"T_4b03d_row50_col98\" class=\"data row50 col98\" >69.096549</td>\n",
              "      <td id=\"T_4b03d_row50_col99\" class=\"data row50 col99\" >85.110508</td>\n",
              "      <td id=\"T_4b03d_row50_col100\" class=\"data row50 col100\" >7.754944</td>\n",
              "      <td id=\"T_4b03d_row50_col101\" class=\"data row50 col101\" >63.396665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "      <td id=\"T_4b03d_row51_col0\" class=\"data row51 col0\" >Substructure Replacement</td>\n",
              "      <td id=\"T_4b03d_row51_col1\" class=\"data row51 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1CN)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1</td>\n",
              "      <td id=\"T_4b03d_row51_col2\" class=\"data row51 col2\" >1.765294</td>\n",
              "      <td id=\"T_4b03d_row51_col3\" class=\"data row51 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](N)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1CN)C(=C(C)[C@H](C)OC(=O)[C@@H](N)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O.O.c1ccccc1</td>\n",
              "      <td id=\"T_4b03d_row51_col4\" class=\"data row51 col4\" >835.008000</td>\n",
              "      <td id=\"T_4b03d_row51_col5\" class=\"data row51 col5\" >2.940100</td>\n",
              "      <td id=\"T_4b03d_row51_col6\" class=\"data row51 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row51_col7\" class=\"data row51 col7\" >4</td>\n",
              "      <td id=\"T_4b03d_row51_col8\" class=\"data row51 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row51_col9\" class=\"data row51 col9\" >0.103702</td>\n",
              "      <td id=\"T_4b03d_row51_col10\" class=\"data row51 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row51_col11\" class=\"data row51 col11\" >275.360000</td>\n",
              "      <td id=\"T_4b03d_row51_col12\" class=\"data row51 col12\" >0.500805</td>\n",
              "      <td id=\"T_4b03d_row51_col13\" class=\"data row51 col13\" >0.453258</td>\n",
              "      <td id=\"T_4b03d_row51_col14\" class=\"data row51 col14\" >0.791006</td>\n",
              "      <td id=\"T_4b03d_row51_col15\" class=\"data row51 col15\" >0.023846</td>\n",
              "      <td id=\"T_4b03d_row51_col16\" class=\"data row51 col16\" >0.397932</td>\n",
              "      <td id=\"T_4b03d_row51_col17\" class=\"data row51 col17\" >0.100348</td>\n",
              "      <td id=\"T_4b03d_row51_col18\" class=\"data row51 col18\" >0.364788</td>\n",
              "      <td id=\"T_4b03d_row51_col19\" class=\"data row51 col19\" >0.168019</td>\n",
              "      <td id=\"T_4b03d_row51_col20\" class=\"data row51 col20\" >0.397830</td>\n",
              "      <td id=\"T_4b03d_row51_col21\" class=\"data row51 col21\" >0.701056</td>\n",
              "      <td id=\"T_4b03d_row51_col22\" class=\"data row51 col22\" >0.974822</td>\n",
              "      <td id=\"T_4b03d_row51_col23\" class=\"data row51 col23\" >0.187807</td>\n",
              "      <td id=\"T_4b03d_row51_col24\" class=\"data row51 col24\" >0.543881</td>\n",
              "      <td id=\"T_4b03d_row51_col25\" class=\"data row51 col25\" >0.658485</td>\n",
              "      <td id=\"T_4b03d_row51_col26\" class=\"data row51 col26\" >0.990807</td>\n",
              "      <td id=\"T_4b03d_row51_col27\" class=\"data row51 col27\" >0.047799</td>\n",
              "      <td id=\"T_4b03d_row51_col28\" class=\"data row51 col28\" >0.050280</td>\n",
              "      <td id=\"T_4b03d_row51_col29\" class=\"data row51 col29\" >0.083794</td>\n",
              "      <td id=\"T_4b03d_row51_col30\" class=\"data row51 col30\" >0.085583</td>\n",
              "      <td id=\"T_4b03d_row51_col31\" class=\"data row51 col31\" >0.031046</td>\n",
              "      <td id=\"T_4b03d_row51_col32\" class=\"data row51 col32\" >0.105439</td>\n",
              "      <td id=\"T_4b03d_row51_col33\" class=\"data row51 col33\" >0.053891</td>\n",
              "      <td id=\"T_4b03d_row51_col34\" class=\"data row51 col34\" >0.641881</td>\n",
              "      <td id=\"T_4b03d_row51_col35\" class=\"data row51 col35\" >0.882274</td>\n",
              "      <td id=\"T_4b03d_row51_col36\" class=\"data row51 col36\" >0.285273</td>\n",
              "      <td id=\"T_4b03d_row51_col37\" class=\"data row51 col37\" >0.129594</td>\n",
              "      <td id=\"T_4b03d_row51_col38\" class=\"data row51 col38\" >0.046408</td>\n",
              "      <td id=\"T_4b03d_row51_col39\" class=\"data row51 col39\" >0.719476</td>\n",
              "      <td id=\"T_4b03d_row51_col40\" class=\"data row51 col40\" >0.101874</td>\n",
              "      <td id=\"T_4b03d_row51_col41\" class=\"data row51 col41\" >0.167680</td>\n",
              "      <td id=\"T_4b03d_row51_col42\" class=\"data row51 col42\" >0.828904</td>\n",
              "      <td id=\"T_4b03d_row51_col43\" class=\"data row51 col43\" >-5.877900</td>\n",
              "      <td id=\"T_4b03d_row51_col44\" class=\"data row51 col44\" >107.020400</td>\n",
              "      <td id=\"T_4b03d_row51_col45\" class=\"data row51 col45\" >69.575875</td>\n",
              "      <td id=\"T_4b03d_row51_col46\" class=\"data row51 col46\" >48.371664</td>\n",
              "      <td id=\"T_4b03d_row51_col47\" class=\"data row51 col47\" >-10.066122</td>\n",
              "      <td id=\"T_4b03d_row51_col48\" class=\"data row51 col48\" >4.332870</td>\n",
              "      <td id=\"T_4b03d_row51_col49\" class=\"data row51 col49\" >1.765295</td>\n",
              "      <td id=\"T_4b03d_row51_col50\" class=\"data row51 col50\" >88.530695</td>\n",
              "      <td id=\"T_4b03d_row51_col51\" class=\"data row51 col51\" >-3.832151</td>\n",
              "      <td id=\"T_4b03d_row51_col52\" class=\"data row51 col52\" >1.695130</td>\n",
              "      <td id=\"T_4b03d_row51_col53\" class=\"data row51 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row51_col54\" class=\"data row51 col54\" >60.915083</td>\n",
              "      <td id=\"T_4b03d_row51_col55\" class=\"data row51 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row51_col56\" class=\"data row51 col56\" >86.021714</td>\n",
              "      <td id=\"T_4b03d_row51_col57\" class=\"data row51 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row51_col58\" class=\"data row51 col58\" >5.661109</td>\n",
              "      <td id=\"T_4b03d_row51_col59\" class=\"data row51 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row51_col60\" class=\"data row51 col60\" >95.075611</td>\n",
              "      <td id=\"T_4b03d_row51_col61\" class=\"data row51 col61\" >82.706475</td>\n",
              "      <td id=\"T_4b03d_row51_col62\" class=\"data row51 col62\" >27.723924</td>\n",
              "      <td id=\"T_4b03d_row51_col63\" class=\"data row51 col63\" >49.476541</td>\n",
              "      <td id=\"T_4b03d_row51_col64\" class=\"data row51 col64\" >43.582784</td>\n",
              "      <td id=\"T_4b03d_row51_col65\" class=\"data row51 col65\" >78.596355</td>\n",
              "      <td id=\"T_4b03d_row51_col66\" class=\"data row51 col66\" >35.672741</td>\n",
              "      <td id=\"T_4b03d_row51_col67\" class=\"data row51 col67\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row51_col68\" class=\"data row51 col68\" >61.535479</td>\n",
              "      <td id=\"T_4b03d_row51_col69\" class=\"data row51 col69\" >82.745250</td>\n",
              "      <td id=\"T_4b03d_row51_col70\" class=\"data row51 col70\" >73.051570</td>\n",
              "      <td id=\"T_4b03d_row51_col71\" class=\"data row51 col71\" >98.604110</td>\n",
              "      <td id=\"T_4b03d_row51_col72\" class=\"data row51 col72\" >56.494765</td>\n",
              "      <td id=\"T_4b03d_row51_col73\" class=\"data row51 col73\" >91.314463</td>\n",
              "      <td id=\"T_4b03d_row51_col74\" class=\"data row51 col74\" >62.776270</td>\n",
              "      <td id=\"T_4b03d_row51_col75\" class=\"data row51 col75\" >42.303218</td>\n",
              "      <td id=\"T_4b03d_row51_col76\" class=\"data row51 col76\" >84.024816</td>\n",
              "      <td id=\"T_4b03d_row51_col77\" class=\"data row51 col77\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row51_col78\" class=\"data row51 col78\" >68.902675</td>\n",
              "      <td id=\"T_4b03d_row51_col79\" class=\"data row51 col79\" >70.492439</td>\n",
              "      <td id=\"T_4b03d_row51_col80\" class=\"data row51 col80\" >65.490500</td>\n",
              "      <td id=\"T_4b03d_row51_col81\" class=\"data row51 col81\" >58.123304</td>\n",
              "      <td id=\"T_4b03d_row51_col82\" class=\"data row51 col82\" >84.839085</td>\n",
              "      <td id=\"T_4b03d_row51_col83\" class=\"data row51 col83\" >41.915471</td>\n",
              "      <td id=\"T_4b03d_row51_col84\" class=\"data row51 col84\" >92.128732</td>\n",
              "      <td id=\"T_4b03d_row51_col85\" class=\"data row51 col85\" >70.143466</td>\n",
              "      <td id=\"T_4b03d_row51_col86\" class=\"data row51 col86\" >90.810392</td>\n",
              "      <td id=\"T_4b03d_row51_col87\" class=\"data row51 col87\" >73.594416</td>\n",
              "      <td id=\"T_4b03d_row51_col88\" class=\"data row51 col88\" >92.865452</td>\n",
              "      <td id=\"T_4b03d_row51_col89\" class=\"data row51 col89\" >75.649477</td>\n",
              "      <td id=\"T_4b03d_row51_col90\" class=\"data row51 col90\" >16.169058</td>\n",
              "      <td id=\"T_4b03d_row51_col91\" class=\"data row51 col91\" >80.263668</td>\n",
              "      <td id=\"T_4b03d_row51_col92\" class=\"data row51 col92\" >14.269097</td>\n",
              "      <td id=\"T_4b03d_row51_col93\" class=\"data row51 col93\" >92.245056</td>\n",
              "      <td id=\"T_4b03d_row51_col94\" class=\"data row51 col94\" >87.165568</td>\n",
              "      <td id=\"T_4b03d_row51_col95\" class=\"data row51 col95\" >86.777821</td>\n",
              "      <td id=\"T_4b03d_row51_col96\" class=\"data row51 col96\" >45.560295</td>\n",
              "      <td id=\"T_4b03d_row51_col97\" class=\"data row51 col97\" >99.108181</td>\n",
              "      <td id=\"T_4b03d_row51_col98\" class=\"data row51 col98\" >55.680496</td>\n",
              "      <td id=\"T_4b03d_row51_col99\" class=\"data row51 col99\" >70.492439</td>\n",
              "      <td id=\"T_4b03d_row51_col100\" class=\"data row51 col100\" >35.052346</td>\n",
              "      <td id=\"T_4b03d_row51_col101\" class=\"data row51 col101\" >51.686700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4b03d_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "      <td id=\"T_4b03d_row52_col0\" class=\"data row52 col0\" >Stereochemistry Alteration</td>\n",
              "      <td id=\"T_4b03d_row52_col1\" class=\"data row52 col1\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](O)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row52_col2\" class=\"data row52 col2\" >3.081101</td>\n",
              "      <td id=\"T_4b03d_row52_col3\" class=\"data row52 col3\" >CC(=O)O[C@@H](C(=O)[C@]1(C)[C@H](O)C[C@H]2OC[C@@]2(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1ccccc1)C(C)C.O</td>\n",
              "      <td id=\"T_4b03d_row52_col4\" class=\"data row52 col4\" >831.956000</td>\n",
              "      <td id=\"T_4b03d_row52_col5\" class=\"data row52 col5\" >4.403500</td>\n",
              "      <td id=\"T_4b03d_row52_col6\" class=\"data row52 col6\" >12</td>\n",
              "      <td id=\"T_4b03d_row52_col7\" class=\"data row52 col7\" >3</td>\n",
              "      <td id=\"T_4b03d_row52_col8\" class=\"data row52 col8\" >2.000000</td>\n",
              "      <td id=\"T_4b03d_row52_col9\" class=\"data row52 col9\" >0.098075</td>\n",
              "      <td id=\"T_4b03d_row52_col10\" class=\"data row52 col10\" >9</td>\n",
              "      <td id=\"T_4b03d_row52_col11\" class=\"data row52 col11\" >215.490000</td>\n",
              "      <td id=\"T_4b03d_row52_col12\" class=\"data row52 col12\" >0.387917</td>\n",
              "      <td id=\"T_4b03d_row52_col13\" class=\"data row52 col13\" >0.332681</td>\n",
              "      <td id=\"T_4b03d_row52_col14\" class=\"data row52 col14\" >0.749548</td>\n",
              "      <td id=\"T_4b03d_row52_col15\" class=\"data row52 col15\" >0.009997</td>\n",
              "      <td id=\"T_4b03d_row52_col16\" class=\"data row52 col16\" >0.124013</td>\n",
              "      <td id=\"T_4b03d_row52_col17\" class=\"data row52 col17\" >0.061366</td>\n",
              "      <td id=\"T_4b03d_row52_col18\" class=\"data row52 col18\" >0.218892</td>\n",
              "      <td id=\"T_4b03d_row52_col19\" class=\"data row52 col19\" >0.040728</td>\n",
              "      <td id=\"T_4b03d_row52_col20\" class=\"data row52 col20\" >0.097298</td>\n",
              "      <td id=\"T_4b03d_row52_col21\" class=\"data row52 col21\" >0.690743</td>\n",
              "      <td id=\"T_4b03d_row52_col22\" class=\"data row52 col22\" >0.825173</td>\n",
              "      <td id=\"T_4b03d_row52_col23\" class=\"data row52 col23\" >0.148640</td>\n",
              "      <td id=\"T_4b03d_row52_col24\" class=\"data row52 col24\" >0.262799</td>\n",
              "      <td id=\"T_4b03d_row52_col25\" class=\"data row52 col25\" >0.748266</td>\n",
              "      <td id=\"T_4b03d_row52_col26\" class=\"data row52 col26\" >0.992404</td>\n",
              "      <td id=\"T_4b03d_row52_col27\" class=\"data row52 col27\" >0.101576</td>\n",
              "      <td id=\"T_4b03d_row52_col28\" class=\"data row52 col28\" >0.246526</td>\n",
              "      <td id=\"T_4b03d_row52_col29\" class=\"data row52 col29\" >0.088185</td>\n",
              "      <td id=\"T_4b03d_row52_col30\" class=\"data row52 col30\" >0.191500</td>\n",
              "      <td id=\"T_4b03d_row52_col31\" class=\"data row52 col31\" >0.018916</td>\n",
              "      <td id=\"T_4b03d_row52_col32\" class=\"data row52 col32\" >0.140662</td>\n",
              "      <td id=\"T_4b03d_row52_col33\" class=\"data row52 col33\" >0.057134</td>\n",
              "      <td id=\"T_4b03d_row52_col34\" class=\"data row52 col34\" >0.653945</td>\n",
              "      <td id=\"T_4b03d_row52_col35\" class=\"data row52 col35\" >0.841265</td>\n",
              "      <td id=\"T_4b03d_row52_col36\" class=\"data row52 col36\" >0.330410</td>\n",
              "      <td id=\"T_4b03d_row52_col37\" class=\"data row52 col37\" >0.146724</td>\n",
              "      <td id=\"T_4b03d_row52_col38\" class=\"data row52 col38\" >0.045287</td>\n",
              "      <td id=\"T_4b03d_row52_col39\" class=\"data row52 col39\" >0.912250</td>\n",
              "      <td id=\"T_4b03d_row52_col40\" class=\"data row52 col40\" >0.184572</td>\n",
              "      <td id=\"T_4b03d_row52_col41\" class=\"data row52 col41\" >0.101444</td>\n",
              "      <td id=\"T_4b03d_row52_col42\" class=\"data row52 col42\" >0.669242</td>\n",
              "      <td id=\"T_4b03d_row52_col43\" class=\"data row52 col43\" >-5.503417</td>\n",
              "      <td id=\"T_4b03d_row52_col44\" class=\"data row52 col44\" >132.670598</td>\n",
              "      <td id=\"T_4b03d_row52_col45\" class=\"data row52 col45\" >113.839211</td>\n",
              "      <td id=\"T_4b03d_row52_col46\" class=\"data row52 col46\" >33.224949</td>\n",
              "      <td id=\"T_4b03d_row52_col47\" class=\"data row52 col47\" >-10.343788</td>\n",
              "      <td id=\"T_4b03d_row52_col48\" class=\"data row52 col48\" >4.368650</td>\n",
              "      <td id=\"T_4b03d_row52_col49\" class=\"data row52 col49\" >3.081101</td>\n",
              "      <td id=\"T_4b03d_row52_col50\" class=\"data row52 col50\" >103.055698</td>\n",
              "      <td id=\"T_4b03d_row52_col51\" class=\"data row52 col51\" >-5.843886</td>\n",
              "      <td id=\"T_4b03d_row52_col52\" class=\"data row52 col52\" >3.953493</td>\n",
              "      <td id=\"T_4b03d_row52_col53\" class=\"data row52 col53\" >94.804188</td>\n",
              "      <td id=\"T_4b03d_row52_col54\" class=\"data row52 col54\" >82.900349</td>\n",
              "      <td id=\"T_4b03d_row52_col55\" class=\"data row52 col55\" >92.322606</td>\n",
              "      <td id=\"T_4b03d_row52_col56\" class=\"data row52 col56\" >77.258627</td>\n",
              "      <td id=\"T_4b03d_row52_col57\" class=\"data row52 col57\" >9.848779</td>\n",
              "      <td id=\"T_4b03d_row52_col58\" class=\"data row52 col58\" >5.389686</td>\n",
              "      <td id=\"T_4b03d_row52_col59\" class=\"data row52 col59\" >93.951144</td>\n",
              "      <td id=\"T_4b03d_row52_col60\" class=\"data row52 col60\" >93.059325</td>\n",
              "      <td id=\"T_4b03d_row52_col61\" class=\"data row52 col61\" >75.067856</td>\n",
              "      <td id=\"T_4b03d_row52_col62\" class=\"data row52 col62\" >20.240403</td>\n",
              "      <td id=\"T_4b03d_row52_col63\" class=\"data row52 col63\" >42.923614</td>\n",
              "      <td id=\"T_4b03d_row52_col64\" class=\"data row52 col64\" >33.772780</td>\n",
              "      <td id=\"T_4b03d_row52_col65\" class=\"data row52 col65\" >53.470337</td>\n",
              "      <td id=\"T_4b03d_row52_col66\" class=\"data row52 col66\" >22.140364</td>\n",
              "      <td id=\"T_4b03d_row52_col67\" class=\"data row52 col67\" >77.161691</td>\n",
              "      <td id=\"T_4b03d_row52_col68\" class=\"data row52 col68\" >29.274913</td>\n",
              "      <td id=\"T_4b03d_row52_col69\" class=\"data row52 col69\" >61.574254</td>\n",
              "      <td id=\"T_4b03d_row52_col70\" class=\"data row52 col70\" >71.655680</td>\n",
              "      <td id=\"T_4b03d_row52_col71\" class=\"data row52 col71\" >91.547111</td>\n",
              "      <td id=\"T_4b03d_row52_col72\" class=\"data row52 col72\" >48.584723</td>\n",
              "      <td id=\"T_4b03d_row52_col73\" class=\"data row52 col73\" >73.206669</td>\n",
              "      <td id=\"T_4b03d_row52_col74\" class=\"data row52 col74\" >67.739434</td>\n",
              "      <td id=\"T_4b03d_row52_col75\" class=\"data row52 col75\" >44.125630</td>\n",
              "      <td id=\"T_4b03d_row52_col76\" class=\"data row52 col76\" >90.694067</td>\n",
              "      <td id=\"T_4b03d_row52_col77\" class=\"data row52 col77\" >93.253199</td>\n",
              "      <td id=\"T_4b03d_row52_col78\" class=\"data row52 col78\" >69.600620</td>\n",
              "      <td id=\"T_4b03d_row52_col79\" class=\"data row52 col79\" >84.490112</td>\n",
              "      <td id=\"T_4b03d_row52_col80\" class=\"data row52 col80\" >51.725475</td>\n",
              "      <td id=\"T_4b03d_row52_col81\" class=\"data row52 col81\" >68.514928</td>\n",
              "      <td id=\"T_4b03d_row52_col82\" class=\"data row52 col82\" >85.498255</td>\n",
              "      <td id=\"T_4b03d_row52_col83\" class=\"data row52 col83\" >42.652191</td>\n",
              "      <td id=\"T_4b03d_row52_col84\" class=\"data row52 col84\" >89.918573</td>\n",
              "      <td id=\"T_4b03d_row52_col85\" class=\"data row52 col85\" >73.361768</td>\n",
              "      <td id=\"T_4b03d_row52_col86\" class=\"data row52 col86\" >91.547111</td>\n",
              "      <td id=\"T_4b03d_row52_col87\" class=\"data row52 col87\" >73.322993</td>\n",
              "      <td id=\"T_4b03d_row52_col88\" class=\"data row52 col88\" >98.177588</td>\n",
              "      <td id=\"T_4b03d_row52_col89\" class=\"data row52 col89\" >84.218689</td>\n",
              "      <td id=\"T_4b03d_row52_col90\" class=\"data row52 col90\" >5.854983</td>\n",
              "      <td id=\"T_4b03d_row52_col91\" class=\"data row52 col91\" >70.686313</td>\n",
              "      <td id=\"T_4b03d_row52_col92\" class=\"data row52 col92\" >23.342381</td>\n",
              "      <td id=\"T_4b03d_row52_col93\" class=\"data row52 col93\" >97.440869</td>\n",
              "      <td id=\"T_4b03d_row52_col94\" class=\"data row52 col94\" >97.363319</td>\n",
              "      <td id=\"T_4b03d_row52_col95\" class=\"data row52 col95\" >78.131059</td>\n",
              "      <td id=\"T_4b03d_row52_col96\" class=\"data row52 col96\" >42.923614</td>\n",
              "      <td id=\"T_4b03d_row52_col97\" class=\"data row52 col97\" >99.263280</td>\n",
              "      <td id=\"T_4b03d_row52_col98\" class=\"data row52 col98\" >82.551377</td>\n",
              "      <td id=\"T_4b03d_row52_col99\" class=\"data row52 col99\" >94.649089</td>\n",
              "      <td id=\"T_4b03d_row52_col100\" class=\"data row52 col100\" >6.203955</td>\n",
              "      <td id=\"T_4b03d_row52_col101\" class=\"data row52 col101\" >67.700659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADMET predictions saved as 'generated_molecules_admet.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Visualizing ADMET Predictions for the Optimized Molecule**"
      ],
      "metadata": {
        "id": "UgTCDev_Gw5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section predicts and visualizes the ADMET properties of the optimized molecule (best_smiles) using the ADMET-AI model. It assesses the drug-likeness probability based on percentile scores of DrugBank-approved molecules.\n",
        "\n",
        "Key Steps:\n",
        "\n",
        "Predict ADMET Properties – Uses ADMETModel to evaluate key pharmacokinetic and toxicity parameters.\n",
        "\n",
        "Extract Percentile Scores – Filters properties containing \"drugbank_approved_percentile\" for drug-likeness assessment.\n",
        "\n",
        "Compute Drug-Likeness Probability – Calculates the average percentile score to estimate the molecule’s drug potential.\n",
        "\n",
        "Visualize Predictions – Generates a bar chart showcasing percentile values for different ADMET properties, with a red dashed line indicating the overall average.\n",
        "\n",
        "Save Results – The graph is saved as \"admet_predictions.png\" for further analysis.\n",
        "\n",
        "This visualization helps in understanding the molecule’s pharmacokinetic profile and aids in selecting promising drug candidates based on ADMET criteria."
      ],
      "metadata": {
        "id": "KDngqX6j41Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from admet_ai import ADMETModel\n",
        "\n",
        "# Ensure that best_smiles (the optimized molecule's SMILES) is defined\n",
        "# For example:\n",
        "# best_smiles = \"optimized_SMILES_string_here\"\n",
        "print(\"Optimized SMILES:\", best_smiles)  # Print the optimized SMILES\n",
        "\n",
        "# Initialize the ADMET-AI model\n",
        "model = ADMETModel()\n",
        "\n",
        "# Predict ADMET properties for the optimized molecule (using a list with a single SMILES)\n",
        "admet_prediction = model.predict(smiles=[best_smiles])\n",
        "\n",
        "# Check if prediction was successful\n",
        "if isinstance(admet_prediction, pd.DataFrame) and not admet_prediction.empty:\n",
        "    # Optionally display the column names to see what properties are available\n",
        "    print(\"ADMET Predictions Columns:\", admet_prediction.columns.tolist())\n",
        "\n",
        "    # Filter for columns that contain \"drugbank_approved_percentile\"\n",
        "    percentile_cols = [col for col in admet_prediction.columns if \"drugbank_approved_percentile\" in col]\n",
        "\n",
        "    if not percentile_cols:\n",
        "        print(\"No 'drugbank_approved_percentile' properties found in the predictions.\")\n",
        "    else:\n",
        "        # Use iloc[0] to get the first (and only) row as a Series\n",
        "        molecule_percentiles = admet_prediction.iloc[0][percentile_cols]\n",
        "\n",
        "        # Compute the overall \"drug-likeness probability\" as the average of these percentiles\n",
        "        overall_probability = molecule_percentiles.mean()\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.bar(range(len(percentile_cols)), molecule_percentiles.values, color='skyblue')\n",
        "        plt.xticks(range(len(percentile_cols)), percentile_cols, rotation=90)\n",
        "        plt.xlabel(\"DrugBank Approved Percentile Properties\")\n",
        "        plt.ylabel(\"Percentile Value (%)\")\n",
        "        plt.title(f\"DrugBank Approved Percentile Properties for Optimized Molecule\\nSMILES: {best_smiles}\")\n",
        "        plt.axhline(overall_probability, color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Overall Average: {overall_probability:.2f}%')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the graph as an image file before showing it\n",
        "        plt.savefig(\"admet_predictions.png\", dpi=300)\n",
        "        print(\"Graph saved as 'admet_predictions.png'.\")\n",
        "\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Overall drug-likeness probability (average): {overall_probability:.2f}%\")\n",
        "else:\n",
        "    print(\"Error: ADMET prediction failed or returned an empty dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PlQwiM_RDi7t",
        "outputId": "bc6ce819-77b4-45f8-fbad-1c166dfcc1c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized SMILES: CC(=O)O[C@@H](C(=O)[C@]1(C)[C@@H](O)CC[C@](C)(OC(C)=O)[C@H]1COCc1ccccc1)C(=C(C)[C@H](C)OC(=O)[C@H](O)[C@@H](NC(=O)c1ccccc1)c1cccc(C)c1)C(C)C.O\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:473: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vars(torch.load(path, map_location=lambda storage, loc: storage)[\"args\"]),\n",
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/chemprop/utils.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
            "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
            "Loading pretrained parameter \"readout.1.weight\".\n",
            "Loading pretrained parameter \"readout.1.bias\".\n",
            "Loading pretrained parameter \"readout.4.weight\".\n",
            "Loading pretrained parameter \"readout.4.bias\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SMILES to Mol: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
            "Computing physchem properties: 100%|██████████| 1/1 [00:00<00:00, 137.38it/s]\n",
            "RDKit fingerprints: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
            "model ensembles:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models:  80%|████████  | 4/5 [00:00<00:00, 32.16it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 32.36it/s]\n",
            "model ensembles:  50%|█████     | 1/2 [00:00<00:00,  6.28it/s]\n",
            "individual models:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "                                     \u001b[A\u001b[A\n",
            "individual models: 100%|██████████| 5/5 [00:00<00:00, 40.50it/s]\n",
            "model ensembles: 100%|██████████| 2/2 [00:00<00:00,  6.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADMET Predictions Columns: ['molecular_weight', 'logP', 'hydrogen_bond_acceptors', 'hydrogen_bond_donors', 'Lipinski', 'QED', 'stereo_centers', 'tpsa', 'AMES', 'BBB_Martins', 'Bioavailability_Ma', 'CYP1A2_Veith', 'CYP2C19_Veith', 'CYP2C9_Substrate_CarbonMangels', 'CYP2C9_Veith', 'CYP2D6_Substrate_CarbonMangels', 'CYP2D6_Veith', 'CYP3A4_Substrate_CarbonMangels', 'CYP3A4_Veith', 'Carcinogens_Lagunin', 'ClinTox', 'DILI', 'HIA_Hou', 'NR-AR-LBD', 'NR-AR', 'NR-AhR', 'NR-Aromatase', 'NR-ER-LBD', 'NR-ER', 'NR-PPAR-gamma', 'PAMPA_NCATS', 'Pgp_Broccatelli', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53', 'Skin_Reaction', 'hERG', 'Caco2_Wang', 'Clearance_Hepatocyte_AZ', 'Clearance_Microsome_AZ', 'Half_Life_Obach', 'HydrationFreeEnergy_FreeSolv', 'LD50_Zhu', 'Lipophilicity_AstraZeneca', 'PPBR_AZ', 'Solubility_AqSolDB', 'VDss_Lombardo', 'molecular_weight_drugbank_approved_percentile', 'logP_drugbank_approved_percentile', 'hydrogen_bond_acceptors_drugbank_approved_percentile', 'hydrogen_bond_donors_drugbank_approved_percentile', 'Lipinski_drugbank_approved_percentile', 'QED_drugbank_approved_percentile', 'stereo_centers_drugbank_approved_percentile', 'tpsa_drugbank_approved_percentile', 'AMES_drugbank_approved_percentile', 'BBB_Martins_drugbank_approved_percentile', 'Bioavailability_Ma_drugbank_approved_percentile', 'CYP1A2_Veith_drugbank_approved_percentile', 'CYP2C19_Veith_drugbank_approved_percentile', 'CYP2C9_Substrate_CarbonMangels_drugbank_approved_percentile', 'CYP2C9_Veith_drugbank_approved_percentile', 'CYP2D6_Substrate_CarbonMangels_drugbank_approved_percentile', 'CYP2D6_Veith_drugbank_approved_percentile', 'CYP3A4_Substrate_CarbonMangels_drugbank_approved_percentile', 'CYP3A4_Veith_drugbank_approved_percentile', 'Carcinogens_Lagunin_drugbank_approved_percentile', 'ClinTox_drugbank_approved_percentile', 'DILI_drugbank_approved_percentile', 'HIA_Hou_drugbank_approved_percentile', 'NR-AR-LBD_drugbank_approved_percentile', 'NR-AR_drugbank_approved_percentile', 'NR-AhR_drugbank_approved_percentile', 'NR-Aromatase_drugbank_approved_percentile', 'NR-ER-LBD_drugbank_approved_percentile', 'NR-ER_drugbank_approved_percentile', 'NR-PPAR-gamma_drugbank_approved_percentile', 'PAMPA_NCATS_drugbank_approved_percentile', 'Pgp_Broccatelli_drugbank_approved_percentile', 'SR-ARE_drugbank_approved_percentile', 'SR-ATAD5_drugbank_approved_percentile', 'SR-HSE_drugbank_approved_percentile', 'SR-MMP_drugbank_approved_percentile', 'SR-p53_drugbank_approved_percentile', 'Skin_Reaction_drugbank_approved_percentile', 'hERG_drugbank_approved_percentile', 'Caco2_Wang_drugbank_approved_percentile', 'Clearance_Hepatocyte_AZ_drugbank_approved_percentile', 'Clearance_Microsome_AZ_drugbank_approved_percentile', 'Half_Life_Obach_drugbank_approved_percentile', 'HydrationFreeEnergy_FreeSolv_drugbank_approved_percentile', 'LD50_Zhu_drugbank_approved_percentile', 'Lipophilicity_AstraZeneca_drugbank_approved_percentile', 'PPBR_AZ_drugbank_approved_percentile', 'Solubility_AqSolDB_drugbank_approved_percentile', 'VDss_Lombardo_drugbank_approved_percentile']\n",
            "Graph saved as 'admet_predictions.png'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall drug-likeness probability (average): 65.57%\n"
          ]
        }
      ]
    }
  ]
}